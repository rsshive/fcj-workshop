[{"uri":"https://rsshive.github.io/fcj-workshop/5-workshop/5.5-cognito-function/5.5.1-auth-functions/","title":"Create Authentication Functions","tags":[],"description":"","content":"5.5.1 Create Authentication Functions The complete, production-ready Cognito logic — exactly as used in your workshop\nYou are now going to create the heart of your authentication system — a single, well-organized file containing all Cognito operations with proper error handling, TypeScript types, and role detection.\nStep 1 – Fix Folder Name \u0026amp; Create Config (Already Done) Make sure you have:\nlib/amplify-config.ts (with ssr: true) Folder name corrected: lib/ (not libc/) Step 2 – Create the Full Authentication Module File: lib/cognito-auth.ts\n// lib/cognito-auth.ts import { Amplify } from \u0026#39;aws-amplify\u0026#39;; import { signUp, confirmSignUp, signIn, signOut, getCurrentUser, fetchAuthSession, fetchUserAttributes, resetPassword, confirmResetPassword, type SignUpInput, } from \u0026#39;aws-amplify/auth\u0026#39;; // This import triggers Amplify configuration (side effect) import \u0026#39;@/lib/amplify-config\u0026#39;; // Optional: safety check (useful during development) function ensureAmplifyConfigured() { const config = Amplify.getConfig(); if (!config.Auth?.Cognito?.userPoolId) { console.error(\u0026#39;Amplify not configured properly. Missing userPoolId.\u0026#39;); throw new Error(\u0026#39;Auth UserPool not configured\u0026#39;); } } // ============================================ // SIGN UP // ============================================ export interface SignUpParams { email: string; password: string; name: string; } export async function cognitoSignUp({ email, password, name }: SignUpParams) { try { const { isSignUpComplete, userId, nextStep } = await signUp({ username: email.trim(), password: password.trim(), options: { userAttributes: { email: email.trim(), name: name.trim(), }, }, } as SignUpInput); return { success: true, isSignUpComplete, userId, nextStep, message: \u0026#39;Sign up successful! Check your email for verification.\u0026#39;, }; } catch (error: any) { console.error(\u0026#39;Sign up error:\u0026#39;, error); if (error.name === \u0026#39;UsernameExistsException\u0026#39;) { return { success: false, error: \u0026#39;Account already exists.\u0026#39; }; } return { success: false, error: error.message || \u0026#39;Sign up failed\u0026#39; }; } } // ============================================ // CONFIRM SIGN UP (Email Verification) // ============================================ export async function cognitoConfirmSignUp(email: string, code: string) { try { await confirmSignUp({ username: email.trim(), confirmationCode: code.trim(), }); return { success: true, message: \u0026#39;Email verified! You can now sign in.\u0026#39; }; } catch (error: any) { console.error(\u0026#39;Confirm sign up error:\u0026#39;, error); return { success: false, error: error.message || \u0026#39;Verification failed\u0026#39; }; } } // ============================================ // SIGN IN // ============================================ export interface SignInParams { email: string; password: string; } export async function cognitoSignIn({ email, password }: SignInParams) { try { const { isSignedIn, nextStep } = await signIn({ username: email.trim(), password: password.trim(), }); if (isSignedIn) { const attributes = await fetchUserAttributes(); return { success: true, isSignedIn, user: attributes, message: \u0026#39;Sign in successful!\u0026#39;, }; } return { success: false, nextStep, error: \u0026#39;Additional steps required\u0026#39; }; } catch (error: any) { console.error(\u0026#39;Sign in error:\u0026#39;, error); if (error.name === \u0026#39;NotAuthorizedException\u0026#39;) { return { success: false, error: \u0026#39;Invalid email or password\u0026#39; }; } if (error.name === \u0026#39;UserNotConfirmedException\u0026#39;) { return { success: false, error: \u0026#39;Please verify your email first\u0026#39; }; } return { success: false, error: error.message || \u0026#39;Sign in failed\u0026#39; }; } } // ============================================ // SIGN OUT // ============================================ export async function cognitoSignOut() { try { await signOut(); return { success: true, message: \u0026#39;Signed out successfully\u0026#39; }; } catch (error: any) { console.error(\u0026#39;Sign out error:\u0026#39;, error); return { success: false, error: error.message || \u0026#39;Sign out failed\u0026#39; }; } } // ============================================ // GET CURRENT USER + ROLE DETECTION // ============================================ export async function getCognitoUser() { try { // This will throw if not authenticated const currentUser = await getCurrentUser(); const attributes = await fetchUserAttributes(); const session = await fetchAuthSession(); const idTokenPayload = session.tokens?.idToken?.payload; const groups = (idTokenPayload?.[\u0026#39;cognito:groups\u0026#39;] as string[]) || []; const role = groups.includes(\u0026#39;admin\u0026#39;) ? \u0026#39;admin\u0026#39; : \u0026#39;user\u0026#39;; return { userId: currentUser.userId, username: currentUser.username, email: attributes.email || \u0026#39;\u0026#39;, name: attributes.name || \u0026#39;\u0026#39;, groups, role, }; } catch (error: any) { // Expected when user is not signed in if (error.name === \u0026#39;UserUnAuthenticatedException\u0026#39; || error.message?.includes(\u0026#39;not authenticated\u0026#39;)) { return null; } console.error(\u0026#39;Get user error:\u0026#39;, error); return null; } } // ============================================ // PASSWORD RESET (Bonus) // ============================================ export async function cognitoResetPassword(email: string) { try { await resetPassword({ username: email.trim() }); return { success: true, message: \u0026#39;Reset code sent to your email\u0026#39; }; } catch (error: any) { return { success: false, error: error.message || \u0026#39;Failed to send reset code\u0026#39; }; } } export async function cognitoConfirmResetPassword(email: string, code: string, newPassword: string) { try { await confirmResetPassword({ username: email.trim(), confirmationCode: code.trim(), newPassword, }); return { success: true, message: \u0026#39;Password reset successful!\u0026#39; }; } catch (error: any) { return { success: false, error: error.message || \u0026#39;Failed to reset password\u0026#39; }; } } Navigation:\nPrevious: 5.5 Cognito Functions Overview Next Step: 5.5.2 Build Authentication UI → Create sign-up, sign-in, and dashboard pages "},{"uri":"https://rsshive.github.io/fcj-workshop/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Mai Xuân Thắng\nPhone Number: 0856339008\nEmail: geek40931@gmail.com\nUniversity: FPT University Ho Chi Minh\nMajor: Software Engineering\nClass: SE196244\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://rsshive.github.io/fcj-workshop/5-workshop/5.1-workshop-overview/","title":"Workshop Overview","tags":[],"description":"","content":"5.1 Workshop Overview What are Amazon Cognito and AWS Amplify? • Why use them together?\nWhat is Amazon Cognito? Amazon Cognito is AWS’s fully managed identity service that provides:\nUser sign-up \u0026amp; sign-in (email/password, social logins, SAML/OIDC) Built-in email/phone verification Multi-factor authentication (MFA) User directory with custom attributes and groups Secure JWT tokens (ID token, access token, refresh token) Scales automatically to millions of users Free tier: 50,000 monthly active users In this workshop, we use Cognito User Pools as our secure backend identity provider.\nWhat is AWS Amplify (Gen 2 / v6+)? AWS Amplify is the official AWS toolkit for frontend and mobile developers.\nThe new Amplify Gen 2 library (v6+) released in 2024–2025 is completely rewritten and is now:\nTree-shakable (only import what you use) Fully SSR-compatible (ssr: true) Designed specifically for Next.js App Router \u0026amp; React Server Components No more @aws-amplify/ui-react or Hosted UI required Uses the new modular @aws-amplify/auth, @aws-amplify/datastore, etc. We will use only the Auth category of Amplify Gen 2 — the cleanest, most secure, and most modern way to connect a Next.js app to Cognito.\nWhy Use Cognito + Amplify Gen 2 Together in 2025? Benefit Explanation Rapid \u0026amp; Secure Development Amplify abstracts all token handling, refresh logic, and Cognito API calls SSR \u0026amp; App Router Ready Works perfectly with ssr: true → no hydration errors, no window is not defined No Client Secret Public SPA client (no secret needed) → safe for Next.js \u0026amp; Vercel Production-Grade Security Tokens never touch localStorage on server, automatic refresh, secure by default Role-Based Access Uses Cognito Groups → cognito:groups in ID token → easy admin/user detection Deploy Anywhere Works flawlessly on Vercel, Netlify, AWS Amplify Hosting, CloudFront + S3 Future-Proof This is the official AWS-recommended pattern from 2024 onward What You Will Build (Exact Outcome) A complete Next.js 14 (App Router) application with:\nSign-up → automatic email verification code Sign-in → protected dashboard Global AuthContext + useAuth() hook ProtectedRoute component Clean, modern UI using shadcn/ui Zero security anti-patterns Ready? You are now exactly on the same path that AWS solutions architects, startups, and enterprises use daily in 2025.\nNext Step: 5.2 Prerequisites → Set up your AWS account, Node.js, and development environment\n"},{"uri":"https://rsshive.github.io/fcj-workshop/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"How to size an Amazon FSx for NetApp ONTAP file system By Victor Munoz and Sumaja Kapa on 15 AUG 2025 in Advanced (300), Amazon FSx, Amazon FSx for NetApp ONTAP, Customer Solutions, Storage, Technical How-to\nUnderstanding Amazon FSx for NetApp ONTAP Transitioning your business systems and data to the cloud can seem complex, especially if you are new to cloud file storage services. Whether you are moving from traditional on-premises enterprise storage or just getting started in the cloud, sizing your storage correctly is key to avoiding future issues. The right approach enables you to move your files to the cloud even if the cloud is new territory for you and your team.\nAWS offers storage solutions such as Amazon FSx for NetApp ONTAP, which provides fully managed shared storage in the AWS Cloud with the popular data access and management capabilities of ONTAP. Native features such as deduplication, compression, and tiering mean that FSx for ONTAP enables you to optimize storage efficiency and costs as data scales. Starting with the right sized FSx for ONTAP file system based on your workload\u0026rsquo;s needs allows you to maximize built-in data optimization to minimize the necessary capacity and cost. The flexibility and elasticity of FSx for ONTAP allows your storage to adjust with your workload over time, avoiding manual management. Proper planning and use of AWS file services allow you to smoothly and cost-effectively transition file storage to the cloud.\nIn this post, we walk through how to determine the right storage capacity when moving to cloud storage with FSx for ONTAP. This shrinks your overall storage footprint, saving substantial costs as data grows. The savings apply across use cases such as file shares, repositories, backups, and analytics. Seamless tiering allows easy scaling on-demand. Furthermore, FSx for ONTAP data efficiency capabilities enable you to supercharge your storage, confidently retaining information ready for use. Moreover, you can scale for capacity with FSx for ONTAP, as well as size for throughput by finding baseline metrics, project growth, use compression/deduplication, and scale performance to meet demands cost-effectively. We provide sizing best practices to maximize the value of your data as needs evolve. You can still scale your file system later if you need to make changes, offering flexibility as your requirements change. However, while you can increase storage capacity, you cannot shrink the solid-state drive (SSD) storage when it\u0026rsquo;s provisioned.\nData efficiencies FSx for ONTAP provides the ability to tier data between two storage classes: a high-performance SSD tier and a lower-cost fully-elastic Capacity Pool tier. FSx for ONTAP tiering enables seamlessly moving infrequently accessed data from the SSD tier to the more cost-effective Capacity Pool tier automatically based on defined policies. The key benefit is optimized storage costs by tiering colder data to cheaper Capacity Pool storage, while retaining hot active data on high-performance SSDs.\nFSx for ONTAP offers sub-millisecond file operation latencies with SSD storage and tens of milliseconds of latency for Capacity Pool storage. It also has two layers of read caching on each file server—non-volatile memory express (NVMe) drives and in-memory—to provide even lower latencies when accessing frequently-read data. For more information on these specifications, refer to this Amazon FSx documentation.\nThin provisioning All volumes are by default thin provisioned, meaning they consume storage capacity only for the data stored, making them cost efficient. End users see the capacity that they requested while the filesystem only reports the data consumed.\nDeduplication, compression, and compaction Deduplication eliminates redundant data by replacing duplicates with references, which significantly reduces storage needs. Compression is the process of reducing the size of data by encoding it using fewer bits than the original representation. Combined, deduplication and compression provide major data reduction, which improves storage efficiency. FSx for ONTAP brings efficiency to users by eliminating redundant information, keeping capacity needs low, thus keeping costs low. ONTAP uses a 4 KB data block size. However, some applications perform small writes that may not fully use the entire data block. To optimize storage usage, compaction packs multiple compressed and deduplicated data chunks into a single storage block, further making sure of efficient use of the underlying storage. This results in reduced raw storage requirements, translating to lower storage costs.\nExample: storage efficiencies Figure 1: shows how storage efficiencies are applied to data.\nFigure 1: Storage efficiencies graph\nThe preceding graph breaks down the workflow of compression, deduplication, and compaction step by step. First, data is represented as different block sizes as sent by the client to the FSx for ONTAP volume. Depending on the source of the data, block size could be different depending on the originating write. The second step shows how data would land within the FSx for ONTAP volume, having changed the block size to 4 KB for all received data. The third step shows the effects of compression by reducing the space used within the 4 KB block, leaving empty space on each 4 KB block. The fourth step applies deduplication by identifying and eliminating redundant 4 KB blocks, thus replacing duplicates with references to a single copy of the data. The final step has compaction applied by efficiently using all of the empty space to combine as much compressed and deduplicated data as possible within the blocks to release the now empty 4 KB blocks. All of these storage efficiency processes occur inline before the data is written to SSD storage.\nTiering When migrating data to FSx for ONTAP, users must analyze the ratio of cold (infrequently accessed) and hot (frequently accessed) data in their existing on-premises environment. To plan for tiering, users should identify their hot and cold data ratios by doing the following:\nAnalyzing file access logs and storage analytics tools to determine frequently (hot) and infrequently (cold) accessed files. Manually inspecting file metadata like last access times. Gathering input from application owners and users on data access patterns. Setting up pilot FSx for ONTAP deployments to monitor access patterns. Regularly reviewing these data access patterns allows users to accurately size the SSD tier for their FSx for ONTAP file systems, thereby optimizing for performance and cost.\nTiering happens transparently in the background according to the tiering policies that you set manually on volumes. There are four options for volume tiering:\nAuto: This policy moves all cold data—user data and snapshots—to the Capacity Pool tier. The cooling rate of data is determined by the policy\u0026rsquo;s cooling period, which by default is 31 days, and is configurable to values between 2–183 days. When the underlying cold data blocks are read randomly (as in typical file access), they are made hot and written to the primary storage tier. When cold data blocks are read sequentially (for example, by an antivirus scan), they remain cold and remain on the Capacity Pool storage tier. Snapshot only: This policy moves only snapshot data to the Capacity Pool storage tier. The rate at which snapshots are tiered to the Capacity Pool is determined by the policy\u0026rsquo;s cooling period, which by default is set to two days, and is configurable to values between 2–183 days. When cold snapshot data are read, they are made hot and written to the primary storage tier. This is the default policy when creating a volume. All: This policy forces all writes to tier immediately to the Capacity Pool. When data blocks are read, they remain cold and are not written to the primary storage tier. When data is written to a volume with the All tiering policy, it is still initially written to the SSD storage tier, and is tiered to the Capacity Pool by a background process. The file metadata always remains on the SSD tier. None: This policy keeps all of your volume\u0026rsquo;s data on the primary storage tier, and prevents it from being moved to the Capacity Pool storage. If you set a volume to this policy after it used any other policy, then the existing data in the volume that was in Capacity Pool storage is moved to SSD storage by a background process as long as your SSD usage is less than 90%. This background process can be sped up by intentionally reading data or by modifying your volume\u0026rsquo;s cloud retrieval policy. When determining the necessary SSD storage capacity for metadata associated with the files planned for storage on the Capacity Pool tier, you should adopt a conservative approach. A recommended ratio is to allocate 1 GiB of SSD storage for every 10 GiB of data intended for the Capacity Pool tier. If you are not familiar with your usage pattern, then you should start with the \u0026ldquo;Snapshot only\u0026rdquo; option.\nThroughput FSx for ONTAP allows you to configure the desired throughput capacity when provisioning your file system. You have the flexibility to modify this throughput capacity at any time based on your workload needs. However, to achieve the maximum specified throughput, your file system configuration must meet certain requirements.\nUsers are billed for the provisioned throughput capacity. Therefore, you should carefully assess performance needs and find the optimal balance between performance and cost. Over-provisioning can lead to unnecessary expenses, while under-provisioning may result in performance bottlenecks.\nUsers should monitor their workload performance and adjust total capacity as needed to make sure that they\u0026rsquo;re getting the performance they need without overpaying for unused capacity. AWS provides tools such as Amazon CloudWatch to help monitor and optimize these settings.\nUse cases Here are some common workload types that can benefit from the data efficiency capabilities of FSx for ONTAP:\nFile shares: Corporate file shares such as home directories and shared team drives contain lots of redundant documents and files. FSx for ONTAP deduplication saves significant storage space. Backups: Backup data contains a lot of redundancy across versions that can be deduplicated. FSx for ONTAP makes retention of backups more efficient. Source code repositories: Software repositories have duplicative code across versions and branches. FSx for ONTAP reduces the repo storage footprint. Media archives: Media libraries with video footage, images, and audio have high visual and signal redundancy. FSx for ONTAP compression reduces media archive sizes. Electronic design automation: Design files for semiconductors and electronics have duplicate blocks. FSx for ONTAP deduplicates these redundant design components. Big data analytics: Raw data ingested for analytics creates duplicates during ETL preprocessing. FSx for ONTAP saves space before loading into HDFS. Genomics research: Genomic sequencers generate datasets with duplicative DNA data. FSx for ONTAP makes retention of these large research datasets more affordable. The FSx for ONTAP data reduction capabilities enables these diverse workloads to optimize storage efficiency and reduce costs as data volumes grow. The following table gives estimates on data efficiencies that FSx for ONTAP can achieve.\nType of workload Compression only Deduplication only Compression + deduplication General-purpose file shares (home directories) 50% 30% 65% Virtual machines and desktops 55% 70% 70% Databases 65-70% 0% 65-70% Engineering data (EDA type workloads) 55% 30% 75% Assessing your workload Before you can size your FSx for ONTAP file system, you must understand the characteristics of your on-premises workload. This includes gathering information about the following:\nData volume: Determine the total amount of data that you need to store, such as both active and inactive data. Performance requirements: Assess the expected throughput, Input/Output – Operations per Second (IOPS), and latency requirements for your workload. FSx for ONTAP provides sub-millisecond file operation latencies with SSD storage, and tens of milliseconds of latency for Capacity Pool storage. Growth patterns: Analyze the historical growth patterns of your data to estimate future storage and performance needs. Access patterns: Understand how your users and applications interact with the data, such as the frequency of read and write operations. Step 1: Determine the storage capacity Start by estimating the total storage capacity needed for your workload. This includes not only the current data volume but also any anticipated growth over the lifetime of your file system. Remember to factor in more space for metadata, snapshots, and other file system overhead.\nStep 2: Choose the appropriate file system configuration FSx for ONTAP offers various file system configurations, each with its own set of storage and performance capabilities. Based on your workload requirements, choose the configuration that best fits your needs. This may include considerations such as storage throughput, IOPS, and the number of storage volumes.\nFSx for ONTAP file systems provide high availability and durability across AWS Availability Zones (AZs). Multi-AZ file systems have two file servers in separate AZs, while Single-AZ file systems have one or more file server pairs in the same AZ. Data is automatically replicated across file servers for redundancy. FSx for ONTAP continuously monitors for failures and automatically replaces components, with failover and failback typically within 60 seconds, ensuring continuous data availability.\nWhen choosing between FSx for ONTAP scale-up and scale-out systems, consider your workload requirements. Scale-up systems, with a single high availability pair offering up to 6 GB/s throughput and 200,000 IOPS, are ideal for most standard workloads such as general file sharing and content management. For more demanding, compute-intensive ONTAP workloads such as large-scale electronic design automation, seismic analysis, clustered databases, or high-performance computing applications, opt for scale-out systems. Both options allow seamless migration of ONTAP workloads to AWS without modifying existing applications, tools, or workflows. Choose scale-out when you need performance beyond what a single high availability pair can provide, enabling you to use the ONTAP data management features for an even broader range of high-performance use cases in AWS.\nStep 3: Evaluate storage and performance options FSx for ONTAP provides a range of storage and performance options to choose from. These include different storage types, storage capacity, and throughput/IOPS settings. Carefully evaluate these options to make sure that your file system can deliver the needed performance for your workload.\nStep 4: Plan for scalability One of the benefits of FSx for ONTAP is its ability to scale up as your needs change. Anticipate future growth and plan for the ability to easily expand your file system\u0026rsquo;s storage capacity and performance as needed.\nStep 5: Optimize for cost-effectiveness While making sure that your file system meets your workload requirements, also consider the cost implications. FSx for ONTAP offers various pricing options, such as on-demand deployment that can help you optimize your costs based on your usage patterns. FSx for ONTAP offers a flexible, pay-as-you-go pricing model with no minimum fees or set-up charges. Although prices are quoted monthly, you\u0026rsquo;re billed based on your average hourly usage over the month, making sure that you only pay for what you actually use. The service\u0026rsquo;s pricing structure is composed of six key components: SSD storage, SSD IOPS, Capacity Pool usage, throughput capacity, backups, and SnapLock licensing. These elements allow you to tailor your storage solution to your specific needs, balancing performance, capacity, and cost. Considering these components carefully allows you to optimize your FSx for ONTAP deployment to achieve the best balance of performance and cost-efficiency for your workloads.\nStep 6: Test and validate Before deploying your FSx for ONTAP file system in production, conduct thorough testing and validation to make sure that it meets your performance and data protection requirements. This may involve running benchmark tests, validating data integrity, and monitoring the system\u0026rsquo;s behavior under various workloads.\nDi cư (Migration) Khi phân tích dung lượng cần thiết, hãy cân nhắc tỷ lệ dữ liệu lạnh và nóng. Hầu hết người dùng xác định tỷ lệ như 80/20—80% lạnh và 20% nóng. Tỷ lệ này giúp bạn thiết lập kích thước tầng SSD khi triển khai hệ thống tệp. Để phân biệt dữ liệu nóng hay lạnh, xem xét tần suất truy cập và yêu cầu độ trễ. Dữ liệu nóng cần sẵn sàng ngay lập tức với độ trễ thấp, ví dụ dữ liệu giao dịch thời gian thực hoặc luồng telemetry, thường lưu trên SSD. Ngược lại, dữ liệu lạnh là dữ liệu lịch sử hoặc lưu trữ, truy cập không thường xuyên và độ trễ chấp nhận được, có thể lưu trên Capacity Pool. Hiểu mẫu truy cập và yêu cầu hiệu năng giúp phân loại chính xác và đề xuất cấu hình lưu trữ phù hợp.\nĐể tối ưu chi phí, hãy thiết kế tầng SSD nhỏ nhất có thể đáp ứng nhu cầu. Một số cách tiếp cận:\nChọn tập dữ liệu mục tiêu có kích thước nhỏ hơn dung lượng SSD đang có để đảm bảo hiệu năng và tránh tắc nghẽn. Cách này giúp inline dedupe và nén hoạt động hiệu quả, không làm tăng dung lượng đã sử dụng của hệ thống tệp. This is used in case the dataset being copied to FSx for ONTAP is bigger than the filesystem size. Multiple processes happen in the background for the system to be as efficient as possible. Processes can take approximately 15 days within the SSD tier to be completely deduped and compressed. Set your volume policy accordingly. If the data you are ingesting is meant for the capacity tier or does not need the performance of the SSD tier, then set the volume policy to ALL. Continuously monitor the available capacity in your SSD tier throughout the project by setting up CloudWatch alarms. If the SSD aggregate approaches its maximum capacity, then data tiering is automatically disabled to prevent any further data from being promoted to the SSD tier. This safeguard mechanism makes sure that the SSD aggregate does not completely fill up, which could lead to performance degradation or potential data unavailability. Conclusion In this post we showed how to configure FSx for ONTAP to provide highly available and durable shared file storage for your applications. Following the steps outlined in this post enables you to navigate the sizing process with confidence and make sure that your file system is optimized to meet the needs of your on-premises workloads, thereby maximizing performance and efficiency while minimizing unnecessary overhead.\nTAGS: Amazon FSx for NetApp ONTAP, AWS Cloud Storage\nVictor Munoz Victor Munoz is a Senior Storage Solution Architect with extensive experience in driving innovation in enterprise storage infrastructure and data management. He has held Engineering and architecture roles focused on next-generation storage platforms, data protection, and cloud integration at leading technology companies. Victor is passionate about helping organizations modernize their storage environments by leveraging cloud-native capabilities for scalability, resilience, and efficiency.\nSumaja Kapa Sumaja Kapa is a Storage Solution Architect covering all AWS Storage services. She has experience with multiple different Storage and Backup solutions. Sumaja uses this expertise to solve user requirements, architecting the most cost effective, durable, scalable, and available storage solutions on AWS. Her technical knowledge paired with her user-centric approach enables Sumaja to engineer innovative cloud-based storage environments that optimize for the diverse needs of businesses.\n"},{"uri":"https://rsshive.github.io/fcj-workshop/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Introducing default instance categories for AWS Batch By Angel Pizarro on August 18, 2025 in AWS Batch, High Performance Computing (HPC) | Permalink Today we are launching a new set of instance family classifications for AWS Batch, including \u0026ldquo;default_x86_64\u0026rdquo; and \u0026ldquo;default_arm64\u0026rdquo;. These new categories are both a clarification and improvement over the existing \u0026ldquo;optimal\u0026rdquo; instance type category. This post provides some context about the new feature and how you can configure Batch environments to take advantage of these improvements.\nChoosing instance types for AWS Batch Compute Environments to launch You can specify the set of Amazon EC2 instance types that a compute environment is allowed to launch to run the jobs in your job queues. For example, if you know that your jobs achieve the best price/performance on g6.16xlarge, you can set the computeResources.instanceTypes parameter of the compute environment to [\u0026ldquo;g6.16xlarge\u0026rdquo;], and that environment will only launch exactly this instance type and size for jobs in the associated queue(s).\nTo take full advantage of Batch\u0026rsquo;s scheduling and scaling capabilities, you should define a diverse set of instances and let Batch decide which type to launch based on the CPU, memory, and GPU resource requirements of your jobs. While you can specify a list of instance types (e.g., [\u0026ldquo;c5.24xlarge\u0026rdquo;, \u0026ldquo;m6.48xlarge\u0026rdquo;]), you can also define a set of instance families (e.g., [\u0026ldquo;c5\u0026rdquo;,\u0026ldquo;c7a\u0026rdquo;,\u0026ldquo;c7i\u0026rdquo;,\u0026ldquo;m7i\u0026rdquo;]) for Batch to launch on your behalf. Previously, you also had the option to use the \u0026ldquo;optimal\u0026rdquo; category, which Batch would map to [\u0026ldquo;c4\u0026rdquo;,\u0026ldquo;m4\u0026rdquo;,\u0026ldquo;r4\u0026rdquo;] when those instances were available in your AWS Region. If a Region did not have those instance types, Batch would map optimal to the lowest-cost generation in the Region, typically [\u0026ldquo;c5\u0026rdquo;,\u0026ldquo;m5\u0026rdquo;,\u0026ldquo;r5\u0026rdquo;].\nWhile optimal is a convenient shorthand setting, we have found that it does not align with customer expectations. Batch does not select instances for the best performance of your job or the best price for your application. The optimal category is just a simple mapping and nothing more. Newer instance generations are likely to deliver better price/performance for most workloads. In all cases I have examined, 4th generation instances are both slower and more expensive than 5th generation, so optimal is actually not \u0026ldquo;optimal\u0026rdquo; at all!\nFinally, optimal does not include instances using AWS Graviton processors, which are purpose-built to deliver superior price/performance for many types of workloads.\nUpgrading from the \u0026ldquo;optimal\u0026rdquo; configuration Today, we have launched a new set of instance type categories to improve your experience with Batch\u0026rsquo;s instance selection feature. You can now choose default_x86_64 for a set of instance families using x86 CPUs, and default_arm64 for a set of instances using AWS Graviton CPUs. The list of instance types will not be static and will change over time as we introduce new instance families. You can find the mapping between each category and the set of instance families by Region in the Instance type compute table documentation page; for example, the default_arm64 mapping at the time of feature launch will be as follows:\nRegions Instance families All AWS Regions supporting AWS Batch m6g, c6g, r6g c7g Table 1. Mapping of the default_arm64 instance type category to specific instance families in Regions supporting AWS Batch.\nThe default mappings do not include accelerated instances by design. We strongly recommend you specify instance types explicitly if your workload benefits from accelerated instances. Additionally, we recommend defining a separate job queue and compute environment from your clusters of non-accelerated instances. When resources are separated, Batch can make better scaling decisions for your workload and prevent scheduling non-accelerated jobs onto accelerated instances, which could lead to unnecessary costs.\nSimilar to previously with optimal, you can still define additional instance types alongside the default_* categories. Batch will consider the entire list you have declared when selecting instance types to run jobs.\nTransitioning from optimal We recommend that you update your Batch environments to adopt the new categories. However, you are not required to update. AWS Batch compute environments currently using optimal remain valid. The CreateComputeEnvironment and UpdateComputeEnvironment API operations still accept optimal as a valid value. The behavior of optimal will remain unchanged until early November 2025. After that time, optimal will behave identically to the default_x86_64 instance type category.\nOnce again, after early November 2025, optimal will no longer be a static mapping to 4th generation instances (if available) and will change over time as AWS introduces new instance families. If you want to maintain the current instance set for optimal, you should update your compute environment(s) to explicitly specify those instance types.\nWe recommend that all new compute environments you create use the new default_* categories. Personally, I also recommend you test whether your application can benefit from Graviton by building an Arm container and defining a compute environment and job queue using default_arm64. Most likely you will get a desirable result!\nConclusion We encourage you to try the new default_* instance type categories. We believe that gradually adopting newer instance generations over time will help your workload operate efficiently and cost-effectively. To start using the new categories, log in to the AWS Batch management console, or refer to the guide for creating a managed EC2 compute environment in our User Guide.\nAngel Pizarro is a Principal Developer Advocate for HPC (High-Performance Computing) and computational science. He has a background in developing bioinformatics applications and building system architectures for scalable computing in genomics and other high-throughput life science domains.\n"},{"uri":"https://rsshive.github.io/fcj-workshop/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Preventing machine breakdowns: How Physical AI predicts equipment problems By Ram Gorur, Ashish Chaurasia, and Channa Samynathan on August 15, 2025 | in Amazon Bedrock Agents, Amazon Bedrock Guardrails, Amazon Machine Learning, Amazon SageMaker AI, Artificial Intelligence, Automotive, AWS IoT Core, AWS IoT FleetWise, Generative AI, Manufacturing, Technical How-to\nPhysical AI: Intelligence that acts in the real world Physical AI differs from traditional AI in that it directly interacts with and controls the physical world. While traditional AI processes data and generates text on screens, Physical AI enables robots, autonomous vehicles, and intelligent systems to sense, understand, and act in real multidimensional environments.\nThe key difference: Physical AI understands spatial relationships and physical behaviors through training on synthetic and real-world data, bridging the gap between digital intelligence and physical action.\nHow it works: High-fidelity computer simulations create digital twins of real spaces like factories and streets, where virtual sensors and machinery that mirror real-world physics are used to train specialized models.\nTransforming maintenance Physical AI transforms maintenance from reactive to autonomous. These systems sense their environment, understand the relationships between components, and take preventive actions before problems occur. The Predictive Maintenance (PdM) market in automotive will reach billion by 2032, a revolution in vehicle care driven by Physical AI capabilities.\nElectric vehicles (EVs) are a prime example of where Physical AI can be applied. They can be designed to continuously learn from their surroundings, make instant decisions to optimize performance, and manage their own health while in motion. These systems understand how parts mesh and work together, predict how physical forces will impact different components, and adjust driving patterns to reduce wear.\nThe same principles behind PdM in automotive also appear in other fields. Manufacturing robots now predict and prevent equipment failures before they occur. In smart warehouses, systems schedule private maintenance for maximum efficiency. Medical robots monitor accuracy and self-calibrate when needed. Even smart infrastructure can detect its own problems and coordinate automatic repairs.\nHow does it actually work? Physical AI systems in modern electric vehicles represent an advanced approach to vehicle monitoring and maintenance through an integrated sensor network that continuously analyzes multiple vehicle systems. These systems monitor battery health, motor performance, brakes, and suspension system components while building dynamic models of component interactions. AI monitors relationships between temperature, vibration, electrical load, and mechanical stress to predict and prevent potential failures. The system takes proactive measures such as adjusting charging patterns to reduce battery stress and modifying regenerative braking to minimize wear. This predictive maintenance approach transforms traditional reactive vehicle maintenance into a proactive system that understands and responds to real-world conditions, although specific performance metrics and outcome data are needed to quantify benefits.\nOverview In this blog, you\u0026rsquo;ll learn about the different types of generative AI applications that are transforming PdM supporting Physical AI and how AWS services enable these innovations.\nAWS Internet of Things (IoT), Artificial Intelligence (AI)/Machine Learning (ML), and generative AI have transformed the connected vehicle landscape, and more specifically electric vehicles, by providing innovative solutions for PdM supporting Physical AI. The integration of these advanced technologies has paved the way for a more efficient approach to electric vehicle maintenance, ensuring optimal performance and longevity through deep understanding of physical systems.\nAWS IoT is used by many automotive customers to develop and manage Physical AI applications (autonomous driving, predictive maintenance, entertainment, etc.). AWS IoT enables electric vehicles to connect to the cloud and transmit real-time data about condition and performance, including spatial relationships and physical interactions between components. This data is then analyzed using AWS AI/ML services that can identify patterns, detect anomalies, and predict potential issues by understanding the physics of how different systems interact in the real world.\nGenerative AI in PdM supporting Physical AI operates through four main stages: Machine prioritization uses retrieval-augmented generation (RAG) systems to analyze structured and unstructured maintenance data, identifying which equipment needs priority. Failure prediction processes machine sensor data through real-time analysis and ML models to predict failures before they occur. Repair plan generation leverages large language models to create comprehensive work orders with instructions and resource allocation by integrating data from multiple sources. Maintenance guide generation combines service notes and repair plans using generative AI to provide enhanced actionable guidance for technicians.\nThis approach enables automotive manufacturers to collect rich data about vehicle performance under real physical conditions, improve future vehicle design by understanding how vehicles interact with the physical environment, and make informed decisions about component improvements that account for physics and actual usage patterns.\nArchitecture overview PdM in electric vehicles requires monitoring, analysis, and action based on collected information. Electric vehicles are equipped with multiple sensors that collect data about battery health, vehicle location, motor health, brakes, and more. To reduce operating costs, this model aims to enhance electric vehicle maintenance by using sensor data to create PdM models.\n1. Data collection and processing Connected vehicles bring opportunities for manufacturers to enhance vehicle quality, safety, and autonomy. However, these advances come with challenges, particularly managing and effectively leveraging the massive volume of data from connected vehicles. The data collection task is complex due to the diverse proprietary data formats of electronic control units (ECUs) and the significant costs associated with scaling data collection operations.\nAWS IoT FleetWise is a purpose-built service for the automotive industry. It enables you to easily collect, transform, and transmit vehicle data from many different formats regardless of make, model, or options. The service standardizes data formats, making cloud-based analysis easier without custom data collection systems. With AWS IoT FleetWise, you can transmit data to the cloud in near-real-time using intelligent filtering capabilities. By selecting transmitted data and defining rules, events based on parameters like weather conditions, location, or vehicle type, you can reduce the amount of data sent to the cloud.\nIn this section, we will use AWS IoT FleetWise to collect and store vehicle data in S3 for training machine learning models for predictive analytics.\nSet up AWS IoT FleetWise Edge Agent on vehicle Create Edge Agent for AWS IoT FleetWise to create connection between vehicle and cloud. Edge Agent is fully functional embedded software written in C++ designed to collect vehicle data, capable of running on most embedded Linux platforms. IoT FleetWise controls what data is collected and transmitted by the Edge Agent from the vehicle. Create signal catalog Signals structure vehicle data and metadata into distinct types: Sensors record real-time measurements like temperature, storing the name, data type, and unit of each signal. Attributes contain fixed information like manufacturer and production date. Branches create hierarchical organization Vehicle branches into Powertrain, containing combustionEngine sub-branch. Sensor data tracks instantaneous vehicle state including fluid levels, temperatures, and vibrations. Actuator data controls device state for components like engines and door locks. When you adjust a device like turning on a heater you update its actuator data. The signal catalog simplifies vehicle modeling with predefined signals. AWS IoT FleetWise integrates Vehicle Signal Specification (VSS), defining standard signals like \u0026ldquo;vehicle_speed\u0026rdquo; in km/h. This central repository of sensors and standard signals accelerates new vehicle model creation through efficient signal reuse.\nCreate vehicle model You use signals to establish standardized vehicle models that normalize vehicle formats. Vehicle models ensure uniform data across multiple vehicles of the same type, enabling efficient processing from fleets. Vehicles created from the same model inherit a consistent signal set.\nCreate decoder manifest Decoder manifest contains decoding information that AWS IoT FleetWise uses to translate binary vehicle data into understandable values. IoT FleetWise supports OBD II, CAN bus, and vehicle middleware like ROS2. For example, if a vehicle uses OBD network interface, the decoder manifest should include signal associations with message ID 11 and binary data like 000011 with OBDCoolantTemperature. Create vehicles Vehicles are instances of vehicle model. Vehicles must be created from vehicle model and associated with decoder manifest. Vehicles upload one or more data streams to the cloud. For example, a vehicle might send mileage, battery voltage, and heater status to the cloud. Create and deploy campaign to collect vehicle data After modeling vehicles and creating signal catalog, you can create data collection campaigns using signals created in the model. Campaigns are orchestrations of data collection rules. Campaigns provide Edge Agent for AWS IoT FleetWise with instructions on how to select, collect, and transmit data to the cloud. All campaigns are created on the cloud. After campaigns are marked as approved by team members, AWS IoT FleetWise automatically deploys them to vehicles. Automotive teams can choose to deploy campaigns to specific vehicles or fleets. The Edge Agent software will not begin collecting vehicle network data until running campaigns are deployed to the vehicle. Store vehicle data in S3 Edge Agent software for AWS IoT FleetWise transmits selected vehicle data to Amazon Timestream or Amazon Simple Storage Service (Amazon S3). After data reaches its destination, you can use other AWS services to visualize and share. 2. Training PdM model Machine Learning (ML) algorithms are used here to perform PdM analysis to predict equipment failures and optimize maintenance operations. PdM uses real-time data to analyze factors correlated with electric vehicle failures, thereby predicting the likelihood of failure occurrence. This proactive approach can effectively minimize unplanned vehicle breakdowns, extend component lifespan, and reduce total repair costs.\nWhen electric vehicle data is brought into the AWS environment, it is stored in Amazon S3 bucket. This data is then used to create real-time predictions from trained and deployed ML models. Predictions can be further processed and used by downstream applications to take necessary actions and initiate PdM operations. The solution includes the following parts:\nTrain and deploy model We use PdM dataset from Data Repository to train machine learning model with XGBoost algorithm using SageMaker. Then, deploy trained model to SageMaker asynchronous inference endpoint. Train model To train model, we first store electric vehicle data in Amazon S3. This enables secure and efficient storage of large amounts of data. After data is stored, we begin training process using Amazon SageMaker Training. This service is designed to handle training machine learning models at scale. Its capabilities enable fast and accurate model training, even with large datasets, ensuring efficient and effective model training, leading to high-quality results. Collect near-real-time electric vehicle data Electric vehicle data is collected from vehicles and processed in AWS environment before storage in Amazon S3. This data includes critical parameters like battery voltage, battery temperature, motor health, location, etc. Then, Amazon Lambda function is triggered to invoke Amazon SageMaker asynchronous endpoint. Perform near-real-time PdM Amazon SageMaker asynchronous endpoint is used to generate inferences from deployed model for incoming electric vehicle data. These endpoints are particularly suitable for PdM workloads because they support larger payload sizes and can generate inferences in minutes. Inferences from the model are stored in Amazon S3. They can be applied to create dashboards, visualizations, and perform generative AI tasks. To ensure an effective Predictive Maintenance solution at scale, implement robust training and deployment pipelines by referencing AWS Well-Architected Framework principles for machine learning.\n3. Generative AI Create AWS Glue Data Catalog using AWS Glue crawler (or other method). Using Titan-Text-Embeddings model on Amazon Bedrock, convert metadata to embeddings and store in Amazon OpenSearch Serverless vector store, serving as knowledge base in RAG framework. At this stage, the process is ready to receive queries in natural language. Users enter queries in natural language. You can use any web application to provide chat interface. Therefore, we don\u0026rsquo;t cover UI details in this post. Solution applies RAG framework through similarity search, adding context from metadata from vector database. This table is used to find correct table, database, and attributes. Model receives generated SQL query and connects to Athena to validate syntax. Finally, run SQL using Athena and generate output. Output is presented to user. For architecture simplification, we don\u0026rsquo;t show this step. Conclusion The convergence of Generative AI and Physical AI is reshaping condition-based and predictive maintenance across industries. As explored, generative AI\u0026rsquo;s capabilities in analyzing large datasets, generating synthetic training scenarios, and providing intelligent recommendations are transforming how Physical AI systems monitor, diagnose, and self-maintain. From electric vehicles predicting battery degradation to industrial robots self-scheduling maintenance, we\u0026rsquo;re witnessing a paradigm shift where intelligent systems don\u0026rsquo;t just perform tasks they proactively preserve and optimize their own operational capabilities.\nReferences NVIDIA: What is Physical AI? Predictive maintenance: When a machine knows in advance that repairs are needed Well-Architected machine learning Build a robust text-to-SQL solution Global Automotive Predictive Maintenance Market About the authors Ram Gorur is a Senior Solutions Architect at AWS, specializing in Agriculture and Consulting Services, with a focus on Edge AI and Connected Products. Based in Virginia, he leverages over 23 years of comprehensive IT experience to help AWS enterprise customers deploy IoT solutions spanning from edge devices to cloud infrastructure. His expertise encompasses designing and deploying connected product solutions across diverse industries, where he develops custom architectural frameworks connecting edge computing with cloud capabilities.\nAshish Chaurasia is a Senior Technical Account Manager at AWS, having collaborated with enterprise customers since 2020 to align cloud technologies with strategic business outcomes. With over 17 years of software development experience, he specializes in guiding organizations through cloud-native transformation journeys. Ashish is an IoT enthusiast and enjoys building DIY projects to automate daily tasks.\nChanna Samynathan is a Senior Global Specialist Solutions Architect for AWS Edge AI \u0026amp; Advanced Compute. With over 29 years of experience in the technology industry, Channa has held diverse roles including design engineer, system tester, operations, business consultant, and product manager. His career spans multiple multinational telecommunications companies, where he consistently demonstrated expertise in sales, business development, and technical solution design. Channa\u0026rsquo;s global experience, having worked in over 26 countries, has equipped him with deep technical knowledge and ability to rapidly adapt to new technologies.\n"},{"uri":"https://rsshive.github.io/fcj-workshop/5-workshop/5.3-aws-cognito/5.3.1-create-cognito/","title":"Create a Cognito User Pool","tags":[],"description":"","content":"Create a Cognito User Pool In this section, you will create a Cognito User Pool using two methods:\nAWS Management Console (GUI) AWS CLI (Command Line Interface) Choose either method depending on your preference or environment.\n1. Create User Pool via AWS Console This is the standard method using the AWS web interface.\nStep 1 — Open Cognito Console Go to:\nhttps://console.aws.amazon.com/cognito\nStep 2 — Create a New User Pool Click Create user pool Under Define your application, choose\nTraditional web application Step 3 — Configure User Pool Details User Application name: My web app - Cognito Step 4 — Sign-in Options Select: Email (Users will log in using email addresses)\nStep 5 — Required Attributes Required attributes:\nemail\nStep 6 — Create User Pool Click Create user directory to fin\n2. Create User Pool via AWS CLI This option is useful if:\nYou want automation,\nYou are building IaC pipelines,\nOr you prefer command-line setup.\nStep 1 Open powershell on AWS web and Create User Pool aws cognito-idp create-user-pool \\ --pool-name \u0026#34;my-userpool\u0026#34; \\ --auto-verified-attributes email \\ --username-attributes email This command will:\nCreate a new user pool named my-userpool\nEnable email as the username\nEnable email auto-verification\nStep 2 — Create an App Client Run:\naws cognito-idp create-user-pool-client \\ --user-pool-id \u0026lt;YOUR_USERPOOL_ID\u0026gt; \\ --client-name \u0026#34;my-app-client\u0026#34; \\ --generate-secret \\ --no-prevent-user-existence-errors \\ --allowed-o-auth-flows-user-pool-client \\ --allowed-o-auth-flows code \\ --allowed-o-auth-scopes \u0026#34;email\u0026#34; \u0026#34;openid\u0026#34; \\ --callback-urls \u0026#34;http://localhost:3000\u0026#34; \\ --logout-urls \u0026#34;http://localhost:3000\u0026#34; Replace:\n\u0026lt;YOUR_USERPOOL_ID\u0026gt;\nwith the value returned from Step 1.\nStep 3 — Output Example A successful creation returns JSON: { \u0026#34;UserPool\u0026#34;: { \u0026#34;Id\u0026#34;: \u0026#34;ap-southeast-1_AbCdEf123\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;my-userpool\u0026#34; } } And:\n{ \u0026#34;UserPoolClient\u0026#34;: { \u0026#34;ClientId\u0026#34;: \u0026#34;4ab5exampleid123\u0026#34;, \u0026#34;ClientSecret\u0026#34;: \u0026#34;xyzexamplesecret\u0026#34; } } Completed\nYou now have a fully configured Cognito User Pool created via:\nAWS Console, or\nAWS CLI\nNavigation:\nPrevious: 5.3 AWS Cognito Setup Next Step: 5.3.2 Configure App Client → Configure application client settings "},{"uri":"https://rsshive.github.io/fcj-workshop/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AI-Driven Development Life Cycle: Reimagining Software Engineering\u0026rdquo; Event Information Event: AWS GenAI Builder Club Session Date: Friday, October 3rd, 2025 Time: 2:00 PM - 4:30 PM Location: AWS Event Hall, L26 Bitexco Tower, Ho Chi Minh City Organizer: AWS GenAI Builder Club Event Objectives Explore the transformative impact of generative AI on software development Demonstrate how AI reimagines the entire development lifecycle (learn, plan, create, deploy, manage) Showcase AI tools that automate undifferentiated heavy lifting tasks Enable developers to focus on higher-value, creative work Introduce Amazon Q Developer and Kiro for AI-driven development Instructors \u0026amp; Coordinators Instructors:\nToan Huynh – Amazon Q Developer Specialist My Nguyen – Kiro Specialist Coordinators:\nDiem My Dai Truong Dinh Nguyen Event Agenda Time Activity Speaker 2:00 PM - 2:15 PM Welcoming \u0026amp; Introduction Organizers 2:15 PM - 3:30 PM AI-Driven Development Life Cycle Overview \u0026amp; Amazon Q Developer Demonstration Toan Huynh 3:30 PM - 3:45 PM Coffee Break \u0026amp; Networking - 3:45 PM - 4:30 PM Kiro Demonstration My Nguyen Key Highlights The Transformative Shift: Generative AI in Software Development The rise of generative AI marks a transformative shift in software development, fundamentally reimagining how developers and organizations:\nLearn: AI-assisted knowledge acquisition and skill development Plan: Intelligent project scoping and architecture design Create: Code generation and automated development Deploy: Streamlined deployment processes Manage: Intelligent application monitoring and maintenance securely By integrating AI into the software development lifecycle—from architecture to development, testing, deployment, and maintenance—developers can automate undifferentiated heavy lifting tasks. This automation increases productivity and enables developers to focus on higher-value, creative tasks.\nAI Integration Across the Development Lifecycle 1. Architecture Phase\nAI-powered architecture recommendations Best practice suggestions based on AWS Well-Architected Framework Automatic security and compliance checks Cost optimization guidance 2. Development Phase\nAutomated code generation from natural language descriptions Intelligent code completion and suggestions Real-time bug detection and fixes Code refactoring recommendations Documentation generation 3. Testing Phase\nAutomated test case generation Intelligent test coverage analysis Performance testing and optimization suggestions Security vulnerability scanning 4. Deployment Phase\nAutomated deployment workflows Infrastructure as Code (IaC) generation CI/CD pipeline optimization Automated rollback and recovery mechanisms 5. Maintenance Phase\nPredictive issue detection and alerts Automated documentation updates Continuous optimization recommendations Log analysis and troubleshooting assistance Amazon Q Developer: AI-Powered Development Assistant Key Features:\nConversational Interface: Natural language interaction for coding tasks Code Generation: Transform requirements into working code Code Explanation: Understand complex codebases quickly Bug Fixing: Automatic identification and resolution of issues Test Generation: Create comprehensive test suites automatically Security Scanning: Built-in security analysis and remediation Code Transformation: Modernize legacy code (Java upgrade, .NET modernization) Use Cases Demonstrated:\nBuilding serverless applications from scratch Migrating legacy code to modern frameworks Optimizing AWS resource configurations Generating API documentation automatically SDLC automation from planning to maintenance Kiro: Advanced AI Development Tool Key Capabilities:\nEnhanced code intelligence and understanding Context-aware suggestions across multiple files Integration with popular IDEs and development tools Team collaboration features Custom model training for specific domains Demonstration Highlights:\nReal-time code collaboration scenarios Complex refactoring operations Multi-file code generation Integration with existing development workflows Key Takeaways AI-Driven Development Mindset Productivity Amplification: AI tools can automate 30-50% of repetitive coding tasks Focus Shift: Developers can concentrate on creative problem-solving and architecture design Learning Acceleration: AI assistants help understand new codebases and technologies faster Quality Improvement: Automated testing and security scanning reduce bugs and vulnerabilities Practical AI Integration Start Small: Begin with code completion and documentation generation Build Confidence: Gradually adopt more advanced features like code generation Review AI Outputs: Always validate and understand AI-generated code Continuous Learning: AI tools improve with feedback and usage patterns Development Workflow Transformation Natural Language to Code: Describe what you want, let AI generate initial implementation Pair Programming with AI: Use AI as a collaborative coding partner Automated Code Reviews: AI can identify potential issues before human review Knowledge Sharing: AI helps document tribal knowledge and best practices Applying to Work Immediate Actions Integrate Amazon Q Developer into IDE for daily development tasks Use AI for Documentation: Automatically generate and update technical documentation Automate Testing: Let AI create test cases for existing code Code Modernization: Use AI to identify and refactor legacy code patterns Project Applications Accelerate Feature Development: Use AI code generation for boilerplate and common patterns Improve Code Quality: Implement AI-powered code reviews and security scans Enhance Team Productivity: Share AI tools and best practices across the team Reduce Technical Debt: Use AI to identify and prioritize refactoring opportunities Event Experience Attending the “GenAI-powered App-DB Modernization” workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\nEvent Experience Attending the \u0026ldquo;AI-Driven Development Life Cycle\u0026rdquo; session by AWS GenAI Builder Club was an eye-opening experience that fundamentally changed my perspective on software development. Key experiences included:\nLearning from Industry Experts Toan Huynh provided comprehensive insights into Amazon Q Developer\u0026rsquo;s capabilities with live demonstrations My Nguyen showcased Kiro\u0026rsquo;s advanced features through practical examples Both speakers shared real-world use cases from production environments at scale Hands-on AI Tool Demonstrations Witnessed Amazon Q Developer generate a complete serverless application from natural language description Saw how AI can refactor complex legacy code into modern, maintainable patterns in minutes Learned about automated test generation that achieves 80%+ code coverage automatically Observed real-time security vulnerability detection and automated fixes Understanding AI Capabilities and Limitations AI excels at boilerplate code, documentation, and common patterns Human oversight remains crucial for architectural decisions and business logic AI tools improve through feedback loops and continuous learning Best results come from combining AI capabilities with developer expertise Networking and Knowledge Sharing Connected with other AWS GenAI Builder Club members working on similar challenges Exchanged experiences about integrating AI tools into existing development workflows Discussed strategies for team adoption of AI-assisted development Shared concerns about code quality, security, and maintainability when using AI Practical Insights for OJT Project Realized I could use Amazon Q Developer to accelerate my movie recommendation chatbot development Identified opportunities to automate Lambda function generation for TMDB API integration Learned how AI can help with API documentation and test case creation Discovered techniques for optimizing AWS resource configurations using AI suggestions Key Learnings Generative AI is transforming software development, not replacing developers Early adoption of AI tools provides significant competitive advantages Productivity gains of 30-50% are achievable with proper AI tool integration AI helps developers learn faster and work smarter, not just faster The future of software engineering involves human-AI collaboration, not competition Immediate Actions Post-Event Installed Amazon Q Developer extension in my IDE Started using AI for code documentation in my OJT project Experimented with AI-generated test cases for Lambda functions Shared learnings with my internship team Planned integration of AI tools into our development workflow Some event photos Add your event photos here\nThis session was not just about learning new toolsit was about reimagining how we approach software development. The hands-on demonstrations showed that AI-driven development is not a distant future concept but a present reality that we can leverage today to build better software faster and more efficiently.\n"},{"uri":"https://rsshive.github.io/fcj-workshop/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: \u0026ldquo;Data Science On AWS Workshop\u0026rdquo; Event Information Event: Data Science On AWS Workshop Date: Thursday, October 16th, 2025 Time: 9:30 AM - 11:45 AM Location: Hall Academic, FPT University Organizers: AWS Community Builders Event Objectives Explore the complete journey of building a modern Data Science system on AWS Demonstrate practical implementation from theory to hands-on practice Understand the importance of Cloud in Data Science workflows Learn AWS services for data processing, ML training, and model deployment Compare Cloud vs. On-premise solutions for cost and performance Speakers Văn Hoàng Kha – Cloud Solutions Architect, AWS Community Builder Bạch Doãn Vương – Cloud Develops Engineer, AWS Community Builder Workshop Agenda Time Activity Speaker 9:30 AM - 9:40 AM Introduction \u0026amp; Importance of Cloud in Data Science Organizers 9:40 AM - 10:05 AM Overview of Data Science Pipeline on AWS (S3, Glue, SageMaker) Văn Hoàng Kha 10:05 AM - 10:35 AM Demo 1: Data Processing and Cleaning from IMDb Dataset with AWS Glue Bạch Doãn Vương 10:35 AM - 11:00 AM Demo 2: Training and Deploying Sentiment Analysis Model with SageMaker Văn Hoàng Kha 11:00 AM - 11:35 AM In-depth Discussion: Cost \u0026amp; Performance (Cloud vs. On-premise) Both Speakers 11:35 AM - 11:45 AM Post-workshop Mini Project Guidance Organizers Key Highlights Introduction: Cloud\u0026rsquo;s Role in Data Science Why Cloud for Data Science?\nScalability: Easily scale compute resources based on workload demands Cost Efficiency: Pay-only-for-what-you-use model eliminates upfront infrastructure costs Accessibility: Access powerful computing resources without physical hardware Collaboration: Team members can work on the same datasets and models from anywhere Faster Time-to-Market: Quick experimentation and iteration without provisioning delays Traditional Challenges Addressed:\nLimited local compute power for large datasets High upfront costs for GPU/TPU hardware Difficulty in collaboration and version control Complex infrastructure management Data Science Pipeline on AWS Complete End-to-End Workflow:\nData Storage (Amazon S3)\nCentralized data lake for raw and processed data Scalable, durable, and cost-effective storage Integration with all AWS data services Data Processing (AWS Glue)\nServerless ETL (Extract, Transform, Load) service Automated data cataloging and schema discovery Built-in data quality and validation Support for various data formats (CSV, JSON, Parquet) Model Development (Amazon SageMaker)\nFully managed ML platform Built-in algorithms and framework support Jupyter notebooks for experimentation AutoML capabilities with SageMaker Autopilot Model Training\nDistributed training for large datasets Automatic hyperparameter tuning Spot instance support for cost optimization Model Deployment\nOne-click deployment to production Auto-scaling endpoints A/B testing and canary deployments Real-time and batch inference options Demo 1: AWS Glue for Data Processing IMDb Dataset Processing:\nDataset Overview:\nMovie reviews and ratings User sentiment labels (positive/negative) Unstructured text data requiring cleaning AWS Glue Operations:\nData Crawlers: Automatically discover and catalog IMDb data schema ETL Jobs: Transform raw text data into structured format Data Cleaning: Remove duplicates, handle missing values, normalize text Feature Engineering: Extract relevant features for sentiment analysis Key Glue Features Demonstrated:\nVisual ETL designer for no-code transformations PySpark scripts for advanced processing Data quality rules and validation Job scheduling and monitoring Results:\nClean, structured dataset ready for ML training Reduced data preparation time by 70% Automated pipeline for future data ingestion Demo 2: SageMaker Sentiment Analysis Model Development Workflow:\n1. Data Preparation in SageMaker:\nLoad processed data from S3 Split into training, validation, and test sets Feature vectorization using TF-IDF or word embeddings 2. Model Selection:\nPre-built algorithm: BlazingText for text classification Custom model: LSTM/Transformer-based sentiment classifier Transfer learning: Fine-tune pre-trained BERT model 3. Training Process:\nConfigure training instance type (ml.p3.2xlarge for GPU) Set hyperparameters (learning rate, batch size, epochs) Enable SageMaker Experiments for tracking Monitor training metrics in real-time 4. Model Evaluation:\nAccuracy: 92% on test set Precision/Recall analysis Confusion matrix visualization Error analysis for model improvement 5. Deployment:\nCreate SageMaker endpoint Configure auto-scaling policies Set up monitoring and logging Test with sample movie reviews Live Demonstration Results:\nPositive Review: \u0026ldquo;This movie was absolutely fantastic!\u0026rdquo; → Confidence: 95% Negative Review: \u0026ldquo;Worst movie I\u0026rsquo;ve ever seen\u0026rdquo; → Confidence: 93% Neutral Review: \u0026ldquo;It was okay, nothing special\u0026rdquo; → Confidence: 78% Cloud vs. On-Premise Comparison Cost Analysis:\nAspect On-Premise AWS Cloud Initial Investment $50,000+ for hardware $0 upfront GPU Servers $10,000-$30,000 each Pay-per-hour ($3-$30/hour) Maintenance 20% annual cost Managed by AWS Scaling Manual hardware purchase Instant scaling Utilization Fixed cost regardless of usage Pay only for actual usage Performance Considerations:\nTraining Speed: Cloud offers latest GPU instances (A100, V100) Experimentation: Quick iteration without resource constraints Parallel Jobs: Run multiple experiments simultaneously Global Reach: Deploy models closer to users worldwide ROI Analysis:\nBreak-even point: Typically 3-6 months for most projects Cost savings: 40-60% compared to maintaining on-premise infrastructure Time savings: 70-80% reduction in infrastructure setup time Key Takeaways Data Science on Cloud Benefits Speed to Insights: Reduce time from data collection to model deployment by 80% Scalability: Handle datasets of any size without infrastructure worries Cost Optimization: Pay only for compute resources when actually training/inferring Collaboration: Teams can work together seamlessly with shared environments Latest Tools: Access to cutting-edge ML frameworks and services AWS Services for Data Science S3: Foundation for data lake architecture Glue: Serverless ETL that eliminates data engineering overhead SageMaker: Complete ML platform from experimentation to production Additional Services: Athena for SQL queries on S3 QuickSight for data visualization Lambda for serverless inference Step Functions for ML workflow orchestration Best Practices Learned Start with Data Quality: Clean data is crucial for model performance Automate Everything: Use Glue crawlers and SageMaker pipelines Monitor Costs: Set budget alerts and use spot instances Version Control: Track datasets, models, and experiments Incremental Deployment: Test models before full production rollout Post-Workshop Mini Project Suggested Project: Movie Review Sentiment Analyzer\nObjective: Build end-to-end sentiment analysis system using AWS services\nSteps:\nCollect movie review dataset from public sources Store data in S3 bucket Use AWS Glue to clean and prepare data Train sentiment analysis model in SageMaker Deploy model as REST API endpoint Create simple web interface to test predictions Learning Outcomes:\nHands-on experience with complete Data Science pipeline Understanding of AWS service integration Cost estimation and optimization skills Model deployment and monitoring experience Applying to Work For Current OJT Project Apply SageMaker for movie recommendation model training Use S3 for storing TMDB dataset and preprocessed data Leverage Glue for ETL operations on movie metadata Deploy Models using SageMaker endpoints for real-time recommendations Future Applications Chatbot Enhancement: Train custom NLP models for better understanding Recommendation Systems: Build collaborative filtering models User Behavior Analysis: Process and analyze user interaction data A/B Testing: Deploy multiple model versions for comparison Event Experience Attending the “GenAI-powered App-DB Modernization” workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\nEvent Experience Attending the \u0026ldquo;Data Science On AWS\u0026rdquo; workshop at FPT University was an incredibly enriching experience that bridged the gap between academic knowledge and industry practices. Key experiences included:\nLearning from AWS Community Builders Van Ho�ng Kha and B?ch Do�n Vuong brought real-world expertise as active AWS Community Builders Both speakers shared production-ready practices from actual enterprise Data Science projects Their hands-on approach made complex AWS services accessible and practical Emphasis on cost optimization showed real business awareness Comprehensive AWS Data Science Pipeline Gained complete understanding of end-to-end ML workflow on AWS Learned how S3, Glue, and SageMaker work together seamlessly Understood the importance of data quality before model training Saw how AWS abstracts infrastructure complexity, letting data scientists focus on models Live Demonstrations AWS Glue Demo:\nWitnessed real-time ETL processing of IMDb dataset Observed automated data cataloging and schema discovery Learned data cleaning techniques for text data Understood how Glue eliminates traditional data engineering bottlenecks SageMaker Demo:\nWatched end-to-end ML model development from notebook to production Saw hyperparameter tuning and experiment tracking in action Observed one-click model deployment to scalable endpoints Tested live sentiment analysis predictions with impressive accuracy Cost vs. Performance Insights Eye-opening comparison between Cloud and On-premise infrastructure Learned about spot instances for 70% cost savings on training Understood pay-per-use model eliminates waste from idle resources Discovered Cloud provides better ROI for most Data Science workloads Practical Application to OJT Project Realized I can use SageMaker for training recommendation models Identified opportunity to migrate TMDB data processing to Glue Learned how to deploy ML models as REST APIs using SageMaker endpoints Understood cost-effective strategies for running ML workloads Networking and Collaboration Connected with fellow students interested in Data Science and ML Exchanged ideas about applying AWS services to university projects Discussed internship opportunities in cloud and data science roles Built relationship with AWS Community Builders for mentorship Key Realizations Data Science is not just about algorithms - infrastructure and operations matter Cloud democratizes ML - anyone can access powerful compute without huge investments AWS provides mature ecosystem for complete ML lifecycle Automation is key - managed services reduce operational burden significantly Production deployment is as important as model accuracy Immediate Actions Post-Workshop Created AWS account and explored SageMaker Studio Completed the suggested mini-project using IMDb dataset Experimented with Glue crawlers on TMDB data Researched SageMaker pricing for OJT project feasibility Documented learnings for team knowledge sharing Some workshop photos Add your workshop photos here\nThis workshop was a perfect example of how industry and academia should connect. The speakers demonstrated real-world applications while making sure concepts were accessible to students. It reinforced my belief that cloud computing is not optional but essential for modern Data Science work, and AWS provides a comprehensive platform to build production-grade ML systems.\n"},{"uri":"https://rsshive.github.io/fcj-workshop/5-workshop/5.4-next.js-setup/5.4.1-install-nextjs/","title":"Install Next.js Project","tags":[],"description":"","content":"Prepare the Environment In this step, you will create a new Next.js project and install required UI libraries that will be used later in the authentication interface.\n1. Create a New Next.js Project If you are starting from scratch, run the following command:\nnpx create-next-app@latest my-cognito-app When prompted: Ok to proceed? (y)\nChoose y, then navigate into the project directory:\ncd my-cognito-app 2. Install shadcn/ui (UI Component Library) You will use shadcn/ui to build the authentication UI later.\nInitialize shadcn:\nnpx shadcn@latest init This will set up the component generator for your project.\nYour environment is now ready for the next steps, where you will install Amplify SDK and configure the AWS Cognito integration.\nNavigation:\nPrevious: 5.4 Next.js Project Setup Next Step: 5.4.2 Install Amplify SDK → Install and configure Amplify SDK "},{"uri":"https://rsshive.github.io/fcj-workshop/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Participate a learning group to encourage project progress. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations - Created AWS account and completed first module about account security and billing management. 09/9/2025 3 - Learn about AWS and its types of services - Learn about Amazon VPC and prepair preparation steps. 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Set up EC2 instances: + Create and test EC2 Sever\n+ Create NAT Gateway 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: "},{"uri":"https://rsshive.github.io/fcj-workshop/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Learn AWS Identity and Access Management (IAM) fundamentals Understand networking essentials with Amazon VPC Master basic AWS security and access control concepts Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Access Management with AWS Identity and Access Management (IAM) - Learn about users, groups, roles, and policies - Understand IAM best practices 16/09/2025 16/09/2025 https://cloudjourney.awsstudygroup.com/ 3 - Networking Essentials with Amazon Virtual Private Cloud (VPC) - Learn about subnets, route tables, and internet gateways - Understand VPC security groups and NACLs 17/09/2025 17/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Practice: + Create IAM users and groups + Assign policies and permissions + Set up MFA for enhanced security 18/09/2025 18/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Practice: + Create custom VPC with public and private subnets + Configure route tables and security groups + Test network connectivity 19/09/2025 19/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Review and consolidate IAM and VPC knowledge - Troubleshoot common networking and access issues - Prepare for next week\u0026rsquo;s EC2 topics 20/09/2025 20/09/2025 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: Mastered AWS Identity and Access Management (IAM) fundamentals:\nUsers, groups, and roles management Policy creation and attachment Multi-factor authentication (MFA) setup IAM best practices implementation Successfully understood Amazon VPC networking concepts:\nVPC creation and configuration Public and private subnet design Route tables and internet gateway setup Security groups and Network ACLs configuration Gained hands-on experience with:\nCreating and managing IAM users and groups Assigning appropriate permissions and policies Setting up secure network architectures Implementing security best practices Developed understanding of AWS security principles:\nPrinciple of least privilege Network segmentation strategies Access control mechanisms Security monitoring basics \u0026hellip;\n"},{"uri":"https://rsshive.github.io/fcj-workshop/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Master compute essentials with Amazon EC2 Learn instance profiling with IAM roles for EC2 Understand cloud development environment setup Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Compute Essentials with Amazon Elastic Compute Cloud (EC2) - Learn about instance types, AMIs, and storage options - Understand EC2 pricing models 23/09/2025 23/09/2025 https://cloudjourney.awsstudygroup.com/ 3 - Instance Profiling with IAM Roles for EC2 - Learn how to attach IAM roles to EC2 instances - Understand service-linked roles and permissions 24/09/2025 24/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Cloud Development with AWS Cloud9 - Set up cloud-based development environment - Learn collaborative coding features 25/09/2025 25/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Practice: + Launch EC2 instances with different configurations + Attach IAM roles to EC2 instances + Test role-based permissions 26/09/2025 26/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Set up AWS Cloud9 environment + Deploy applications from Cloud9 + Integrate with EC2 instances 27/09/2025 27/09/2025 https://cloudjourney.awsstudygroup.com/ Week 3 Achievements: Mastered Amazon EC2 compute essentials:\nDifferent instance types and their use cases Amazon Machine Images (AMI) creation and management EBS volumes and storage options EC2 pricing models and cost optimization Successfully implemented IAM roles for EC2:\nCreated and attached IAM roles to EC2 instances Configured service-linked roles and permissions Implemented secure access patterns without hardcoded credentials Tested role-based access to AWS services Gained experience with AWS Cloud9:\nSet up cloud-based development environments Explored collaborative coding features Integrated Cloud9 with EC2 instances Deployed applications from the cloud IDE Developed practical skills in:\nLaunching and configuring EC2 instances Managing instance lifecycle and monitoring Implementing security best practices for compute resources Using cloud-based development workflows \u0026hellip;\n"},{"uri":"https://rsshive.github.io/fcj-workshop/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Learn database essentials with Amazon RDS Understand relational database management in the cloud Master database security and backup strategies Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Database Essentials with Amazon Relational Database Service (RDS) - Learn about RDS engine types and configurations - Understand database instance classes 30/09/2025 30/09/2025 https://cloudjourney.awsstudygroup.com/ 3 - RDS security and access management - Learn about database parameter groups and option groups - Understand VPC security for databases 01/10/2025 01/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Database backup and recovery strategies - Learn about automated backups and snapshots - Understand Multi-AZ deployments and read replicas 02/10/2025 02/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Practice: + Create RDS instances with different engines + Configure security groups and parameter groups + Test database connectivity from EC2 03/10/2025 03/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Set up automated backups and snapshots + Create read replicas + Test database recovery procedures 04/10/2025 04/10/2025 https://cloudjourney.awsstudygroup.com/ Week 4 Achievements: Mastered Amazon RDS database essentials:\nDifferent RDS engine types (MySQL, PostgreSQL, Oracle, SQL Server) Database instance classes and sizing considerations Storage types and performance characteristics Cost optimization strategies for databases Successfully implemented RDS security:\nConfigured database security groups and VPC settings Set up parameter groups and option groups Implemented encryption at rest and in transit Managed database user access and permissions Gained expertise in backup and recovery:\nConfigured automated backup policies Created and managed database snapshots Set up Multi-AZ deployments for high availability Implemented read replicas for performance scaling Developed practical database skills:\nCreated and configured RDS instances Established secure connections from EC2 to RDS Performed database maintenance operations Monitored database performance and health \u0026hellip;\n"},{"uri":"https://rsshive.github.io/fcj-workshop/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Collaborate with team to research and develop project ideas Finalize project concept and create development plan Write project requirements and demo specifications Select appropriate AWS services for the project Establish project timeline, deadlines, and role assignments Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Team collaboration for project idea research - Brainstorm potential project concepts - Research market needs and technical feasibility 07/10/2025 07/10/2025 Team collaboration tools, AWS documentation 3 - Finalize project idea and concept - Reach team consensus on project direction - Define project scope and objectives 08/10/2025 08/10/2025 Project planning templates 4 - Create development plan and timeline - Write detailed project requirements - Define demo specifications and success criteria 09/10/2025 09/10/2025 Requirements documentation templates 5 - Select AWS services for the project + Evaluate service compatibility + Consider cost and scalability + Plan service integration 10/10/2025 10/10/2025 https://aws.amazon.com/products/ 6 - Establish project deadlines and milestones - Assign roles and responsibilities to team members - Set up project management and communication channels 11/10/2025 11/10/2025 Project management tools Week 5 Achievements: Successfully completed team project ideation process:\nConducted comprehensive research on potential project ideas Facilitated productive team brainstorming sessions Evaluated technical feasibility and market relevance Reached unanimous team consensus on final project concept Developed comprehensive project planning documentation:\nCreated detailed project requirements document Defined clear demo specifications and success criteria Established project scope, objectives, and deliverables Developed realistic development timeline and milestones Completed AWS service selection and architecture planning:\nEvaluated multiple AWS services for project compatibility Considered cost optimization and scalability factors Designed preliminary system architecture Planned service integration strategies Established effective project management framework:\nSet clear project deadlines and milestone dates Assigned specific roles and responsibilities to team members Implemented project communication channels and tools Created accountability and progress tracking mechanisms \u0026hellip;\n"},{"uri":"https://rsshive.github.io/fcj-workshop/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Finalize business logic and requirements for the project Learn selected AWS services in depth Design application flow and operational direction Collect feedback and implement MVP development Master AWS Lambda and API Gateway fundamentals Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Finalize business logic and requirements - Lock down project scope and specifications - Review and validate all project requirements 14/10/2025 14/10/2025 Project documentation, requirements templates 3 - Deep dive into selected AWS services - Study service documentation and best practices - Understand service limitations and capabilities 15/10/2025 15/10/2025 https://docs.aws.amazon.com/ 4 - Design application flow and architecture - Create operational workflow diagrams - Map user journey and system interactions 16/10/2025 16/10/2025 Architecture design tools, AWS Well-Architected 5 - Collect stakeholder feedback on design - Implement feedback and refine MVP scope - Begin MVP development planning 17/10/2025 17/10/2025 Feedback collection tools, MVP frameworks 6 - Learn AWS Lambda fundamentals + Function creation and deployment + Event-driven architecture - Learn API Gateway basics + REST API creation + Integration patterns 18/10/2025 18/10/2025 https://docs.aws.amazon.com/lambda/, https://docs.aws.amazon.com/apigateway/ Week 6 Achievements: Successfully finalized project business logic:\nLocked down comprehensive project requirements Validated all business rules and specifications Established clear project scope boundaries Created detailed functional requirements documentation Mastered selected AWS services:\nGained deep understanding of chosen AWS services Studied service documentation and implementation patterns Understood service limitations, pricing, and best practices Identified optimal service configurations for the project Designed comprehensive application architecture:\nCreated detailed application flow diagrams Mapped operational workflows and user journeys Designed system interaction patterns Established data flow and processing logic Implemented feedback-driven MVP development:\nCollected valuable stakeholder feedback on designs Refined MVP scope based on feedback analysis Prioritized features for initial development phase Created actionable development roadmap Acquired AWS Lambda and API Gateway expertise:\nLearned serverless computing fundamentals with Lambda Mastered function creation, deployment, and event handling Understood API Gateway REST API creation and management Explored integration patterns between Lambda and API Gateway Identified use cases for serverless architecture expansion \u0026hellip;\n"},{"uri":"https://rsshive.github.io/fcj-workshop/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Finalize and standardize API endpoint structure Test basic functionality of the application Set up AWS Cognito for user authentication and authorization Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Team meeting to discuss and standardize API endpoints - Define API structure and naming conventions - Document API specifications 21/10/2025 21/10/2025 API design best practices, REST standards 3 - Implement standardized API endpoints + User management APIs + Content APIs + Authentication APIs 22/10/2025 22/10/2025 https://docs.aws.amazon.com/apigateway/ 4 - Test basic application functionality - Perform integration testing - Debug and fix identified issues 23/10/2025 23/10/2025 Testing frameworks, Postman 5 - Learn AWS Cognito fundamentals + User pools + Identity pools + Authentication flows - Plan Cognito integration strategy 24/10/2025 24/10/2025 https://docs.aws.amazon.com/cognito/ 6 - Practice: + Set up Cognito user pool + Configure authentication flows + Integrate Cognito with API Gateway 25/10/2025 25/10/2025 https://docs.aws.amazon.com/cognito/ Week 7 Achievements: Successfully standardized all API endpoints with consistent naming conventions and structure\nCompleted integration testing for basic application features:\nUser registration and login flows Content retrieval APIs Data validation and error handling Set up AWS Cognito user pool with:\nUser authentication and authorization Password policies and MFA options Custom attributes for user profiles Integrated Cognito with API Gateway:\nConfigured authorizers Implemented JWT token validation Secured API endpoints Fixed critical bugs and improved API response times\nDocumented API endpoints and authentication flows for team reference\n"},{"uri":"https://rsshive.github.io/fcj-workshop/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Research and discuss AI integration strategies for the project Learn AWS AI/ML services: Bedrock, SageMaker, and Personalize Evaluate which AI services best fit project requirements Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Team discussion on AI use cases for the project - Brainstorm features that could benefit from AI - Define AI integration requirements 28/10/2025 28/10/2025 AI/ML best practices 3 - Learn AWS Bedrock fundamentals + Foundation models + Model customization + API usage and pricing - Explore Bedrock use cases 29/10/2025 29/10/2025 https://docs.aws.amazon.com/bedrock/ 4 - Learn AWS SageMaker basics + Model training and deployment + Endpoints and inference - Learn AWS Personalize + Recommendation systems + Real-time personalization 30/10/2025 30/10/2025 https://docs.aws.amazon.com/sagemaker/, https://docs.aws.amazon.com/personalize/ 5 - Compare AWS AI services for project needs - Create pros/cons analysis for each service - Make decision on which service to implement 31/10/2025 31/10/2025 AWS AI service comparison guides 6 - Practice: + Set up Bedrock access and permissions + Test Bedrock API calls + Experiment with different foundation models 01/11/2025 01/11/2025 https://docs.aws.amazon.com/bedrock/ Week 8 Achievements: Chatbot for movie/TV show search assistance\nPersonalized content recommendations\nNatural language processing for user queries\nResearched AWS AI/ML services in depth:\nAWS Bedrock: Foundation models, Claude, Llama AWS SageMaker: Custom model training and deployment AWS Personalize: Recommendation engines Compared and evaluated services:\nBedrock best suited for conversational chatbot Cost and implementation complexity Scalability and maintenance capabilities Successfully set up Bedrock:\nConfigured IAM permissions Tested API calls with Claude model Evaluated response quality and latency Created proof of concept for chatbot integration\nDeveloped detailed plan for Bedrock integration in the following week\n"},{"uri":"https://rsshive.github.io/fcj-workshop/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Implement AWS Bedrock integration into the project Develop basic chatbot APIs using Bedrock foundation models Test and validate chatbot functionality Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Design chatbot architecture with Bedrock - Plan API structure for chatbot interactions - Define input/output formats 04/11/2025 04/11/2025 Bedrock documentation, API design patterns 3 - Implement Lambda functions for Bedrock API calls + Request processing + Response formatting + Error handling 05/11/2025 05/11/2025 https://docs.aws.amazon.com/bedrock/ 4 - Develop chatbot APIs + Chat endpoint + Context management + Conversation history 06/11/2025 06/11/2025 API Gateway, Lambda integration 5 - Test chatbot functionality + Basic Q\u0026amp;A testing + Response accuracy validation + Latency measurements 07/11/2025 07/11/2025 Testing frameworks 6 - Practice: + Integration testing with frontend + Debug and optimize performance + Refine prompts for better responses 08/11/2025 08/11/2025 Prompt engineering best practices Week 9 Achievements: Successfully integrated AWS Bedrock into the project:\nConfigured Bedrock runtime client Implemented Claude model for conversational AI Set up proper IAM roles and permissions Developed Lambda functions for chatbot operations:\nRequest preprocessing and validation Bedrock API invocation Response formatting and error handling Conversation context management Created chatbot API endpoints:\n/chat - Main conversation endpoint /chat/history - Retrieve conversation history /chat/clear - Clear conversation context Implemented basic chatbot features:\nNatural language understanding Context-aware responses Multi-turn conversation support Error recovery mechanisms Testing and validation:\nResponse time averaging 2-3 seconds 95% accuracy for basic queries Successfully handled edge cases Optimized prompt engineering for better movie/TV show related responses\n"},{"uri":"https://rsshive.github.io/fcj-workshop/1-worklog/","title":"Worklog","tags":[],"description":"","content":"My AWS First Cloud Journey - 12-Week Internship Worklog During my 3-month internship at AWS First Cloud Journey (September 9 - December 9, 2025), I completed a comprehensive cloud computing training program focused on AWS services and practical implementations. This worklog documents my weekly progress, learning objectives, and hands-on experiences throughout the program.\nThe internship was structured around progressive learning, starting from fundamental AWS concepts and gradually advancing to complex cloud architecture patterns, networking solutions, and security best practices. Each week built upon previous knowledge while introducing new services and real-world scenarios.\nWeekly Overview: Week 1: Getting Started with AWS - Account Setup \u0026amp; Basic Services\nInitial setup, AWS account configuration, IAM fundamentals, and introduction to core services.\nWeek 2: AWS Identity \u0026amp; Access Management (IAM) and VPC Fundamentals\nDeep dive into IAM policies, roles, and Amazon VPC networking essentials.\nWeek 3: Amazon EC2 and Compute Services\nLaunching and managing EC2 instances, understanding compute options and pricing models.\nWeek 4: Storage Services - S3, EBS, and EFS\nExploring AWS storage solutions, S3 bucket configurations, and file system options.\nWeek 5: Databases on AWS - RDS, DynamoDB, and More\nWorking with managed database services and understanding different database types.\nWeek 6: Networking Deep Dive - VPC Advanced Concepts\nAdvanced VPC configurations, subnets, route tables, and network security.\nWeek 7: Security and Compliance Best Practices\nImplementing security groups, NACLs, encryption, and AWS security services.\nWeek 8: Serverless Computing - Lambda and API Gateway\nBuilding serverless applications and understanding event-driven architectures.\nWeek 9: Monitoring and Optimization - CloudWatch and Cost Management\nSetting up monitoring, logging, alarms, and optimizing AWS costs.\nWeek 10: High Availability and Disaster Recovery\nDesigning resilient architectures with multi-AZ deployments and backup strategies.\nWeek 11: DevOps on AWS - CI/CD and Infrastructure as Code\nAutomating deployments with CodePipeline, CloudFormation, and best practices.\nWeek 12: Final Project and Workshop Development\nCompleting comprehensive S3 VPC endpoint workshop and internship documentation.\n"},{"uri":"https://rsshive.github.io/fcj-workshop/5-workshop/5.5-cognito-function/5.5.2-auth-ui/","title":"Build Authentication UI","tags":[],"description":"","content":"5.5.2 Build Authentication UI 100% faithful to your original workshop — beautiful, functional, production-grade\nYou now have all the Cognito logic. Time to build the real user interface — exactly as you wrote it in your workshop.\n1. Landing Page – app/page.tsx // app/page.tsx import Link from \u0026#34;next/link\u0026#34;; import { Button } from \u0026#34;@/components/ui/button\u0026#34;; import { Card, CardContent, CardDescription, CardHeader, CardTitle } from \u0026#34;@/components/ui/card\u0026#34;; export default function HomePage() { return ( \u0026lt;div className=\u0026#34;min-h-screen flex items-center justify-center bg-gradient-to-br from-gray-50 to-gray-100\u0026#34;\u0026gt; \u0026lt;Card className=\u0026#34;w-full max-w-md\u0026#34;\u0026gt; \u0026lt;CardHeader className=\u0026#34;text-center\u0026#34;\u0026gt; \u0026lt;CardTitle className=\u0026#34;text-3xl font-bold\u0026#34;\u0026gt;Welcome\u0026lt;/CardTitle\u0026gt; \u0026lt;CardDescription\u0026gt;Secure file management system with user authentication\u0026lt;/CardDescription\u0026gt; \u0026lt;/CardHeader\u0026gt; \u0026lt;CardContent className=\u0026#34;space-y-4\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;grid gap-3\u0026#34;\u0026gt; \u0026lt;Button asChild size=\u0026#34;lg\u0026#34;\u0026gt; \u0026lt;Link href=\u0026#34;/signin\u0026#34;\u0026gt;Login\u0026lt;/Link\u0026gt; \u0026lt;/Button\u0026gt; \u0026lt;Button asChild variant=\u0026#34;outline\u0026#34; size=\u0026#34;lg\u0026#34;\u0026gt; \u0026lt;Link href=\u0026#34;/signup\u0026#34;\u0026gt;Sign Up\u0026lt;/Link\u0026gt; \u0026lt;/Button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div className=\u0026#34;pt-4 border-t\u0026#34;\u0026gt; \u0026lt;p className=\u0026#34;text-sm text-muted-foreground text-center mb-3\u0026#34;\u0026gt;Protected Pages:\u0026lt;/p\u0026gt; \u0026lt;div className=\u0026#34;grid gap-2\u0026#34;\u0026gt; \u0026lt;Button asChild variant=\u0026#34;secondary\u0026#34; size=\u0026#34;sm\u0026#34;\u0026gt; \u0026lt;Link href=\u0026#34;/dashboard\u0026#34;\u0026gt;Dashboard\u0026lt;/Link\u0026gt; \u0026lt;/Button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/CardContent\u0026gt; \u0026lt;/Card\u0026gt; \u0026lt;/div\u0026gt; ); } 2. Sign-Up Page (with Email Verification) – app/signup/page.tsx \u0026#39;use client\u0026#39;; import { useState } from \u0026#39;react\u0026#39;; import { useRouter } from \u0026#39;next/navigation\u0026#39;; import { cognitoSignUp, cognitoConfirmSignUp } from \u0026#39;@/lib/cognito-auth\u0026#39;; export default function SignUpPage() { const router = useRouter(); const [step, setStep] = useState\u0026lt;\u0026#39;signup\u0026#39; | \u0026#39;confirm\u0026#39;\u0026gt;(\u0026#39;signup\u0026#39;); const [email, setEmail] = useState(\u0026#39;\u0026#39;); const [password, setPassword] = useState(\u0026#39;\u0026#39;); const [name, setName] = useState(\u0026#39;\u0026#39;); const [code, setCode] = useState(\u0026#39;\u0026#39;); const [error, setError] = useState(\u0026#39;\u0026#39;); const [loading, setLoading] = useState(false); const handleSignUp = async (e: React.FormEvent) =\u0026gt; { e.preventDefault(); setError(\u0026#39;\u0026#39;); setLoading(true); const result = await cognitoSignUp({ email, password, name }); setLoading(false); if (result.success) { setStep(\u0026#39;confirm\u0026#39;); } else { setError(result.error || \u0026#39;Sign up failed\u0026#39;); } }; const handleConfirm = async (e: React.FormEvent) =\u0026gt; { e.preventDefault(); setError(\u0026#39;\u0026#39;); setLoading(true); const result = await cognitoConfirmSignUp(email, code); setLoading(false); if (result.success) { router.push(\u0026#39;/signin\u0026#39;); } else { setError(result.error || \u0026#39;Verification failed\u0026#39;); } }; // Confirmation Step if (step === \u0026#39;confirm\u0026#39;) { return ( \u0026lt;div className=\u0026#34;min-h-screen flex items-center justify-center\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;max-w-md w-full p-8 bg-white rounded-lg shadow-lg\u0026#34;\u0026gt; \u0026lt;h1 className=\u0026#34;text-2xl font-bold mb-6\u0026#34;\u0026gt;Verify Email\u0026lt;/h1\u0026gt; \u0026lt;p className=\u0026#34;mb-4 text-gray-600\u0026#34;\u0026gt;Enter the verification code sent to {email}\u0026lt;/p\u0026gt; \u0026lt;form onSubmit={handleConfirm}\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; placeholder=\u0026#34;Verification Code\u0026#34; value={code} onChange={(e) =\u0026gt; setCode(e.target.value)} className=\u0026#34;w-full px-4 py-2 border rounded-lg mb-4\u0026#34; required /\u0026gt; {error \u0026amp;\u0026amp; \u0026lt;p className=\u0026#34;text-red-500 mb-4\u0026#34;\u0026gt;{error}\u0026lt;/p\u0026gt;} \u0026lt;button type=\u0026#34;submit\u0026#34; disabled={loading} className=\u0026#34;w-full bg-blue-600 text-white py-2 rounded-lg hover:bg-blue-700 disabled:opacity-50\u0026#34; \u0026gt; {loading ? \u0026#39;Verifying...\u0026#39; : \u0026#39;Verify Email\u0026#39;} \u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; ); } // Sign-Up Form return ( \u0026lt;div className=\u0026#34;min-h-screen flex items-center justify-center\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;max-w-md w-full p-8 bg-white rounded-lg shadow-lg\u0026#34;\u0026gt; \u0026lt;h1 className=\u0026#34;text-2xl font-bold mb-6\u0026#34;\u0026gt;Sign Up\u0026lt;/h1\u0026gt; \u0026lt;form onSubmit={handleSignUp}\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; placeholder=\u0026#34;Full Name\u0026#34; value={name} onChange={(e) =\u0026gt; setName(e.target.value)} className=\u0026#34;w-full px-4 py-2 border rounded-lg mb-4\u0026#34; required /\u0026gt; \u0026lt;input type=\u0026#34;email\u0026#34; placeholder=\u0026#34;Email\u0026#34; value={email} onChange={(e) =\u0026gt; setEmail(e.target.value)} className=\u0026#34;w-full px-4 py-2 border rounded-lg mb-4\u0026#34; required /\u0026gt; \u0026lt;input type=\u0026#34;password\u0026#34; placeholder=\u0026#34;Password\u0026#34; value={password} onChange={(e) =\u0026gt; setPassword(e.target.value)} className=\u0026#34;w-full px-4 py-2 border rounded-lg mb-4\u0026#34; required /\u0026gt; \u0026lt;p className=\u0026#34;text-xs text-gray-500 mb-4\u0026#34;\u0026gt; Password must be at least 8 characters with uppercase, lowercase, number, and special character \u0026lt;/p\u0026gt; {error \u0026amp;\u0026amp; \u0026lt;p className=\u0026#34;text-red-500 mb-4\u0026#34;\u0026gt;{error}\u0026lt;/p\u0026gt;} \u0026lt;button type=\u0026#34;submit\u0026#34; disabled={loading} className=\u0026#34;w-full bg-blue-600 text-white py-2 rounded-lg hover:bg-blue-700 disabled:opacity-50\u0026#34; \u0026gt; {loading ? \u0026#39;Signing up...\u0026#39; : \u0026#39;Sign Up\u0026#39;} \u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; ); } 3. Sign-In Page – app/signin/page.tsx \u0026#39;use client\u0026#39;; import { useState } from \u0026#39;react\u0026#39;; import { useRouter } from \u0026#39;next/navigation\u0026#39;; import { cognitoSignIn } from \u0026#39;@/lib/cognito-auth\u0026#39;; export default function LoginPage() { const router = useRouter(); const [email, setEmail] = useState(\u0026#39;\u0026#39;); const [password, setPassword] = useState(\u0026#39;\u0026#39;); const [error, setError] = useState(\u0026#39;\u0026#39;); const [loading, setLoading] = useState(false); const handleSubmit = async (e: React.FormEvent) =\u0026gt; { e.preventDefault(); setError(\u0026#39;\u0026#39;); setLoading(true); const result = await cognitoSignIn({ email, password }); setLoading(false); if (result.success) { router.push(\u0026#39;/dashboard\u0026#39;); } else { setError(result.error || \u0026#39;Login failed\u0026#39;); } }; return ( \u0026lt;div className=\u0026#34;min-h-screen flex items-center justify-center bg-gray-50\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;max-w-md w-full p-8 bg-white rounded-lg shadow-lg\u0026#34;\u0026gt; \u0026lt;h1 className=\u0026#34;text-3xl font-bold mb-6 text-center\u0026#34;\u0026gt;Sign In\u0026lt;/h1\u0026gt; \u0026lt;form onSubmit={handleSubmit}\u0026gt; \u0026lt;div className=\u0026#34;mb-4\u0026#34;\u0026gt; \u0026lt;label className=\u0026#34;block text-gray-700 mb-2\u0026#34;\u0026gt;Email\u0026lt;/label\u0026gt; \u0026lt;input type=\u0026#34;email\u0026#34; value={email} onChange={(e) =\u0026gt; setEmail(e.target.value)} className=\u0026#34;w-full px-4 py-2 border border-gray-300 rounded-lg focus:outline-none focus:ring-2 focus:ring-blue-500\u0026#34; required /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div className=\u0026#34;mb-6\u0026#34;\u0026gt; \u0026lt;label className=\u0026#34;block text-gray-700 mb-2\u0026#34;\u0026gt;Password\u0026lt;/label\u0026gt; \u0026lt;input type=\u0026#34;password\u0026#34; value={password} onChange={(e) =\u0026gt; setPassword(e.target.value)} className=\u0026#34;w-full px-4 py-2 border border-gray-300 rounded-lg focus:outline-none focus:ring-2 focus:ring-blue-500\u0026#34; required /\u0026gt; \u0026lt;/div\u0026gt; {error \u0026amp;\u0026amp; \u0026lt;div className=\u0026#34;mb-4 p-3 bg-red-100 border border-red-400 text-red-700 rounded-lg\u0026#34;\u0026gt;{error}\u0026lt;/div\u0026gt;} \u0026lt;button type=\u0026#34;submit\u0026#34; disabled={loading} className=\u0026#34;w-full bg-blue-600 text-white py-3 rounded-lg font-semibold hover:bg-blue-700 disabled:opacity-50 disabled:cursor-not-allowed transition\u0026#34; \u0026gt; {loading ? \u0026#39;Signing in...\u0026#39; : \u0026#39;Sign In\u0026#39;} \u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;div className=\u0026#34;mt-6 text-center\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;/signup\u0026#34; className=\u0026#34;text-blue-600 hover:underline\u0026#34;\u0026gt;Don\u0026#39;t have an account? Sign up\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div className=\u0026#34;mt-2 text-center\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;/forgot-password\u0026#34; className=\u0026#34;text-gray-600 hover:underline text-sm\u0026#34;\u0026gt;Forgot password?\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; ); } 4. forgot and change password - app/forgot-password/page.tsx \u0026#39;use client\u0026#39;; import { useState } from \u0026#39;react\u0026#39;; import { useRouter } from \u0026#39;next/navigation\u0026#39;; import Link from \u0026#39;next/link\u0026#39;; import { cognitoResetPassword, cognitoConfirmResetPassword } from \u0026#39;@/lib/cognito-auth\u0026#39;; import { Button } from \u0026#39;@/components/ui/button\u0026#39;; import { Input } from \u0026#39;@/components/ui/input\u0026#39;; import { Label } from \u0026#39;@/components/ui/label\u0026#39;; import { Card, CardContent, CardDescription, CardHeader, CardTitle } from \u0026#39;@/components/ui/card\u0026#39;; import { Alert, AlertDescription } from \u0026#39;@/components/ui/alert\u0026#39;; export default function ForgotPasswordPage() { const [step, setStep] = useState\u0026lt;\u0026#39;request\u0026#39; | \u0026#39;confirm\u0026#39;\u0026gt;(\u0026#39;request\u0026#39;); const [email, setEmail] = useState(\u0026#39;\u0026#39;); const [code, setCode] = useState(\u0026#39;\u0026#39;); const [newPassword, setNewPassword] = useState(\u0026#39;\u0026#39;); const [confirmPassword, setConfirmPassword] = useState(\u0026#39;\u0026#39;); const [error, setError] = useState(\u0026#39;\u0026#39;); const [success, setSuccess] = useState(\u0026#39;\u0026#39;); const [loading, setLoading] = useState(false); const router = useRouter(); const handleRequestReset = async (e: React.FormEvent) =\u0026gt; { e.preventDefault(); setError(\u0026#39;\u0026#39;); setSuccess(\u0026#39;\u0026#39;); setLoading(true); try { const result = await cognitoResetPassword(email); if (result.success) { setSuccess(result.message || \u0026#39;Password reset code sent to your email\u0026#39;); setStep(\u0026#39;confirm\u0026#39;); } else { setError(result.error || \u0026#39;Failed to send reset code\u0026#39;); } } catch (err: any) { setError(err.message || \u0026#39;An error occurred\u0026#39;); } finally { setLoading(false); } }; const handleConfirmReset = async (e: React.FormEvent) =\u0026gt; { e.preventDefault(); setError(\u0026#39;\u0026#39;); setSuccess(\u0026#39;\u0026#39;); if (newPassword !== confirmPassword) { setError(\u0026#39;Passwords do not match\u0026#39;); return; } if (newPassword.length \u0026lt; 😎 { setError(\u0026#39;Password must be at least 8 characters long\u0026#39;); return; } setLoading(true); try { const result = await cognitoConfirmResetPassword( email, code, newPassword, ); if (result.success) { setSuccess(\u0026#39;Password reset successful! Redirecting to login...\u0026#39;); setTimeout(() =\u0026gt; { router.push(\u0026#39;/signin\u0026#39;); }, 2000); } else { setError(result.error || \u0026#39;Failed to reset password\u0026#39;); } } catch (err: any) { setError(err.message || \u0026#39;An error occurred\u0026#39;); } finally { setLoading(false); } }; if (step === \u0026#39;confirm\u0026#39;) { return ( \u0026lt;div className=\u0026#34;min-h-screen flex items-center justify-center bg-gradient-to-br from-blue-50 to-indigo-100 px-4\u0026#34;\u0026gt; \u0026lt;Card className=\u0026#34;w-full max-w-md\u0026#34;\u0026gt; \u0026lt;CardHeader className=\u0026#34;space-y-1\u0026#34;\u0026gt; \u0026lt;CardTitle className=\u0026#34;text-2xl font-bold text-center\u0026#34;\u0026gt;Reset Password\u0026lt;/CardTitle\u0026gt; \u0026lt;CardDescription className=\u0026#34;text-center\u0026#34;\u0026gt; Enter the code sent to {email} and your new password \u0026lt;/CardDescription\u0026gt; \u0026lt;/CardHeader\u0026gt; \u0026lt;CardContent\u0026gt; \u0026lt;form onSubmit={handleConfirmReset} className=\u0026#34;space-y-4\u0026#34;\u0026gt; {error \u0026amp;\u0026amp; ( \u0026lt;Alert variant=\u0026#34;destructive\u0026#34;\u0026gt; \u0026lt;AlertDescription\u0026gt;{error}\u0026lt;/AlertDescription\u0026gt; \u0026lt;/Alert\u0026gt; )} {success \u0026amp;\u0026amp; ( \u0026lt;Alert className=\u0026#34;bg-green-50 border-green-200\u0026#34;\u0026gt; \u0026lt;AlertDescription className=\u0026#34;text-green-800\u0026#34;\u0026gt;{success}\u0026lt;/AlertDescription\u0026gt; \u0026lt;/Alert\u0026gt; )} \u0026lt;div className=\u0026#34;space-y-2\u0026#34;\u0026gt; \u0026lt;Label htmlFor=\u0026#34;code\u0026#34;\u0026gt;Verification Code\u0026lt;/Label\u0026gt; \u0026lt;Input id=\u0026#34;code\u0026#34; type=\u0026#34;text\u0026#34; placeholder=\u0026#34;Enter 6-digit code\u0026#34; value={code} onChange={(e) =\u0026gt; setCode(e.target.value)} required disabled={loading} maxLength={6} /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div className=\u0026#34;space-y-2\u0026#34;\u0026gt; \u0026lt;Label htmlFor=\u0026#34;newPassword\u0026#34;\u0026gt;New Password\u0026lt;/Label\u0026gt; \u0026lt;Input id=\u0026#34;newPassword\u0026#34; type=\u0026#34;password\u0026#34; placeholder=\u0026#34;••••••••\u0026#34; value={newPassword} onChange={(e) =\u0026gt; setNewPassword(e.target.value)} required disabled={loading} /\u0026gt; \u0026lt;p className=\u0026#34;text-xs text-gray-500\u0026#34;\u0026gt; Must be at least 8 characters with uppercase, lowercase, number, and special character \u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div className=\u0026#34;space-y-2\u0026#34;\u0026gt; \u0026lt;Label htmlFor=\u0026#34;confirmPassword\u0026#34;\u0026gt;Confirm New Password\u0026lt;/Label\u0026gt; \u0026lt;Input id=\u0026#34;confirmPassword\u0026#34; type=\u0026#34;password\u0026#34; placeholder=\u0026#34;••••••••\u0026#34; value={confirmPassword} onChange={(e) =\u0026gt; setConfirmPassword(e.target.value)} required disabled={loading} /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;Button type=\u0026#34;submit\u0026#34; className=\u0026#34;w-full\u0026#34; disabled={loading}\u0026gt; {loading ? \u0026#39;Resetting...\u0026#39; : \u0026#39;Reset Password\u0026#39;} \u0026lt;/Button\u0026gt; \u0026lt;div className=\u0026#34;text-center text-sm text-gray-600\u0026#34;\u0026gt; \u0026lt;button type=\u0026#34;button\u0026#34; onClick={() =\u0026gt; setStep(\u0026#39;request\u0026#39;)} className=\u0026#34;text-blue-600 hover:underline\u0026#34; \u0026gt; Back to request reset \u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/CardContent\u0026gt; \u0026lt;/Card\u0026gt; \u0026lt;/div\u0026gt; ); } return ( \u0026lt;div className=\u0026#34;min-h-screen flex items-center justify-center bg-gradient-to-br from-blue-50 to-indigo-100 px-4\u0026#34;\u0026gt; \u0026lt;Card className=\u0026#34;w-full max-w-md\u0026#34;\u0026gt; \u0026lt;CardHeader className=\u0026#34;space-y-1\u0026#34;\u0026gt; \u0026lt;CardTitle className=\u0026#34;text-2xl font-bold text-center\u0026#34;\u0026gt;Forgot Password\u0026lt;/CardTitle\u0026gt; \u0026lt;CardDescription className=\u0026#34;text-center\u0026#34;\u0026gt; Enter your email to receive a password reset code \u0026lt;/CardDescription\u0026gt; \u0026lt;/CardHeader\u0026gt; \u0026lt;CardContent\u0026gt; \u0026lt;form onSubmit={handleRequestReset} className=\u0026#34;space-y-4\u0026#34;\u0026gt; {error \u0026amp;\u0026amp; ( \u0026lt;Alert variant=\u0026#34;destructive\u0026#34;\u0026gt; \u0026lt;AlertDescription\u0026gt;{error}\u0026lt;/AlertDescription\u0026gt; \u0026lt;/Alert\u0026gt; )} {success \u0026amp;\u0026amp; ( \u0026lt;Alert className=\u0026#34;bg-green-50 border-green-200\u0026#34;\u0026gt; \u0026lt;AlertDescription className=\u0026#34;text-green-800\u0026#34;\u0026gt;{success}\u0026lt;/AlertDescription\u0026gt; \u0026lt;/Alert\u0026gt; )} \u0026lt;div className=\u0026#34;space-y-2\u0026#34;\u0026gt; \u0026lt;Label htmlFor=\u0026#34;email\u0026#34;\u0026gt;Email\u0026lt;/Label\u0026gt; \u0026lt;Input id=\u0026#34;email\u0026#34; type=\u0026#34;email\u0026#34; placeholder=\u0026#34;you@example.com\u0026#34; value={email} onChange={(e) =\u0026gt; setEmail(e.target.value)} required disabled={loading} /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;Button type=\u0026#34;submit\u0026#34; className=\u0026#34;w-full\u0026#34; disabled={loading}\u0026gt; {loading ? \u0026#39;Sending...\u0026#39; : \u0026#39;Send Reset Code\u0026#39;} \u0026lt;/Button\u0026gt; \u0026lt;div className=\u0026#34;text-center text-sm text-gray-600\u0026#34;\u0026gt; Remember your password?{\u0026#39; \u0026#39;} \u0026lt;Link href=\u0026#34;/signin\u0026#34; className=\u0026#34;text-blue-600 hover:underline font-medium\u0026#34;\u0026gt; Sign in \u0026lt;/Link\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/CardContent\u0026gt; \u0026lt;/Card\u0026gt; \u0026lt;/div\u0026gt; ); } 5. Dashboard Page – app/dashboard/page.tsx \u0026#39;use client\u0026#39;; import { ProtectedRoute } from \u0026#34;@/components/ProtectedRoute\u0026#34;; import { useAuth } from \u0026#34;@/context/AuthContext\u0026#34;; import { LogOut, User, Mail, Shield, KeyRound, ArrowRight } from \u0026#34;lucide-react\u0026#34;; import Link from \u0026#34;next/link\u0026#34;; export default function DashboardPage() { const { user, signOut } = useAuth(); const handleSignOut = async () =\u0026gt; { await signOut(); }; return ( \u0026lt;ProtectedRoute\u0026gt; \u0026lt;div className=\u0026#34;min-h-screen bg-gradient-to-br from-background via-background to-secondary/5\u0026#34;\u0026gt; {/* Header */} \u0026lt;header className=\u0026#34;border-b border-border/50 backdrop-blur-sm sticky top-0 z-50\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;max-w-6xl mx-auto px-6 py-4 flex items-center justify-between\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;flex items-center gap-3\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;w-10 h-10 rounded-lg bg-gradient-to-br from-primary to-primary/60 flex items-center justify-center\u0026#34;\u0026gt; \u0026lt;User className=\u0026#34;w-6 h-6 text-primary-foreground\u0026#34; /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;h1 className=\u0026#34;text-xl font-bold text-foreground\u0026#34;\u0026gt;Dashboard\u0026lt;/h1\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;button onClick={handleSignOut} className=\u0026#34;inline-flex items-center gap-2 px-4 py-2 rounded-lg text-sm font-medium bg-destructive/10 text-destructive hover:bg-destructive/20 transition-colors\u0026#34;\u0026gt; \u0026lt;LogOut className=\u0026#34;w-4 h-4\u0026#34; /\u0026gt; Sign Out \u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/header\u0026gt; {/* Main Content */} \u0026lt;main className=\u0026#34;max-w-6xl mx-auto px-6 py-12\u0026#34;\u0026gt; {/* Welcome + Stats */} \u0026lt;div className=\u0026#34;mb-12\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;space-y-2 mb-8\u0026#34;\u0026gt; \u0026lt;h2 className=\u0026#34;text-4xl font-bold text-foreground\u0026#34;\u0026gt;Welcome back\u0026lt;/h2\u0026gt; \u0026lt;p className=\u0026#34;text-lg text-muted-foreground\u0026#34;\u0026gt;{user?.name || user?.email}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div className=\u0026#34;grid grid-cols-1 md:grid-cols-3 gap-6\u0026#34;\u0026gt; {/* Account Status */} \u0026lt;div className=\u0026#34;group relative overflow-hidden rounded-2xl bg-card border border-border/50 p-6 hover:shadow-lg transition-shadow duration-300\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;absolute inset-0 bg-gradient-to-br from-primary/5 to-transparent opacity-0 group-hover:opacity-100 transition-opacity\u0026#34; /\u0026gt; \u0026lt;div className=\u0026#34;relative space-y-4\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;w-12 h-12 rounded-xl bg-primary/10 flex items-center justify-center\u0026#34;\u0026gt; \u0026lt;Shield className=\u0026#34;w-6 h-6 text-primary\u0026#34; /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div\u0026gt; \u0026lt;h3 className=\u0026#34;text-sm font-semibold text-muted-foreground uppercase tracking-wide\u0026#34;\u0026gt;Account Status\u0026lt;/h3\u0026gt; \u0026lt;p className=\u0026#34;text-2xl font-bold text-foreground mt-2\u0026#34;\u0026gt;Active\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {/* Role */} \u0026lt;div className=\u0026#34;group relative overflow-hidden rounded-2xl bg-card border border-border/50 p-6 hover:shadow-lg transition-shadow duration-300\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;absolute inset-0 bg-gradient-to-br from-accent/5 to-transparent opacity-0 group-hover:opacity-100 transition-opacity\u0026#34; /\u0026gt; \u0026lt;div className=\u0026#34;relative space-y-4\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;w-12 h-12 rounded-xl bg-accent/10 flex items-center justify-center\u0026#34;\u0026gt; \u0026lt;User className=\u0026#34;w-6 h-6 text-accent\u0026#34; /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div\u0026gt; \u0026lt;h3 className=\u0026#34;text-sm font-semibold text-muted-foreground uppercase tracking-wide\u0026#34;\u0026gt;User Role\u0026lt;/h3\u0026gt; \u0026lt;p className=\u0026#34;text-2xl font-bold text-foreground mt-2 capitalize\u0026#34;\u0026gt;{user?.role || \u0026#34;Member\u0026#34;}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {/* Email Status */} \u0026lt;div className=\u0026#34;group relative overflow-hidden rounded-2xl bg-card border border-border/50 p-6 hover:shadow-lg transition-shadow duration-300\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;absolute inset-0 bg-gradient-to-br from-chart-1/5 to-transparent opacity-0 group-hover:opacity-100 transition-opacity\u0026#34; /\u0026gt; \u0026lt;div className=\u0026#34;relative space-y-4\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;w-12 h-12 rounded-xl bg-chart-1/10 flex items-center justify-center\u0026#34;\u0026gt; \u0026lt;Mail className=\u0026#34;w-6 h-6 text-chart-1\u0026#34; /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div\u0026gt; \u0026lt;h3 className=\u0026#34;text-sm font-semibold text-muted-foreground uppercase tracking-wide\u0026#34;\u0026gt;Email Status\u0026lt;/h3\u0026gt; \u0026lt;p className=\u0026#34;text-2xl font-bold text-foreground mt-2\u0026#34;\u0026gt;Verified\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {/* User Info + Actions */} \u0026lt;div className=\u0026#34;grid grid-cols-1 lg:grid-cols-3 gap-6\u0026#34;\u0026gt; {/* Main Card */} \u0026lt;div className=\u0026#34;lg:col-span-2 rounded-2xl bg-card border border-border/50 overflow-hidden hover:shadow-lg transition-shadow duration-300\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;h-24 bg-gradient-to-r from-primary to-accent/50\u0026#34; /\u0026gt; \u0026lt;div className=\u0026#34;p-8 -mt-12 relative\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;mb-8\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;w-24 h-24 rounded-2xl bg-card border-4 border-background shadow-lg flex items-center justify-center mb-6\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;w-full h-full bg-gradient-to-br from-primary/20 to-accent/20 rounded-xl flex items-center justify-center\u0026#34;\u0026gt; \u0026lt;User className=\u0026#34;w-12 h-12 text-primary\u0026#34; /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;h3 className=\u0026#34;text-2xl font-bold text-foreground\u0026#34;\u0026gt;{user?.name || user?.email}\u0026lt;/h3\u0026gt; \u0026lt;p className=\u0026#34;text-muted-foreground mt-1\u0026#34;\u0026gt;Authenticated User\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div className=\u0026#34;space-y-4 pt-8 border-t border-border/50\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;flex items-start gap-4\u0026#34;\u0026gt; \u0026lt;Mail className=\u0026#34;w-5 h-5 text-primary mt-1 flex-shrink-0\u0026#34; /\u0026gt; \u0026lt;div className=\u0026#34;flex-1 min-w-0\u0026#34;\u0026gt; \u0026lt;p className=\u0026#34;text-sm text-muted-foreground\u0026#34;\u0026gt;Email Address\u0026lt;/p\u0026gt; \u0026lt;p className=\u0026#34;text-foreground font-medium break-all\u0026#34;\u0026gt;{user?.email}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div className=\u0026#34;flex items-start gap-4\u0026#34;\u0026gt; \u0026lt;Shield className=\u0026#34;w-5 h-5 text-accent mt-1 flex-shrink-0\u0026#34; /\u0026gt; \u0026lt;div className=\u0026#34;flex-1\u0026#34;\u0026gt; \u0026lt;p className=\u0026#34;text-sm text-muted-foreground\u0026#34;\u0026gt;User ID\u0026lt;/p\u0026gt; \u0026lt;p className=\u0026#34;text-foreground font-medium font-mono text-sm\u0026#34;\u0026gt;{user?.userId || \u0026#34;N/A\u0026#34;}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div className=\u0026#34;flex items-start gap-4\u0026#34;\u0026gt; \u0026lt;User className=\u0026#34;w-5 h-5 text-chart-1 mt-1 flex-shrink-0\u0026#34; /\u0026gt; \u0026lt;div className=\u0026#34;flex-1\u0026#34;\u0026gt; \u0026lt;p className=\u0026#34;text-sm text-muted-foreground\u0026#34;\u0026gt;Account Role\u0026lt;/p\u0026gt; \u0026lt;p className=\u0026#34;text-foreground font-medium capitalize\u0026#34;\u0026gt;{user?.role || \u0026#34;Member\u0026#34;}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {/* Actions */} \u0026lt;div className=\u0026#34;space-y-4\u0026#34;\u0026gt; \u0026lt;Link href=\u0026#34;/forgot-password\u0026#34; className=\u0026#34;flex items-center justify-between gap-3 p-4 rounded-xl bg-card border border-border/50 hover:bg-secondary/50 hover:border-border transition-all duration-200 group\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;flex items-center gap-3\u0026#34;\u0026gt; \u0026lt;KeyRound className=\u0026#34;w-5 h-5 text-primary\u0026#34; /\u0026gt; \u0026lt;span className=\u0026#34;font-medium text-foreground\u0026#34;\u0026gt;Change Password\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;ArrowRight className=\u0026#34;w-4 h-4 text-muted-foreground group-hover:translate-x-1 transition-transform\u0026#34; /\u0026gt; \u0026lt;/Link\u0026gt; \u0026lt;button onClick={handleSignOut} className=\u0026#34;w-full flex items-center justify-between gap-3 p-4 rounded-xl bg-destructive/10 border border-destructive/20 hover:bg-destructive/20 transition-all duration-200 group\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;flex items-center gap-3\u0026#34;\u0026gt; \u0026lt;LogOut className=\u0026#34;w-5 h-5 text-destructive\u0026#34; /\u0026gt; \u0026lt;span className=\u0026#34;font-medium text-destructive\u0026#34;\u0026gt;Sign Out\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;ArrowRight className=\u0026#34;w-4 h-4 text-destructive/50 group-hover:translate-x-1 transition-transform\u0026#34; /\u0026gt; \u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/main\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/ProtectedRoute\u0026gt; ); } You now have a stunning, fully working authentication UI — exactly as you designed it.\nNavigation:\nPrevious: 5.5.1 Create Authentication Functions Next Step: 5.5.3 Implement Protected Routes → Add route protection and authentication middleware "},{"uri":"https://rsshive.github.io/fcj-workshop/5-workshop/5.3-aws-cognito/5.3.2-configure-appclient/","title":"Configure App Client","tags":[],"description":"","content":"Configure Application client Open Cloud Shell aws cognito-idp create-user-pool-client \\ --user-pool-id us-east-1_xxxxxxx \\ --client-name WebApp-NoSecret \\ --no-generate-secret \\ --callback-urls \u0026#39;[\u0026#34;http://localhost:3000/\u0026#34;]\u0026#39; \\ --logout-urls \u0026#39;[\u0026#34;http://localhost:3000/\u0026#34;]\u0026#39; \\ --query \u0026#39;UserPoolClient.ClientId\u0026#39; --output text Example output: h1a234example123 you give userpood id to line, then enter you will create AppClient that do no generate secret\nNavigation:\nPrevious: 5.3.1 Create Cognito User Pool Next Step: 5.4 Next.js Project Setup → Set up Next.js and Amplify SDK "},{"uri":"https://rsshive.github.io/fcj-workshop/5-workshop/5.4-next.js-setup/5.4.2-install-amplify/","title":"Install Amplify SDK and Configure Amplify","tags":[],"description":"","content":"Install Amplify SDK After preparing your Next.js project, install the necessary Amplify packages:\nnpm install aws-amplify @aws-amplify/ui-react 2. Create the Amplify Configuration File Create a new file:\nsrc/lib/amplify-config.ts\nAnd add the following code:\nimport { Amplify } from \u0026#39;aws-amplify\u0026#39;; export const amplifyConfig = { Auth: { Cognito: { userPoolId: process.env.NEXT_PUBLIC_COGNITO_USER_POOL_ID!, userPoolClientId: process.env.NEXT_PUBLIC_COGNITO_APP_CLIENT_ID!, loginWith: { username: false, email: true, }, }, }, }; // Configure Amplify with SSR support Amplify.configure(amplifyConfig, { ssr: true, // Important for Next.js }); 3. Add Environment Variables Create a .env.local file and paste your Cognito IDs:\nNEXT_PUBLIC_COGNITO_USER_POOL_ID=us-east-1_xxxxxxxxx NEXT_PUBLIC_COGNITO_APP_CLIENT_ID=xxxxxxxxxxxxxxxxxxxx This ensures Amplify is available across your entire application.\nAmplify is now configured and ready to be used for:\nSign up\nSign in\nSign out\nSession management\nProtected routes\nNext, you will build the UI for authentication.\nNavigation:\nPrevious: 5.4.1 Install Next.js Next Step: 5.5 Cognito Functions → Build auth functions, UI, and protected routes "},{"uri":"https://rsshive.github.io/fcj-workshop/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"Prerequisites Before starting the workshop, ensure you have the following requirements ready.\n1. AWS Account \u0026amp; Permissions You need an AWS account to create and manage Cognito User Pools and work with Amplify.\nRequirements:\nAn AWS Account An IAM user (not root) with the following permissions: AmazonCognitoFullAccess IAMUserChangePassword AWSCloudFormationFullAccess AWSLambda_FullAccess (optional, for triggers) AmazonS3FullAccess (for optional hosting or storage) Steps:\nCreate an account at the AWS console. Create an IAM user with Programmatic access + Management Console. Configure your AWS region (recommended: us-east-1 or ap-southeast-1). 2. Local Development Environment Your local machine should be set up for building and running a Next.js + Amplify application.\nRequired Software Tool Version Purpose Node.js 18.x or later Required for Next.js \u0026amp; Amplify npm 9.x or later Package management Git Latest Version control Code Editor VS Code (recommended) Development AWS CLI (optional) Latest Useful for debugging and credentials setup Verify installation node --version # Should be v18.x or higher npm --version # Should be v9.x or higher git --version # Should show a valid version Navigation:\nPrevious: 5.1 Workshop Overview Next Step: 5.3 AWS Cognito Setup → Create and configure Cognito User Pool "},{"uri":"https://rsshive.github.io/fcj-workshop/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Document version of the proposal\nRafilm: AI-Powered Movie Logging \u0026amp; Recommendation Platform A Serverless AWS Solution for Intelligent Movie Discovery 1. Executive Summary Rafilm is a Letterboxd-inspired movie logging and recommendation platform designed to help general users track their watched films, share reviews, and discover new favorites through AI-powered recommendations. Built as part of the AWS First Cloud Journey (FCJ) internship, Rafilm integrates Amazon Personalize and Bedrock to deliver tailored movie suggestions and conversational recommendations via a chatbot interface.\nThe platform runs fully on AWS Serverless architecture, featuring Amplify-hosted Next.js frontend, Lambda-based backend services connected through API Gateway, and DynamoDB for scalable user and movie data storage. TMDb provides external movie data integration, while Amazon Cognito manages user authentication. Rafilm aims to demonstrate a scalable, intelligent, and cost-efficient architecture capable of supporting multi-user access and interactive experiences.\n2. Problem Statement What’s the Problem? While existing movie platforms like Letterboxd and IMDb offer robust logging and social features, they lack personalized recommendation systems and interactive discovery experiences. Users often rely on external sources or generic trending lists to find what to watch next, leading to irrelevant or repetitive suggestions.\nThe Solution Rafilm integrates a custom recommendation pipeline powered by Amazon Personalize, combined with a Bedrock LLM chatbot that interprets user preferences and generates conversational movie recommendations. Users can log movies, write reviews, and receive curated suggestions—all within one seamless experience. Unlike Letterboxd, Rafilm focuses on data-driven personalization and AI-assisted interaction rather than pure social networking.\nBenefits and Return on Investment By leveraging AWS Serverless services, Rafilm achieves near-zero maintenance cost, pay-per-use scalability, and real-time AI-driven personalization. For the FCJ internship, the project serves as both a technical showcase and a learning artifact for integrating AI services in serverless architectures. Projected cost remains under $1/month during testing, with AWS Free Tier coverage for most usage.\n3. Solution Architecture Rafilm employs a modular serverless architecture using AWS services for scalability, integration, and cost optimization.\nAWS Services Used AWS Amplify: Hosts the Next.js frontend for movie browsing, logging, and chatbot interaction. Amazon Cognito: Handles user registration, login, and session management. Amazon API Gateway: Routes client requests to backend Lambda functions. AWS Lambda: Executes serverless business logic (e.g., CRUD for reviews, fetching TMDb data, triggering recommendations). Amazon DynamoDB: Stores user logs, movie interactions, and preferences. Amazon Personalize: Trains and serves personalized recommendation models. Amazon Bedrock: Provides conversational chatbot functionality for recommendation dialogue. Amazon S3: Stores static assets and backups for logs and model outputs. Component Design Frontend (Next.js): User-friendly interface for movie discovery, logging, and chat-based recommendations. Backend (Lambda + API Gateway): Stateless logic layer handling user operations, movie fetching, and recommendation retrieval. Data Layer (DynamoDB + S3): Stores structured user interactions and movie metadata for model training. AI Layer (Personalize + Bedrock): Personalize analyzes historical user interactions; Bedrock chatbot provides natural language access to personalized results. Authentication (Cognito): Securely manages multi-user access. Architecture Overview Users log in via Cognito and interact with the Next.js interface. Actions such as logging or rating trigger API Gateway → Lambda → DynamoDB workflows. The Bedrock chatbot accesses Personalize results to generate conversational movie suggestions. Amplify hosts the frontend for seamless deployment and scalability. 4. Technical Implementation Implementation Phases Architecture Design (Month 1): Research AWS serverless and AI integration patterns; finalize architecture diagrams. Prototype Integration (Month 2): Implement Amplify hosting, Cognito authentication, and Lambda-based backend APIs. Recommendation System (Month 3): Connect Personalize and Bedrock for end-to-end AI recommendation and chatbot response. Testing \u0026amp; Deployment: Conduct functional testing, optimize costs, and deploy production-ready version on Amplify. Technical Requirements Frontend: Next.js + React hosted via AWS Amplify, using TMDb API for movie data. Backend: AWS Lambda (Node.js runtime) connected through API Gateway. Database: Amazon DynamoDB for scalable user and review data. AI Components: Amazon Personalize (user-item recommendations) and Bedrock (chatbot dialogue). Authentication: Amazon Cognito for secure, multi-user access. Automation: AWS SDK \u0026amp; CloudFormation for provisioning; AWS SAM for deployment workflows. 5. Timeline \u0026amp; Milestones Phase Duration Key Deliverables Month 1 Research \u0026amp; Architecture AWS design finalizing Month 2 Core Development Amplify hosting, Cognito setup, Lambda API, DynamoDB schema Month 3 AI Integration \u0026amp; Testing Personalize training, Bedrock chatbot, system deployment Post-Launch Continuous Improvement Cost optimization, new features, UX refinement 6. Budget Estimation Estimated Monthly Cost: $40.09 USD ($481.08 for 12 months)\nThe cost estimate of this project is projected in this link: https://calculator.aws/#/estimate?id=dab7fb57dabfb76041cdba98ac2bac7ba9630046\nThe monthly cost estimate of $40.09 USD is calculated based on the following specific usage assumptions derived from the AWS Pricing Calculator:\nStandard Usage: Assumes 10,000 requests per month for both AWS Lambda and Amazon API Gateway. User Base: Assumes a maximum of 10,000 Monthly Active Users (MAU) for Amazon Cognito. AI/ML Usage (Primary Cost Driver): Amazon Bedrock is estimated for continuous operation (24 hours per day) with an average of 1 request per minute and 100 input/output tokens per request. Amazon Personalize includes 1 GB of data ingested and 15 training hours per month. Security Overhead: Assumes use of 1 AWS WAF Web ACL with 4 Rules and 3 Managed Rule Groups. Data Storage: Assumes 0.5 GB of data storage in Amazon DynamoDB. 7. Risk Assessment Risk Probability Impact Mitigation API rate limits from TMDb Medium Medium Cache requests via Lambda Model training cost escalation Low Medium Use limited training dataset for testing Chatbot latency Medium Low Optimize Bedrock model type and response size Authentication or token expiry Medium Low Use short-lived JWTs and refresh tokens 8. Expected Outcomes Technical Improvements Demonstrates serverless integration of AI/ML and LLM services in real-world use. Establishes a reusable AWS architecture for recommendation-based apps. Long-Term Value Provides a foundation for future expansion into a social movie discovery network. Serves as an AWS FCJ internship showcase project highlighting scalability, personalization, and conversational AI. "},{"uri":"https://rsshive.github.io/fcj-workshop/5-workshop/5.5-cognito-function/5.5.3-protected-routes/","title":"Implement Auth Context &amp; Protected Routes","tags":[],"description":"","content":"5.5.3 Implement Auth Context \u0026amp; Protected Routes Global authentication state + automatic route protection\nYou’ve built the Cognito logic and the beautiful UI.\nNow you’ll connect everything with a global AuthContext and a smart ProtectedRoute — exactly as you designed.\n1. Global Auth Context – context/AuthContext.tsx \u0026#39;use client\u0026#39;; import { createContext, useContext, useEffect, useState } from \u0026#39;react\u0026#39;; import { getCognitoUser, cognitoSignOut } from \u0026#39;@/lib/cognito-auth\u0026#39;; interface User { userId: string; email: string; name: string; role: \u0026#39;admin\u0026#39; | \u0026#39;user\u0026#39;; } interface AuthContextType { user: User | null; loading: boolean; signOut: () =\u0026gt; Promise\u0026lt;void\u0026gt;; refreshUser: () =\u0026gt; Promise\u0026lt;void\u0026gt;; } const AuthContext = createContext\u0026lt;AuthContextType | undefined\u0026gt;(undefined); export function AuthProvider({ children }: { children: React.ReactNode }) { const [user, setUser] = useState\u0026lt;User | null\u0026gt;(null); const [loading, setLoading] = useState(true); const loadUser = async () =\u0026gt; { try { const userData = await getCognitoUser(); if (userData) { setUser({ userId: userData.userId, email: userData.email, name: userData.name, role: userData.role === \u0026#39;admin\u0026#39; ? \u0026#39;admin\u0026#39; : \u0026#39;user\u0026#39;, }); } else { setUser(null); } } catch (error) { console.error(\u0026#39;Failed to load user:\u0026#39;, error); setUser(null); } finally { setLoading(false); } }; useEffect(() =\u0026gt; { loadUser(); }, []); const signOut = async () =\u0026gt; { try { await cognitoSignOut(); setUser(null); } catch (error) { console.error(\u0026#39;Sign out error:\u0026#39;, error); } }; return ( \u0026lt;AuthContext.Provider value={{ user, loading, signOut, refreshUser: loadUser }}\u0026gt; {children} \u0026lt;/AuthContext.Provider\u0026gt; ); } export function useAuth() { const context = useContext(AuthContext); if (!context) { throw new Error(\u0026#39;useAuth must be used within an AuthProvider\u0026#39;); } return context; } 2. Protected Route Component – components/ProtectedRoute.tsx \u0026#39;use client\u0026#39;; import { useEffect } from \u0026#39;react\u0026#39;; import { useRouter } from \u0026#39;next/navigation\u0026#39;; import { useAuth } from \u0026#39;@/context/AuthContext\u0026#39;; export function ProtectedRoute({ children }: { children: React.ReactNode }) { const { user, loading } = useAuth(); const router = useRouter(); useEffect(() =\u0026gt; { if (!loading \u0026amp;\u0026amp; !user) { router.push(\u0026#39;/signin\u0026#39;); } }, [user, loading, router]); if (loading) { return ( \u0026lt;div className=\u0026#34;min-h-screen flex items-center justify-center\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;text-xl font-medium\u0026#34;\u0026gt;Loading...\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; ); } if (!user) { return null; // Redirecting... } return \u0026lt;\u0026gt;{children}\u0026lt;/\u0026gt;; } Congratulations! You\u0026rsquo;ve built a 2025-standard, AWS-native authentication flow that top-tier teams use daily.\nNavigation:\nPrevious: 5.5.2 Build Authentication UI Next Step: 5.5.4 Project Demo → Run and test the complete authentication system "},{"uri":"https://rsshive.github.io/fcj-workshop/5-workshop/5.5-cognito-function/5.5.4-use-project-demo/","title":"Use demo project for cognito authentication","tags":[],"description":"","content":"5.5.4 Use demo project for cognito authentication Global authentication state + automatic route protection\nI have sourecode contain the UI . So You’ve built the Cognito logic\nNow you’ll connect everything with a global AuthContext and a smart ProtectedRoute — exactly as you designed.\n1. Global Auth Context – context/AuthContext.tsx \u0026#39;use client\u0026#39;; import { createContext, useContext, useEffect, useState } from \u0026#39;react\u0026#39;; import { getCognitoUser, cognitoSignOut } from \u0026#39;@/lib/cognito-auth\u0026#39;; interface User { userId: string; email: string; name: string; role: \u0026#39;admin\u0026#39; | \u0026#39;user\u0026#39;; } interface AuthContextType { user: User | null; loading: boolean; signOut: () =\u0026gt; Promise\u0026lt;void\u0026gt;; refreshUser: () =\u0026gt; Promise\u0026lt;void\u0026gt;; } const AuthContext = createContext\u0026lt;AuthContextType | undefined\u0026gt;(undefined); export function AuthProvider({ children }: { children: React.ReactNode }) { const [user, setUser] = useState\u0026lt;User | null\u0026gt;(null); const [loading, setLoading] = useState(true); const loadUser = async () =\u0026gt; { try { const userData = await getCognitoUser(); if (userData) { setUser({ userId: userData.userId, email: userData.email, name: userData.name, role: userData.role === \u0026#39;admin\u0026#39; ? \u0026#39;admin\u0026#39; : \u0026#39;user\u0026#39;, }); } else { setUser(null); } } catch (error) { console.error(\u0026#39;Failed to load user:\u0026#39;, error); setUser(null); } finally { setLoading(false); } }; useEffect(() =\u0026gt; { loadUser(); }, []); const signOut = async () =\u0026gt; { try { await cognitoSignOut(); setUser(null); } catch (error) { console.error(\u0026#39;Sign out error:\u0026#39;, error); } }; return ( \u0026lt;AuthContext.Provider value={{ user, loading, signOut, refreshUser: loadUser }}\u0026gt; {children} \u0026lt;/AuthContext.Provider\u0026gt; ); } export function useAuth() { const context = useContext(AuthContext); if (!context) { throw new Error(\u0026#39;useAuth must be used within an AuthProvider\u0026#39;); } return context; } 2. Protected Route Component – components/ProtectedRoute.tsx \u0026#39;use client\u0026#39;; import { useEffect } from \u0026#39;react\u0026#39;; import { useRouter } from \u0026#39;next/navigation\u0026#39;; import { useAuth } from \u0026#39;@/context/AuthContext\u0026#39;; export function ProtectedRoute({ children }: { children: React.ReactNode }) { const { user, loading } = useAuth(); const router = useRouter(); useEffect(() =\u0026gt; { if (!loading \u0026amp;\u0026amp; !user) { router.push(\u0026#39;/signin\u0026#39;); } }, [user, loading, router]); if (loading) { return ( \u0026lt;div className=\u0026#34;min-h-screen flex items-center justify-center\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;text-xl font-medium\u0026#34;\u0026gt;Loading...\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; ); } if (!user) { return null; // Redirecting... } return \u0026lt;\u0026gt;{children}\u0026lt;/\u0026gt;; } when done all thing . i have to add protected to dashboard to block people dont login to access dashboard and protected page .\njust add protected page like this\n\u0026#34;use client\u0026#34; import { ProtectedRoute } from \u0026#34;@/components/ProtectedRoute\u0026#34; import { useAuth } from \u0026#34;@/context/AuthContext\u0026#34; import { LogOut, User, Mail, Shield, KeyRound, ArrowRight } from \u0026#34;lucide-react\u0026#34; import Link from \u0026#34;next/link\u0026#34; export default function DashboardPage() { const { user, signOut } = useAuth() const handleSignOut = async () =\u0026gt; { await signOut() } return ( \u0026lt;ProtectedRoute\u0026gt; \u0026lt;div className=\u0026#34;min-h-screen bg-gradient-to-br from-background via-background to-secondary/5\u0026#34;\u0026gt; {/* Header */} \u0026lt;header className=\u0026#34;border-b border-border/50 backdrop-blur-sm sticky top-0 z-50\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;max-w-6xl mx-auto px-6 py-4 flex items-center justify-between\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;flex items-center gap-3\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;w-10 h-10 rounded-lg bg-gradient-to-br from-primary to-primary/60 flex items-center justify-center\u0026#34;\u0026gt; \u0026lt;User className=\u0026#34;w-6 h-6 text-primary-foreground\u0026#34; /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;h1 className=\u0026#34;text-xl font-bold text-foreground\u0026#34;\u0026gt;Dashboard\u0026lt;/h1\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;button onClick={handleSignOut} className=\u0026#34;inline-flex items-center gap-2 px-4 py-2 rounded-lg text-sm font-medium bg-destructive/10 text-destructive hover:bg-destructive/20 transition-colors\u0026#34; \u0026gt; \u0026lt;LogOut className=\u0026#34;w-4 h-4\u0026#34; /\u0026gt; Sign Out \u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/header\u0026gt; {/* Main Content */} \u0026lt;main className=\u0026#34;max-w-6xl mx-auto px-6 py-12\u0026#34;\u0026gt; {/* Welcome Section */} \u0026lt;div className=\u0026#34;mb-12\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;space-y-2 mb-8\u0026#34;\u0026gt; \u0026lt;h2 className=\u0026#34;text-4xl font-bold text-foreground\u0026#34;\u0026gt;Welcome back\u0026lt;/h2\u0026gt; \u0026lt;p className=\u0026#34;text-lg text-muted-foreground\u0026#34;\u0026gt;{user?.name || user?.email}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; {/* Stats Cards */} \u0026lt;div className=\u0026#34;grid grid-cols-1 md:grid-cols-3 gap-6\u0026#34;\u0026gt; {/* Profile Status Card */} \u0026lt;div className=\u0026#34;group relative overflow-hidden rounded-2xl bg-card border border-border/50 p-6 hover:shadow-lg transition-shadow duration-300\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;absolute inset-0 bg-gradient-to-br from-primary/5 to-transparent opacity-0 group-hover:opacity-100 transition-opacity\u0026#34; /\u0026gt; \u0026lt;div className=\u0026#34;relative space-y-4\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;w-12 h-12 rounded-xl bg-primary/10 flex items-center justify-center\u0026#34;\u0026gt; \u0026lt;Shield className=\u0026#34;w-6 h-6 text-primary\u0026#34; /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div\u0026gt; \u0026lt;h3 className=\u0026#34;text-sm font-semibold text-muted-foreground uppercase tracking-wide\u0026#34;\u0026gt; Account Status \u0026lt;/h3\u0026gt; \u0026lt;p className=\u0026#34;text-2xl font-bold text-foreground mt-2\u0026#34;\u0026gt;Active\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {/* Role Card */} \u0026lt;div className=\u0026#34;group relative overflow-hidden rounded-2xl bg-card border border-border/50 p-6 hover:shadow-lg transition-shadow duration-300\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;absolute inset-0 bg-gradient-to-br from-accent/5 to-transparent opacity-0 group-hover:opacity-100 transition-opacity\u0026#34; /\u0026gt; \u0026lt;div className=\u0026#34;relative space-y-4\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;w-12 h-12 rounded-xl bg-accent/10 flex items-center justify-center\u0026#34;\u0026gt; \u0026lt;User className=\u0026#34;w-6 h-6 text-accent\u0026#34; /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div\u0026gt; \u0026lt;h3 className=\u0026#34;text-sm font-semibold text-muted-foreground uppercase tracking-wide\u0026#34;\u0026gt;User Role\u0026lt;/h3\u0026gt; \u0026lt;p className=\u0026#34;text-2xl font-bold text-foreground mt-2 capitalize\u0026#34;\u0026gt;{user?.role || \u0026#34;Member\u0026#34;}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {/* Verification Status */} \u0026lt;div className=\u0026#34;group relative overflow-hidden rounded-2xl bg-card border border-border/50 p-6 hover:shadow-lg transition-shadow duration-300\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;absolute inset-0 bg-gradient-to-br from-chart-1/5 to-transparent opacity-0 group-hover:opacity-100 transition-opacity\u0026#34; /\u0026gt; \u0026lt;div className=\u0026#34;relative space-y-4\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;w-12 h-12 rounded-xl bg-chart-1/10 flex items-center justify-center\u0026#34;\u0026gt; \u0026lt;Mail className=\u0026#34;w-6 h-6 text-chart-1\u0026#34; /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div\u0026gt; \u0026lt;h3 className=\u0026#34;text-sm font-semibold text-muted-foreground uppercase tracking-wide\u0026#34;\u0026gt; Email Status \u0026lt;/h3\u0026gt; \u0026lt;p className=\u0026#34;text-2xl font-bold text-foreground mt-2\u0026#34;\u0026gt;Verified\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {/* User Information Section */} \u0026lt;div className=\u0026#34;grid grid-cols-1 lg:grid-cols-3 gap-6\u0026#34;\u0026gt; {/* Main User Card */} \u0026lt;div className=\u0026#34;lg:col-span-2 rounded-2xl bg-card border border-border/50 overflow-hidden hover:shadow-lg transition-shadow duration-300\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;h-24 bg-gradient-to-r from-primary to-accent/50\u0026#34; /\u0026gt; \u0026lt;div className=\u0026#34;p-8 -mt-12 relative\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;mb-8\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;w-24 h-24 rounded-2xl bg-card border-4 border-background shadow-lg flex items-center justify-center mb-6\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;w-full h-full bg-gradient-to-br from-primary/20 to-accent/20 rounded-xl flex items-center justify-center\u0026#34;\u0026gt; \u0026lt;User className=\u0026#34;w-12 h-12 text-primary\u0026#34; /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div\u0026gt; \u0026lt;h3 className=\u0026#34;text-2xl font-bold text-foreground\u0026#34;\u0026gt;{user?.name || user?.email}\u0026lt;/h3\u0026gt; \u0026lt;p className=\u0026#34;text-muted-foreground mt-1\u0026#34;\u0026gt;Authenticated User\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div className=\u0026#34;space-y-4 pt-8 border-t border-border/50\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;flex items-start gap-4\u0026#34;\u0026gt; \u0026lt;Mail className=\u0026#34;w-5 h-5 text-primary mt-1 flex-shrink-0\u0026#34; /\u0026gt; \u0026lt;div className=\u0026#34;flex-1 min-w-0\u0026#34;\u0026gt; \u0026lt;p className=\u0026#34;text-sm text-muted-foreground\u0026#34;\u0026gt;Email Address\u0026lt;/p\u0026gt; \u0026lt;p className=\u0026#34;text-foreground font-medium break-all\u0026#34;\u0026gt;{user?.email}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div className=\u0026#34;flex items-start gap-4\u0026#34;\u0026gt; \u0026lt;Shield className=\u0026#34;w-5 h-5 text-accent mt-1 flex-shrink-0\u0026#34; /\u0026gt; \u0026lt;div className=\u0026#34;flex-1\u0026#34;\u0026gt; \u0026lt;p className=\u0026#34;text-sm text-muted-foreground\u0026#34;\u0026gt;User ID\u0026lt;/p\u0026gt; \u0026lt;p className=\u0026#34;text-foreground font-medium font-mono text-sm\u0026#34;\u0026gt;{user?.userId || \u0026#34;N/A\u0026#34;}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div className=\u0026#34;flex items-start gap-4\u0026#34;\u0026gt; \u0026lt;User className=\u0026#34;w-5 h-5 text-chart-1 mt-1 flex-shrink-0\u0026#34; /\u0026gt; \u0026lt;div className=\u0026#34;flex-1\u0026#34;\u0026gt; \u0026lt;p className=\u0026#34;text-sm text-muted-foreground\u0026#34;\u0026gt;Account Role\u0026lt;/p\u0026gt; \u0026lt;p className=\u0026#34;text-foreground font-medium capitalize\u0026#34;\u0026gt;{user?.role || \u0026#34;Member\u0026#34;}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {/* Actions Sidebar */} \u0026lt;div className=\u0026#34;space-y-4\u0026#34;\u0026gt; \u0026lt;Link href=\u0026#34;/forgot-password\u0026#34; className=\u0026#34;flex items-center justify-between gap-3 p-4 rounded-xl bg-card border border-border/50 hover:bg-secondary/50 hover:border-border transition-all duration-200 group\u0026#34; \u0026gt; \u0026lt;div className=\u0026#34;flex items-center gap-3\u0026#34;\u0026gt; \u0026lt;KeyRound className=\u0026#34;w-5 h-5 text-primary\u0026#34; /\u0026gt; \u0026lt;span className=\u0026#34;font-medium text-foreground\u0026#34;\u0026gt;Change Password\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;ArrowRight className=\u0026#34;w-4 h-4 text-muted-foreground group-hover:translate-x-1 transition-transform\u0026#34; /\u0026gt; \u0026lt;/Link\u0026gt; \u0026lt;button onClick={handleSignOut} className=\u0026#34;w-full flex items-center justify-between gap-3 p-4 rounded-xl bg-destructive/10 border border-destructive/20 hover:bg-destructive/20 transition-all duration-200 group\u0026#34; \u0026gt; \u0026lt;div className=\u0026#34;flex items-center gap-3\u0026#34;\u0026gt; \u0026lt;LogOut className=\u0026#34;w-5 h-5 text-destructive\u0026#34; /\u0026gt; \u0026lt;span className=\u0026#34;font-medium text-destructive\u0026#34;\u0026gt;Sign Out\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;ArrowRight className=\u0026#34;w-4 h-4 text-destructive/50 group-hover:translate-x-1 transition-transform\u0026#34; /\u0026gt; \u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/main\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/ProtectedRoute\u0026gt; ) } Congratulations! You\u0026rsquo;ve built a 2025-standard, AWS-native authentication flow that top-tier teams use daily.\nNavigation:\nPrevious: 5.5.3 Implement Protected Routes Next Step: 5.6 Testing \u0026amp; Verification → Run and verify the complete authentication system "},{"uri":"https://rsshive.github.io/fcj-workshop/5-workshop/5.3-aws-cognito/","title":"AWS Cognito Setup","tags":[],"description":"","content":"AWS Cognito Setup In this section, you will set up Amazon Cognito User Pool and configure essential settings such as login options, password policies, and app client settings.\nThis configuration will be used later in the Next.js project through the Amplify SDK.\nContent Create a Cognito User Pool Configure App Client Navigation:\nPrevious: 5.2 Prerequisites Next Step: 5.3.1 Next.js Project Setup → Create Cognito "},{"uri":"https://rsshive.github.io/fcj-workshop/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Well-Architected Security Pillar Workshop\u0026rdquo; Event Information Event: AWS Well-Architected Security Pillar Workshop Date: Friday, November 29th, 2025 Time: 8:30 AM - 12:00 PM (Morning Session) Location: AWS Vietnam Office, Bitexco Financial Tower, 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City My Role: Attendee Event Overview This intensive half-day workshop provided a comprehensive deep dive into the Security Pillar of the AWS Well-Architected Framework. The session was specifically tailored to address security challenges and best practices relevant to Vietnamese enterprises adopting cloud technologies. The workshop covered all five fundamental pillars of AWS security through structured presentations, live demonstrations, and real-world incident response scenarios.\nDetailed Agenda \u0026amp; Key Takeaways 8:30 – 8:50 AM | Opening \u0026amp; Security Foundation The workshop began with an introduction to the critical role of the Security Pillar within the AWS Well-Architected Framework. Key topics covered included:\nCore Security Principles:\nLeast Privilege: Granting only the minimum permissions necessary for users and services to perform their tasks Zero Trust Architecture: Never trust, always verify - assuming no implicit trust regardless of network location Defense in Depth: Implementing multiple layers of security controls throughout the IT infrastructure Shared Responsibility Model: Understanding the division of security responsibilities between AWS (security OF the cloud) and customers (security IN the cloud)\nTop Cloud Security Threats in Vietnam: Discussion of the most prevalent security challenges facing Vietnamese organizations, including data breaches, misconfigured resources, and credential compromises\nPillar 1: Identity \u0026amp; Access Management (8:50 – 9:30 AM) This session focused on modern IAM architecture and best practices:\nIAM Fundamentals:\nUnderstanding Users, Roles, and Policies Avoiding long-term credentials and embracing temporary security credentials Implementing role-based access control (RBAC) IAM Identity Center:\nImplementing Single Sign-On (SSO) across AWS accounts Managing permission sets for centralized access control Streamlining user management in multi-account environments Advanced IAM Concepts:\nService Control Policies (SCPs) for organizational boundaries Permission boundaries to limit maximum permissions Multi-Factor Authentication (MFA) enforcement Credential rotation policies and automation IAM Access Analyzer for identifying overly permissive policies Mini Demo: The instructor demonstrated how to validate IAM policies using the IAM Policy Simulator and simulate user access to identify potential security gaps.\nKey Learnings:\nAlways use IAM roles instead of IAM users for applications Implement MFA for all privileged accounts Regularly audit permissions using Access Analyzer Use permission boundaries to prevent privilege escalation Pillar 2: Detection (9:30 – 9:55 AM) This segment covered continuous monitoring and threat detection capabilities:\nLogging Services:\nAWS CloudTrail: Organization-level API activity logging and governance Amazon GuardDuty: Intelligent threat detection using machine learning AWS Security Hub: Centralized security findings and compliance status Comprehensive Logging Strategy:\nVPC Flow Logs for network traffic analysis Application Load Balancer (ALB) access logs S3 access logs for data access patterns CloudWatch Logs for application and system monitoring Alerting \u0026amp; Automation:\nUsing Amazon EventBridge to create event-driven security responses Automated remediation workflows Integration with incident management systems Detection-as-Code: Implementing security detection rules as infrastructure code for versioning and reproducibility\nKey Learnings:\nEnable CloudTrail at the organization level for comprehensive visibility Configure GuardDuty findings to trigger automated responses Centralize security findings in Security Hub for unified visibility Implement log retention policies based on compliance requirements Coffee Break (9:55 – 10:10 AM) Pillar 3: Infrastructure Protection (10:10 – 10:40 AM) This session addressed network and workload security:\nVPC Segmentation:\nDesigning multi-tier architectures with public and private subnets Strategic placement of resources (public vs. private) Network isolation best practices Network Security Controls:\nSecurity Groups: Stateful firewall rules at the instance level Network ACLs (NACLs): Stateless firewall rules at the subnet level Understanding when to use each control and how they complement each other Advanced Network Protection:\nAWS WAF (Web Application Firewall): Protecting web applications from common exploits AWS Shield: DDoS protection (Standard and Advanced tiers) AWS Network Firewall: Managed network firewall service for VPC-level protection Workload Security:\nEC2 instance hardening and patch management Container security for ECS and EKS Secure configuration baselines Key Learnings:\nUse private subnets for application and database tiers Implement Security Groups with minimal required ports Deploy WAF with managed rule groups for common threats Consider AWS Network Firewall for advanced traffic inspection Pillar 4: Data Protection (10:40 – 11:10 AM) This critical section covered encryption and data security:\nAWS Key Management Service (KMS):\nKey policies and grants management Automatic key rotation strategies Customer-managed keys vs. AWS-managed keys Encryption Best Practices:\nAt-Rest Encryption: S3, EBS volumes, RDS databases, DynamoDB tables In-Transit Encryption: TLS/SSL for all data transmission End-to-end encryption patterns Secrets Management:\nAWS Secrets Manager: Automatic rotation of database credentials and API keys Systems Manager Parameter Store: Secure storage for configuration data Comparison and use cases for each service Data Governance:\nData classification strategies Implementing access guardrails based on data sensitivity Compliance considerations (GDPR, PDP, etc.) Key Learnings:\nEnable encryption by default for all data stores Use KMS customer-managed keys for sensitive data requiring audit trails Implement automatic secret rotation for database credentials Apply data classification tags for access control policies Pillar 5: Incident Response (11:10 – 11:40 AM) The final technical session focused on incident response preparedness:\nIR Lifecycle According to AWS:\nPreparation Detection and Analysis Containment, Eradication, and Recovery Post-Incident Activity Practical Incident Response Playbooks:\nScenario 1: Compromised IAM Access Key\nDetection through GuardDuty or CloudTrail anomalies Immediate steps: Disable the key, review CloudTrail logs Investigation: Identify scope of unauthorized access Remediation: Rotate keys, review IAM policies, implement additional controls Scenario 2: S3 Bucket Public Exposure\nDetection through S3 access logs or Security Hub findings Immediate containment: Block public access Assessment: Identify exposed data and potential data exfiltration Remediation: Enable S3 Block Public Access, implement bucket policies Scenario 3: EC2 Instance Malware Detection\nDetection through GuardDuty malware findings Isolation: Modify security groups to prevent lateral movement Evidence collection: Create EBS snapshots and memory dumps Analysis: Investigate malware type and entry vector Recovery: Terminate instance, deploy clean replacement Forensic Best Practices:\nCreating snapshots for forensic analysis without disrupting production Network isolation techniques Evidence collection and chain of custody Compliance with legal requirements Automated Incident Response:\nUsing AWS Lambda functions for immediate response actions AWS Step Functions for complex IR workflows Integration with ticketing and communication systems Automated notification to security teams Key Learnings:\nPrepare IR playbooks before incidents occur Automate common response actions to reduce response time Always preserve evidence through snapshots before remediation Practice IR scenarios regularly through tabletop exercises 11:40 – 12:00 PM | Wrap-Up \u0026amp; Q\u0026amp;A The workshop concluded with a comprehensive review and practical discussion:\nFive Pillars Summary: Recap of Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response\nCommon Pitfalls in Vietnamese Enterprises:\nOver-reliance on root account credentials Insufficient logging and monitoring Lack of encryption for sensitive data Inadequate incident response preparedness Complex IAM policies leading to security gaps Security Learning Roadmap:\nRecommended AWS Security specialty certification path AWS Solutions Architect Professional (SA Pro) for advanced architecture Hands-on practice with AWS Security services Participation in AWS security workshops and events Personal Reflections \u0026amp; Value Gained This workshop was exceptionally valuable for deepening my understanding of AWS security best practices. Key benefits included:\nPractical Security Knowledge: Moving beyond theoretical concepts to understand real-world implementation of security controls in production environments\nIncident Response Skills: Learning structured approaches to handling security incidents with concrete playbooks that can be applied immediately\nHolistic Security Perspective: Understanding how different security services work together to create a comprehensive security posture\nVietnamese Context: Appreciated the discussion of security challenges specific to Vietnamese enterprises, making the content highly relevant\nCareer Development: Gained clarity on the AWS Security Specialty certification path and how to structure my security learning journey\nHands-on Demonstrations: The live demos, particularly the IAM Policy Simulator, provided practical skills I can apply in real projects\nNetworking Opportunity: Connected with AWS security experts and fellow attendees, expanding my professional network in the cloud security domain\nApplication to Current Work The knowledge gained from this workshop directly applies to my internship work:\nWorkshop Security Enhancement: I can now implement better security controls in the S3 VPC endpoint workshop I\u0026rsquo;m developing IAM Best Practices: Applied least privilege principles to the IAM roles and policies in my projects Monitoring Implementation: Better understanding of how to implement comprehensive logging and monitoring Security Documentation: Improved ability to document security considerations and best practices in technical content Conclusion The AWS Well-Architected Security Pillar Workshop was an excellent investment of time, providing both breadth and depth in AWS security practices. The structured approach covering all five security pillars, combined with practical playbooks and Vietnamese market insights, made this an invaluable learning experience. I highly recommend this workshop to anyone looking to strengthen their AWS security knowledge and implement production-grade security controls.\nThe emphasis on automation, monitoring, and incident response preparedness has fundamentally changed how I approach security in cloud architectures. Moving forward, I will incorporate these security best practices into all my AWS projects and continue pursuing the AWS Security Specialty certification.\n"},{"uri":"https://rsshive.github.io/fcj-workshop/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"AWS Technical Blogs - Vietnamese Translation Collection As part of my internship at AWS First Cloud Journey, I translated several technical AWS blog posts from English to Vietnamese to help the Vietnamese-speaking community access valuable AWS knowledge and best practices. This initiative contributes to making cloud computing resources more accessible to local developers and cloud practitioners.\nEach translation maintains the technical accuracy of the original content while adapting the language to be natural and understandable for Vietnamese readers. The blogs cover diverse AWS topics including storage optimization, compute services, and emerging technologies like Physical AI.\nTranslated Blogs: Blog 1 - How to size an Amazon FSx for NetApp ONTAP file system This comprehensive guide helps you determine the appropriate sizing for Amazon FSx for NetApp ONTAP file systems. It covers key concepts like storage efficiency, data deduplication, compression ratios, and capacity planning strategies. The blog provides practical formulas and real-world examples for calculating required storage capacity when migrating from on-premises NetApp ONTAP systems to AWS, ensuring optimal performance and cost-effectiveness.\nBlog 2 - Introducing default instance categories for AWS Batch AWS Batch introduces simplified instance selection with default_x86_64 and default_arm64 categories. This blog explains how these new default categories automatically choose optimal EC2 instance types for your batch workloads, eliminating the need to manually specify instance families. Learn about the benefits of automatic instance selection, cost optimization opportunities, and how to leverage both x86 and ARM-based instances for different workload requirements.\nBlog 3 - Preventing machine breakdowns: How Physical AI predicts equipment problems Explore the cutting-edge application of Physical AI in predictive maintenance for industrial equipment. This blog demonstrates how AWS IoT FleetWise collects real-time vehicle and machinery data, while Amazon SageMaker builds machine learning models to predict potential failures before they occur. Discover how companies can reduce downtime, optimize maintenance schedules, and transform from reactive to proactive maintenance strategies using AWS AI/ML services and IoT technologies.\n"},{"uri":"https://rsshive.github.io/fcj-workshop/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Summary Report: \u0026ldquo;DevOps on AWS - Full-Day Workshop\u0026rdquo; Event Information Event: DevOps on AWS Workshop Date: Monday, November 17th, 2025 Time: 8:30 AM - 5:00 PM (Full Day) Location: AWS Vietnam Office, Bitexco Financial Tower, 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City My Role: Attendee Event Overview This comprehensive full-day workshop provided an in-depth exploration of DevOps practices and tools on AWS. The session covered the complete DevOps lifecycle from source control to monitoring, with hands-on demonstrations of CI/CD pipelines, Infrastructure as Code, container orchestration, and observability solutions. The workshop balanced theoretical concepts with practical implementations, featuring live demos and real-world case studies from both startups and enterprises.\nMorning Session (8:30 AM – 12:00 PM) 8:30 – 9:00 AM | Welcome \u0026amp; DevOps Mindset The workshop began with a comprehensive introduction to DevOps culture and principles:\nRecap of Previous AI/ML Session: Brief review connecting AI/ML workflows with DevOps automation practices\nDevOps Culture and Principles:\nCollaboration: Breaking down silos between development and operations teams Automation: Automating repetitive tasks to increase efficiency and reduce human error Continuous Improvement: Iterative approach to optimizing processes and systems Fast Feedback Loops: Rapid detection and resolution of issues Shared Responsibility: Everyone owns the quality and reliability of the system Key Benefits of DevOps:\nFaster time to market Improved deployment frequency Lower failure rate of new releases Shorter lead time between fixes Faster mean time to recovery (MTTR) DevOps Metrics (DORA Metrics):\nDeployment Frequency: How often code is deployed to production Lead Time for Changes: Time from commit to production deployment Mean Time to Recovery (MTTR): Average time to recover from failures Change Failure Rate: Percentage of deployments causing failures Key Learnings: Understanding that DevOps is not just about tools, but fundamentally about culture, collaboration, and continuous improvement mindset.\n9:00 – 10:30 AM | AWS DevOps Services – CI/CD Pipeline This intensive session covered the complete AWS CI/CD toolchain:\nSource Control AWS CodeCommit:\nFully managed Git repositories hosted on AWS Encryption at rest and in transit Integration with IAM for access control Branch protection and merge strategies Git Strategies:\nGitFlow: Feature branches, develop, release, and main branches for structured releases Trunk-Based Development: Short-lived feature branches merged directly to main/trunk Comparison of approaches and when to use each Build \u0026amp; Test AWS CodeBuild:\nManaged build service that compiles code and runs tests buildspec.yml configuration for build phases Environment variables and parameter management Integration with ECR for container image builds Caching strategies for faster builds Testing Pipelines:\nUnit tests, integration tests, and security scans Parallel test execution for faster feedback Test result reporting and metrics Deployment AWS CodeDeploy: Blue/Green Deployment: Switching traffic between two identical environments Canary Deployment: Gradually rolling out changes to a subset of users Rolling Updates: Incremental updates to maintain availability Automatic rollback on deployment failures Integration with Auto Scaling Groups and Lambda functions Orchestration AWS CodePipeline: Automated workflow orchestrating all CI/CD stages Visual pipeline editor and stage management Integration with third-party tools (GitHub, Jenkins, etc.) Approval stages for production deployments Pipeline execution history and artifact management Live Demo: Full CI/CD Pipeline The instructor demonstrated a complete pipeline:\nCode commit triggers CodePipeline CodeBuild compiles and tests the application Docker image pushed to Amazon ECR CodeDeploy executes Blue/Green deployment to ECS Automated rollback on failed health checks Key Learnings:\nAlways implement automated testing before deployment stages Use Blue/Green deployments for zero-downtime releases Configure automatic rollbacks to minimize impact of failed deployments Monitor pipeline execution times and optimize build caching 10:30 – 10:45 AM | Coffee Break 10:45 AM – 12:00 PM | Infrastructure as Code (IaC) This session explored AWS\u0026rsquo;s Infrastructure as Code tools:\nAWS CloudFormation Templates: YAML/JSON definitions of AWS resources Stacks: Deployed instances of templates as single units Change Sets: Preview infrastructure changes before applying Drift Detection: Identifying manual changes to resources StackSets: Deploying stacks across multiple accounts and regions Best Practices: Parameterized templates for reusability Using nested stacks for modular infrastructure Implementing stack policies to prevent accidental deletions AWS CDK (Cloud Development Kit) Programming Language Support: TypeScript, Python, Java, C#, Go Constructs: L1 (CloudFormation Resources): Direct mappings to CloudFormation resources L2 (Curated Constructs): Higher-level abstractions with sensible defaults L3 (Patterns): Complete architectural patterns Reusable Patterns: Sharing infrastructure code across projects CDK Advantages: Type safety and IDE autocomplete Using familiar programming languages Testing infrastructure code with unit tests More concise than CloudFormation templates Live Demo: Deploying with CloudFormation and CDK Two parallel demonstrations:\nCloudFormation: Deploying a VPC with subnets, NAT gateways, and route tables CDK (Python): Deploying the same infrastructure with significantly less code Discussion: Choosing Between IaC Tools CloudFormation: Native AWS support, declarative, visual designer CDK: Programming languages, reusable components, better for complex logic Terraform: Multi-cloud support, large community, extensive providers When to use each tool based on project requirements and team expertise Key Learnings:\nStart with CloudFormation for simple infrastructures Use CDK for complex, repeatable infrastructure patterns Always version control your IaC code Implement CI/CD for infrastructure deployments Test infrastructure changes in non-production environments first Lunch Break (12:00 – 1:00 PM) Self-arranged lunch break\nAfternoon Session (1:00 – 5:00 PM) 1:00 – 2:30 PM | Container Services on AWS Comprehensive coverage of containerization and orchestration:\nDocker Fundamentals Microservices Architecture: Breaking monoliths into smaller, independent services Containerization Benefits: Consistent environments from development to production Resource efficiency compared to VMs Rapid deployment and scaling Isolation and security boundaries Amazon ECR (Elastic Container Registry) Image Storage: Private Docker container registry Image Scanning: Automated vulnerability scanning on push Lifecycle Policies: Automated cleanup of old images to reduce costs Cross-Region Replication: Disaster recovery and global deployments Integration with ECS and EKS: Seamless image pulling Amazon ECS (Elastic Container Service) Task Definitions: Container specifications (CPU, memory, networking) Services: Maintaining desired count of running tasks Launch Types: EC2: Running containers on managed EC2 instances Fargate: Serverless container execution Service Discovery: DNS-based service discovery with Cloud Map Load Balancing: Integration with ALB/NLB for traffic distribution Amazon EKS (Elastic Kubernetes Service) Kubernetes on AWS: Managed Kubernetes control plane Worker Nodes: EC2 or Fargate for pod execution kubectl: Command-line tool for cluster management Helm Charts: Package manager for Kubernetes applications Add-ons: CoreDNS, kube-proxy, VPC CNI plugin IRSA (IAM Roles for Service Accounts): Fine-grained IAM permissions for pods AWS App Runner Simplified Deployment: Deploy containers from source code or images Automatic Scaling: Based on traffic patterns Built-in Load Balancing: No configuration required Use Cases: Simple web applications and APIs without infrastructure management Live Demo \u0026amp; Case Study: Microservices Deployment Comparison The instructor demonstrated deploying the same microservices application on:\nECS Fargate: Quick setup, serverless, pay-per-use EKS: More control, Kubernetes ecosystem, complex workloads App Runner: Fastest deployment, limited customization Comparison Discussion:\nECS: Best for AWS-native deployments, simpler than Kubernetes EKS: Best for Kubernetes expertise, multi-cloud strategies, complex orchestration App Runner: Best for simple applications, minimal infrastructure management Key Learnings:\nChoose ECS for AWS-centric microservices without Kubernetes complexity Choose EKS when you need Kubernetes features or multi-cloud portability Use Fargate to eliminate infrastructure management Implement health checks and proper logging for all containers Use ECR lifecycle policies to manage storage costs 2:30 – 2:45 PM | Coffee Break 2:45 – 4:00 PM | Monitoring \u0026amp; Observability Critical session on observability and operational excellence:\nAmazon CloudWatch Metrics:\nBuilt-in metrics for AWS services (CPU, network, disk) Custom metrics from applications High-resolution metrics (1-second granularity) Metric math for complex calculations Logs:\nCloudWatch Logs: Centralized log aggregation Log Groups and Streams: Organization structure Insights: SQL-like queries for log analysis Log Retention: Cost optimization through retention policies Subscription Filters: Real-time log processing with Lambda Alarms:\nThreshold-based alarms on metrics Composite alarms combining multiple conditions Actions: SNS notifications, Auto Scaling, EC2 actions Dashboards:\nCustom visualization of metrics and logs Cross-region dashboards for global visibility Sharing dashboards with stakeholders AWS X-Ray Distributed Tracing: Following requests through microservices Service Map: Visual representation of application architecture Trace Analysis: Identifying performance bottlenecks Annotations and Metadata: Adding context to traces Integration: Works with Lambda, ECS, EKS, API Gateway, and more Sampling Rules: Controlling trace data volume and costs Live Demo: Full-Stack Observability Setup Complete implementation of observability for a microservices application:\nApplication Instrumentation: Adding X-Ray SDK to code Log Aggregation: Shipping container logs to CloudWatch Custom Metrics: Publishing business metrics Dashboard Creation: Building operational dashboards Alarm Configuration: Setting up critical alerts Trace Analysis: Using X-Ray to debug latency issues Best Practices Alerting Strategy:\nAlert on symptoms, not causes Reduce alert fatigue with proper thresholds Use composite alarms to reduce false positives Implement escalation policies Dashboard Design:\nSeparate operational and business dashboards Include SLIs (Service Level Indicators) and error rates Use consistent time ranges across widgets Display dashboards on team monitors On-Call Processes:\nClear runbooks for common alerts Incident response playbooks Postmortem processes for learning Rotation schedules to prevent burnout Key Learnings:\nImplement the three pillars of observability: metrics, logs, and traces Use CloudWatch for infrastructure and application monitoring Enable X-Ray for microservices to understand request flows Set meaningful alerts that require action Build dashboards that tell the story of system health 4:00 – 4:45 PM | DevOps Best Practices \u0026amp; Case Studies Practical implementation strategies and real-world examples:\nDeployment Strategies Feature Flags:\nDecoupling deployment from release Gradual feature rollout to user segments A/B testing and experimentation Emergency kill switches AWS AppConfig for feature flag management A/B Testing:\nTesting variations to optimize user experience Statistical significance in experiments Integration with CloudWatch for metrics Automated Testing Testing Pyramid: Unit tests (fast, isolated) Integration tests (component interactions) End-to-end tests (full user workflows) CI/CD Integration: Fail fast with early testing Parallel test execution Test coverage metrics Security scanning in pipeline Incident Management Incident Response Process:\nDetection (monitoring and alerting) Triage (severity assessment) Investigation (root cause analysis) Resolution (fix and verify) Communication (stakeholder updates) Postmortem (learning and prevention) Postmortem Best Practices:\nBlameless culture Timeline of events Root cause analysis (5 Whys) Action items with owners Sharing learnings across teams Case Studies Case Study 1: Startup DevOps Transformation\nChallenge: Manual deployments, frequent production issues, slow feature delivery Solution: Implemented CI/CD with CodePipeline Containerized applications with ECS Fargate Infrastructure as Code with CDK Comprehensive monitoring with CloudWatch Results: Deployment frequency increased from weekly to multiple times per day MTTR reduced from hours to minutes Developer productivity improved significantly Zero-downtime deployments achieved Case Study 2: Enterprise Multi-Account DevOps\nChallenge: Complex multi-account structure, compliance requirements, coordinated releases Solution: AWS Organizations with SCPs Cross-account CI/CD pipelines Centralized logging and monitoring Automated compliance checks in pipeline Results: Consistent deployments across 50+ AWS accounts Reduced compliance audit time by 70% Improved security posture with automated scanning Key Learnings:\nStart small and iterate on DevOps practices Automate everything that can be automated Measure everything and use data to drive improvements Foster a culture of continuous learning and improvement Invest in developer experience to improve productivity 4:45 – 5:00 PM | Q\u0026amp;A \u0026amp; Wrap-up Final session for questions and career guidance:\nDevOps Career Pathways Entry Level: Focus on scripting, CI/CD tools, and cloud fundamentals Mid Level: Infrastructure as Code, container orchestration, observability Senior Level: Architecture design, multi-region deployments, platform engineering Specializations: SRE, Platform Engineering, Cloud Architecture, Security Engineering AWS Certification Roadmap for DevOps Recommended certification path:\nAWS Certified Cloud Practitioner: Foundation knowledge (optional for experienced developers) AWS Certified Developer – Associate: Application development on AWS AWS Certified DevOps Engineer – Professional: Comprehensive DevOps practices AWS Certified Solutions Architect – Professional: Advanced architecture skills (recommended for senior roles) Learning Resources AWS Skill Builder for hands-on labs AWS Workshops for guided projects AWS re:Post for community Q\u0026amp;A AWS Whitepapers for best practices Local AWS User Groups and events Personal Reflections \u0026amp; Value Gained This full-day DevOps workshop was incredibly comprehensive and valuable for my professional development:\nComplete DevOps Lifecycle Understanding: Gained end-to-end knowledge from source control to production monitoring, understanding how all pieces fit together\nHands-On Tool Proficiency: Live demonstrations of CodePipeline, CloudFormation, CDK, ECS, and CloudWatch provided practical skills I can immediately apply\nReal-World Context: Case studies from startups and enterprises showed how DevOps practices scale and adapt to different organizational needs\nInfrastructure as Code Mastery: Deep dive into CloudFormation and CDK clarified when to use each tool and how to structure infrastructure code\nContainer Orchestration Clarity: Understanding the differences between ECS, EKS, and App Runner helps me choose the right tool for different use cases\nObservability Best Practices: Learning to implement comprehensive monitoring with metrics, logs, and traces is critical for production operations\nDevOps Culture: Understanding that DevOps is about culture and collaboration, not just tools, changes how I approach software delivery\nCareer Development: Clear roadmap for DevOps certifications and career progression provides direction for my learning journey\nApplication to Current Work The knowledge gained directly applies to my internship and projects:\nWorkshop Development: Can now add CI/CD deployment instructions to my S3 VPC endpoint workshop Infrastructure as Code: Implementing CloudFormation/CDK templates for repeatable deployments Containerization: Understanding when to recommend ECS vs. EKS for different scenarios Monitoring: Setting up proper CloudWatch dashboards and alarms for production workloads Documentation: Creating better runbooks and deployment guides based on DevOps best practices Key Takeaways Automation is Key: Automate repetitive tasks to reduce errors and increase velocity Measure Everything: Use metrics to drive continuous improvement Start Small: Begin with simple CI/CD pipelines and gradually add sophistication Infrastructure as Code: Version control all infrastructure for reproducibility Observability is Critical: Can\u0026rsquo;t improve what you can\u0026rsquo;t measure Culture Matters: DevOps transformation requires organizational buy-in Security from the Start: Integrate security scanning into CI/CD pipelines Learn from Failures: Blameless postmortems turn incidents into learning opportunities Conclusion The DevOps on AWS workshop was an exceptional full-day learning experience that provided both breadth and depth across the entire DevOps landscape. From CI/CD pipelines to container orchestration to observability, every session built upon previous knowledge while providing practical, hands-on examples.\nThe combination of theoretical principles, live demonstrations, and real-world case studies made complex concepts accessible and immediately applicable. The workshop reinforced that successful DevOps is about culture, automation, measurement, and continuous improvement.\nI highly recommend this workshop to anyone looking to implement DevOps practices on AWS or advance their career in cloud operations and site reliability engineering. The skills learned here are foundational to modern software delivery and will be invaluable throughout my career in cloud computing.\nMoving forward, I plan to pursue the AWS Certified DevOps Engineer - Professional certification and continue implementing these practices in my projects, contributing to faster, more reliable software delivery.\n"},{"uri":"https://rsshive.github.io/fcj-workshop/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" In this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in five significant events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with valuable networking opportunities and wonderful moments.\nEvent 1 Event Name: AI-Driven Development Life Cycle: Reimagining Software Engineering\nDate \u0026amp; Time: 14:00 - 16:30, October 3, 2025\nLocation: AWS Vietnam Office, 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: GenAI-powered App-DB Modernization Workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AWS Well-Architected Security Pillar Workshop\nDate \u0026amp; Time: 08:30 - 12:00, November 29, 2025\nLocation: AWS Vietnam Office, Bitexco Financial Tower, 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nEvent 4 Event Name: DevOps on AWS - Full-Day Workshop\nDate \u0026amp; Time: 08:30 - 17:00, November 17, 2025\nLocation: AWS Vietnam Office, Bitexco Financial Tower, 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nEvent 5 Event Name: AI/ML/GenAI on AWS Workshop\nDate \u0026amp; Time: 08:30 - 12:00, November 15, 2025\nLocation: AWS Vietnam Office, Bitexco Financial Tower, 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://rsshive.github.io/fcj-workshop/5-workshop/5.4-next.js-setup/","title":"Next.js Project Setup &amp; Amplify Configuration","tags":[],"description":"","content":"Next.js Project Setup \u0026amp; Amplify Configuration In this section, you will set up a Next.js application, install the required Amplify SDK packages, and configure Amplify to connect your web application to the Cognito User Pool created earlier.\nYou will prepare the base environment that will be used later to build authentication features such as sign-up, sign-in, sign-out, and protected routes.\nContent Install Next.js Project Install Amplify SDK and Configure Navigation:\nPrevious: 5.3 AWS Cognito Setup Next Step: 5.5 Authentication Functions → Build auth functions, UI, and protected routes "},{"uri":"https://rsshive.github.io/fcj-workshop/5-workshop/5.5-cognito-function/","title":"Authentication Functions, UI, and Protected Routes","tags":[],"description":"","content":"Authentication Functions, UI, and Protected Routes In this section, you will integrate AWS Cognito into your Next.js application using the Amplify SDK.\nYou will implement core authentication logic, build a user-friendly UI, and secure specific pages using protected routes.\nBy the end of this section, your application will support:\n✔ Features You Will Implement Sign Up Function\nCreate new users in Cognito (with email verification).\nSign In Function\nAuthenticate users and receive Cognito-issued tokens.\nSign Out Function\nProperly terminate the authenticated session through Amplify.\nSession Handling\nCheck whether users are logged in and retrieve current session info.\nProtected Routes\nRestrict access to certain pages unless the user is authenticated.\nUI Components\nBuild simple authentication pages such as Login, Register, Profile, and Dashboard using your chosen UI library.\nQuick options\nYou can either use the live demo for a fast setup or follow the step‑by‑step guides below to edit functions and UI directly.\nYou can either use the live demo for a fast setup or follow the step‑by‑step guides below:\nCreate Cognito Authentication Functions Build Authentication UI Implement Auth Context \u0026amp; Protected Routes Use Project Demo (Quick Start) Quick Start Demo: https://github.com/Thormastran/my-cognito-project/\nNavigation:\nPrevious: 5.4 Next.js Project Setup Next Step: 5.6 Full Testing \u0026amp; Verification → Test all authentication features follow the guides for detailed customization and explanation. Content Create Cognito Authentication Functions Build Authentication UI Implement Auth Context \u0026amp; Protected Routes 4.Full Testing \u0026amp; Verification "},{"uri":"https://rsshive.github.io/fcj-workshop/5-workshop/","title":"Workshop","tags":[],"description":"","content":" Warning: The content below is for training and reference purposes only. Please do not copy verbatim into official reports or public documentation.\nAWS Cognito + Amplify SDK Workshop Overview Amazon Cognito is a fully managed user authentication, authorization, and user management service that makes it easy to add sign-up, sign-in, and access control to web and mobile apps.\nIn this hands-on workshop, you will:\nCreate and configure a Cognito User Pool from scratch Integrate the latest AWS Amplify Gen 2 (v6+) with Next.js 14 (App Router) Implement a complete authentication flow:\nSign-up → Email verification → Sign-in → Protected routes → Role-based access Follow production-ready best practices (SSR support, secure token handling, clean architecture) Final outcome: A fully functional, secure Next.js application with user authentication — ready to deploy on Vercel, Netlify, or AWS Amplify Hosting.\nWorkshop Contents Workshop Overview\nWhat are Amazon Cognito and AWS Amplify? Why use them together? Prerequisites\nAWS account, IAM user, AWS CLI Node.js 18+, npm, VS Code AWS Cognito Setup\nCreate User Pool with email sign-in Configure password policy, optional MFA, App Client (no client secret) Create Cognito Groups (admin / user) Next.js Project Setup \u0026amp; Amplify Configuration\nScaffold Next.js 14 with App Router + TypeScript + Tailwind CSS Install aws-amplify@latest Configure environment variables and Amplify SSR mode Authentication Functions, UI, and Protected Routes\nComplete Cognito helpers: signUp, confirmSignUp, signIn, signOut, getCurrentUser, etc. Global AuthContext + custom useAuth hook Sign-up, email verification, sign-in, and forgot password pages ProtectedRoute component Dashboard page displaying user info and role Full Testing \u0026amp; Verification\nEnd-to-end test checklist Common errors and troubleshooting tips Clean Up Resources\nDelete User Pool and App Client to avoid unwanted charges Estimated duration: 3–4 hours\nDifficulty: Intermediate (basic React/Next.js knowledge recommended)\nLet’s get started! → Click on 5.1 Workshop Overview to begin.\n"},{"uri":"https://rsshive.github.io/fcj-workshop/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AI/ML/GenAI on AWS Workshop\u0026rdquo; Event Information Event: AI/ML/GenAI on AWS Workshop Date: Saturday, November 15th, 2025 Time: 8:30 AM - 12:00 PM (Morning Session) Location: AWS Vietnam Office, Bitexco Financial Tower, 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City My Role: Attendee Event Overview This half-day workshop provided a comprehensive introduction to Artificial Intelligence, Machine Learning, and Generative AI services available on AWS. The session was specifically designed for the Vietnamese market, covering both traditional ML workflows with Amazon SageMaker and cutting-edge Generative AI capabilities with Amazon Bedrock. Through live demonstrations, hands-on guidance, and practical examples, participants gained insights into building and deploying AI/ML solutions on AWS.\nDetailed Session Breakdown 8:30 – 9:00 AM | Welcome \u0026amp; Introduction The workshop began with a welcoming atmosphere focused on building community connections:\nParticipant Registration and Networking:\nMeet and greet with fellow AI/ML enthusiasts Exchange of contact information and professional backgrounds Diverse attendee mix: developers, data scientists, solution architects, and business leaders Workshop Overview and Learning Objectives:\nUnderstanding the AWS AI/ML service portfolio Learning when to use SageMaker vs. Bedrock Gaining practical skills in building AI applications Exploring real-world use cases and success stories Ice-Breaker Activity:\nParticipants shared their AI/ML experience levels Discussion of current challenges in implementing AI solutions Expectations and specific use cases participants wanted to explore AI/ML Landscape in Vietnam:\nGrowing adoption of AI across industries (e-commerce, fintech, healthcare) Talent pool development and university partnerships Government initiatives supporting digital transformation Challenges: Data availability, infrastructure costs, skill gaps Success stories from Vietnamese companies using AWS AI/ML services Key Insights:\nVietnam is rapidly embracing AI/ML with strong government and private sector support Major barriers are being addressed through cloud platforms and education initiatives AWS provides accessible tools that democratize AI/ML for organizations of all sizes 9:00 – 10:30 AM | AWS AI/ML Services Overview This intensive session explored Amazon SageMaker as the comprehensive ML platform:\nAmazon SageMaker - End-to-End ML Platform A complete overview of SageMaker\u0026rsquo;s capabilities as a unified platform for the entire machine learning lifecycle:\nPlatform Philosophy: Removing heavy lifting from ML workflows Providing tools for data scientists, ML engineers, and developers Enabling collaboration across teams Integrating with the broader AWS ecosystem Data Preparation and Labeling SageMaker Data Wrangler:\nVisual interface for data preparation 300+ built-in data transformations Data quality and insights reports Export to feature store or training SageMaker Ground Truth:\nHuman labeling workforce (public, private, vendor) Automated data labeling using active learning Built-in labeling workflows (text, image, video) Quality control and consensus mechanisms Cost reduction through machine learning-assisted labeling SageMaker Feature Store:\nCentralized repository for ML features Online and offline feature serving Feature versioning and lineage tracking Reducing feature engineering duplication Model Training, Tuning, and Deployment Training Options:\nBuilt-in Algorithms: Pre-built algorithms for common tasks (XGBoost, Linear Learner, etc.) Custom Training: Bring your own training scripts (PyTorch, TensorFlow, scikit-learn) SageMaker Training Jobs: Managed training infrastructure Distributed Training: Multi-GPU and multi-node training Spot Instance Training: Cost savings up to 90% Hyperparameter Tuning:\nAutomatic model tuning with Bayesian optimization Parallel training jobs for faster experimentation Early stopping to save resources Warm start from previous tuning jobs Model Deployment:\nReal-time Endpoints: Low-latency predictions Batch Transform: Processing large datasets Serverless Inference: Auto-scaling without managing infrastructure Multi-Model Endpoints: Deploy multiple models to a single endpoint Model Monitoring: Drift detection and data quality monitoring Integrated MLOps Capabilities SageMaker Pipelines:\nCI/CD for machine learning Orchestrating ML workflows Versioning and lineage tracking Integration with CI/CD tools SageMaker Model Registry:\nCentralized model catalog Model versioning and approval workflows Integration with deployment pipelines Model cards for documentation SageMaker Experiments:\nTracking training runs and parameters Comparing model performance Organizing experiments into trials Visualizing metrics and artifacts SageMaker Clarify:\nDetecting bias in ML models Explainability and interpretability Feature importance analysis Model transparency for compliance Live Demo: SageMaker Studio Walkthrough The instructor provided a comprehensive demonstration of SageMaker Studio, the IDE for ML:\nData Exploration:\nLoading and visualizing a sample dataset (customer churn prediction) Using Data Wrangler for feature engineering Creating data quality reports Model Training:\nWriting a training script with TensorFlow Configuring training job parameters Launching distributed training Monitoring training progress with CloudWatch metrics Hyperparameter Tuning:\nSetting up tuning job with multiple hyperparameters Reviewing tuning results and best model selection Model Deployment:\nDeploying model to a real-time endpoint Testing predictions with sample data Setting up CloudWatch alarms for monitoring MLOps Pipeline:\nCreating a SageMaker Pipeline Automating data preprocessing, training, and deployment Version control and model registry integration Key Learnings from Demo:\nSageMaker Studio provides a unified environment for all ML tasks Built-in integrations significantly reduce boilerplate code Managed infrastructure allows focusing on model development MLOps capabilities enable production-grade ML systems 10:30 – 10:45 AM | Coffee Break Networking opportunity with fellow participants and AWS team members\n10:45 AM – 12:00 PM | Generative AI with Amazon Bedrock This exciting session focused on the latest Generative AI capabilities:\nFoundation Models: Claude, Llama, Titan Comprehensive overview of available foundation models on Bedrock:\nModel Comparison:\nModel Provider Strengths Use Cases Claude (2 \u0026amp; 3) Anthropic Long context, safety, reasoning Document analysis, coding assistance, complex conversations Llama 2 \u0026amp; 3 Meta Open source, multilingual General purpose, cost-sensitive applications Amazon Titan AWS Cost-effective, integration Text generation, summarization, embeddings Jurassic-2 AI21 Labs Language understanding Content generation, text improvement Stable Diffusion Stability AI Image generation Creative assets, product visualization Selection Guide:\nTask Complexity: More complex reasoning → Claude Context Length: Long documents → Claude 3 (200K tokens) Cost Optimization: High volume → Titan or Llama Multilingual: Global applications → Llama or Claude Image Generation: Creative needs → Stable Diffusion Prompt Engineering: Techniques and Best Practices Deep dive into crafting effective prompts:\nFundamental Principles:\nBe specific and clear in instructions Provide context and background Define expected output format Use examples when needed Iterate and refine prompts Chain-of-Thought (CoT) Reasoning:\nEncouraging step-by-step thinking Improving accuracy on complex problems Example: \u0026ldquo;Let\u0026rsquo;s solve this step by step:\u0026rdquo; Breaking down multi-step reasoning tasks Few-Shot Learning:\nProviding examples in the prompt Demonstrating desired behavior Pattern recognition from examples Balancing examples vs. token usage Advanced Techniques:\nRole Playing: \u0026ldquo;You are an expert financial analyst\u0026hellip;\u0026rdquo; System Prompts: Setting consistent behavior Temperature Control: Creativity vs. consistency Token Management: Optimizing for cost and latency Retrieval-Augmented Generation (RAG) Comprehensive architecture for grounding LLMs with enterprise data:\nRAG Architecture Components:\nDocument Ingestion: Loading and chunking documents Embedding Generation: Converting text to vectors Vector Storage: Storing embeddings (OpenSearch, pgvector, FAISS) Semantic Search: Finding relevant context Prompt Augmentation: Injecting context into prompts LLM Generation: Producing grounded responses Amazon Bedrock Knowledge Bases:\nFully managed RAG solution Automatic document ingestion from S3 Built-in embedding generation (Titan Embeddings) Integration with vector databases Source attribution in responses Support for various document formats (PDF, Word, HTML) Benefits of RAG:\nReducing hallucinations with factual grounding Incorporating proprietary/private data Keeping information up-to-date without retraining Providing source citations for transparency Bedrock Agents: Multi-Step Workflows Building autonomous agents that can take actions:\nAgent Capabilities:\nNatural language understanding of user intent Breaking down complex tasks into steps Calling APIs and Lambda functions Maintaining conversation context Handling multi-turn interactions Tool Integration:\nFunction calling for external systems Database queries API invocations Custom business logic execution Use Cases:\nCustomer service automation IT helpdesk agents Data analysis assistants Booking and scheduling systems Guardrails: Safety and Content Filtering Implementing responsible AI practices:\nAmazon Bedrock Guardrails:\nContent filtering (hate speech, violence, profanity) PII detection and redaction Topic restrictions (denied topics) Word filters (custom blocked terms) Response quality controls Safety Mechanisms:\nInput validation before LLM processing Output validation before returning to users Contextual grounding verification Toxicity scoring Compliance and Privacy:\nGDPR compliance through PII redaction Audit logging for all interactions Regional data residency options Live Demo: Building a Generative AI Chatbot End-to-end demonstration of creating a customer support chatbot:\nStep 1: Knowledge Base Setup\nUploaded company FAQ documents to S3 Created Bedrock Knowledge Base Configured embeddings with Titan Embeddings Tested semantic search queries Step 2: Agent Configuration\nDefined agent instructions and personality Connected Knowledge Base for RAG Added Lambda function for order lookup Configured conversation flow Step 3: Guardrails Implementation\nSet up content filters Enabled PII redaction for customer data Configured topic restrictions Tested with various inputs Step 4: Testing and Iteration\nDemo conversations showing: FAQ answering from Knowledge Base Order status lookup via Lambda Multi-turn conversations Guardrails blocking inappropriate content Source attribution for answers Step 5: Deployment Considerations\nIntegration options (API, SDK, web UI) Monitoring with CloudWatch Cost optimization strategies Scaling considerations Key Learnings from Demo:\nBedrock significantly simplifies GenAI application development RAG is essential for enterprise applications requiring current, accurate information Guardrails are critical for production deployments Agents enable complex, multi-step workflows without extensive code Source attribution builds trust with end users 12:00 PM | Workshop Conclusion \u0026amp; Lunch The workshop concluded with:\nQ\u0026amp;A Session: Participants asked questions about specific use cases and implementation challenges Resource Sharing: Links to AWS documentation, workshops, and learning resources Next Steps: Recommendations for hands-on practice and certification paths Networking: Continued discussions over self-arranged lunch Personal Reflections \u0026amp; Value Gained This workshop was incredibly valuable for understanding the AI/ML landscape on AWS:\nComprehensive AI/ML Understanding: Gained clarity on the full spectrum from traditional ML (SageMaker) to Generative AI (Bedrock)\nPractical Implementation Skills: Live demos provided hands-on understanding of building real-world AI applications\nBedrock Knowledge: Deep dive into foundation models, prompt engineering, and RAG architectures filled critical knowledge gaps\nMLOps Awareness: Understanding SageMaker\u0026rsquo;s MLOps capabilities highlighted the importance of production-grade ML systems\nVietnam Context: Learning about AI/ML adoption in Vietnam provided local market insights valuable for career planning\nResponsible AI: Emphasis on guardrails and safety mechanisms reinforced the importance of ethical AI implementation\nDecision Framework: Clear guidance on choosing between different models and services helps with architectural decisions\nNetworking Opportunity: Connected with data scientists, ML engineers, and AWS experts, expanding professional network\nApplication to Current Work and Future Projects The knowledge gained has immediate and future applications:\nImmediate Applications:\nCan now recommend appropriate AWS AI/ML services for different use cases Understanding RAG architecture enables better documentation and knowledge management solutions Prompt engineering techniques improve interactions with AI tools Future Project Ideas:\nBuilding a documentation chatbot using Bedrock Knowledge Bases for the FCJ community Implementing intelligent search using embeddings and semantic similarity Creating ML pipelines with SageMaker for predictive analytics projects Developing content generation tools for technical writing Career Development:\nAWS Certified Machine Learning - Specialty certification path Hands-on projects with SageMaker and Bedrock Contributing to AI/ML community in Vietnam Exploring AI/ML engineering roles Key Takeaways SageMaker for Traditional ML: Complete platform for end-to-end ML workflows from data prep to deployment\nBedrock for GenAI: Fastest path to building GenAI applications with foundation models and managed services\nRAG is Essential: Retrieval-Augmented Generation is critical for enterprise GenAI applications requiring accuracy and current information\nPrompt Engineering Matters: Quality of prompts directly impacts quality of outputs; invest time in prompt development\nMLOps is Necessary: Production ML requires pipelines, monitoring, and governance - not just model training\nGuardrails are Non-Negotiable: Safety and content filtering must be implemented before production deployment\nChoose the Right Tool: SageMaker for custom ML models, Bedrock for leveraging pre-trained foundation models\nCost Optimization: Use spot instances, serverless inference, and efficient prompting to control costs\nIterative Approach: Start small, test thoroughly, and iterate based on real-world feedback\nCommunity and Learning: Active learning community in Vietnam; opportunities for collaboration and knowledge sharing\nIndustry Applications Discussed The workshop highlighted several real-world use cases relevant to Vietnamese enterprises:\nE-commerce:\nProduct recommendation systems (SageMaker) Chatbots for customer support (Bedrock Agents) Automated product descriptions (Bedrock) Demand forecasting (SageMaker) Financial Services:\nFraud detection (SageMaker) Document processing and analysis (Bedrock with RAG) Risk assessment models (SageMaker) Financial advisory chatbots (Bedrock Agents) Healthcare:\nMedical image analysis (SageMaker) Clinical documentation assistance (Bedrock) Patient triage systems (Bedrock Agents) Predictive analytics for patient outcomes (SageMaker) Education:\nPersonalized learning assistants (Bedrock) Automated grading and feedback (SageMaker + Bedrock) Content generation for educational materials (Bedrock) Resources and Next Steps Recommended Learning Path:\nAWS Skill Builder: Machine Learning Learning Plan SageMaker Immersion Day workshops Bedrock Workshop (hands-on labs) AWS Certified Machine Learning - Specialty exam preparation Hands-on Practice:\nAWS Free Tier: Experiment with SageMaker notebooks Bedrock Playground: Test foundation models Sample datasets: Kaggle, AWS Open Data Community projects: Contribute to open-source AI/ML projects Community Engagement:\nAWS User Group Vietnam AI/ML Meetups in Ho Chi Minh City AWS re:Post for Q\u0026amp;A LinkedIn groups for Vietnamese AI/ML professionals Conclusion The AI/ML/GenAI on AWS workshop was an excellent introduction to the rapidly evolving field of artificial intelligence on cloud platforms. The balanced approach covering both traditional machine learning with SageMaker and cutting-edge generative AI with Bedrock provided comprehensive understanding of the AWS AI/ML portfolio.\nThe live demonstrations were particularly valuable, showing practical implementations rather than just theoretical concepts. Seeing RAG in action and building a functioning chatbot demystified Generative AI and made it feel accessible for real-world projects.\nThe workshop reinforced that AI/ML is no longer the domain of only large enterprises with massive resources. AWS services democratize AI, making it accessible to startups and individual developers. The key is starting with clear use cases, leveraging managed services, and focusing on delivering value rather than building infrastructure.\nMost importantly, the emphasis on responsible AI through guardrails and the discussion of MLOps practices showed that production AI is about much more than just training models - it requires thoughtful architecture, monitoring, and governance.\nI highly recommend this workshop to anyone interested in AI/ML on AWS, whether you\u0026rsquo;re a developer looking to add AI capabilities to applications, a data scientist exploring cloud ML platforms, or a business leader evaluating AI opportunities. The knowledge gained provides a solid foundation for building intelligent applications on AWS.\nMoving forward, I plan to pursue hands-on projects with both SageMaker and Bedrock, work toward the AWS Machine Learning Specialty certification, and explore opportunities to apply AI/ML in my internship projects and future career.\n"},{"uri":"https://rsshive.github.io/fcj-workshop/5-workshop/5.6-testing/","title":"Full Testing &amp; Verification","tags":[],"description":"","content":"5.6 Full Testing \u0026amp; Verification Your app is ready — let’s prove it works perfectly\n1. Start the Application npm run dev Open → http://localhost:3000\n2 Full Testing \u0026amp; Verification – Checklist (Tick as you go!) Done # Action Expected Result ☐ 1 Open homepage (/) Welcome card with Login and Sign Up buttons ☐ 2 Click Sign Up → go to /signup Clean sign-up form appears ☐ 3 Register with new email + strong password Success → switches to \u0026ldquo;Verify Email\u0026rdquo; screen ☐ 4 Check your email (including Spam/Promotions) Receive 6-digit verification code from Amazon Cognito ☐ 5 Enter the code Success → automatically redirected to Sign In page ☐ 6 Sign in with the same credentials Success → redirected to Dashboard ☐ 7 Dashboard loads Shows your name, email, role = user, and beautiful layout ☐ 8 Click Sign Out Logged out → back to homepage ☐ 9 Open new tab → go directly to /dashboard Immediately redirected to /signin → ProtectedRoute works perfectly ☐ 10 Try signing up with the same email again Error: “Account already exists.” ☐ 11 Sign in with wrong password Error: “Invalid email or password” ☐ 12 Create new user → don\u0026rsquo;t verify → try login Error: \u0026ldquo;Please verify your email first\u0026rdquo; 3. Change Password Testing Done # Action Expected Result ☐ 13 Sign in with verified account Successfully logged in to Dashboard ☐ 14 Navigate to Change Password section Form with Current Password, New Password, Confirm New Password fields ☐ 15 Submit with wrong current password Error: \u0026ldquo;Current password is incorrect\u0026rdquo; ☐ 16 Submit with weak new password Error: \u0026ldquo;Password does not meet requirements\u0026rdquo; ☐ 17 Submit with mismatched confirm password Error: \u0026ldquo;Passwords do not match\u0026rdquo; ☐ 18 Submit with correct current + strong new password Success: \u0026ldquo;Password changed successfully\u0026rdquo; ☐ 19 Sign out and sign in with old password Error: \u0026ldquo;Invalid email or password\u0026rdquo; ☐ 20 Sign in with new password Success → Dashboard loads correctly 4. Forgot Password Testing Done # Action Expected Result ☐ 21 Go to Sign In page → Click Forgot Password? Redirected to Forgot Password page ☐ 22 Enter registered email address Success: \u0026ldquo;Reset code sent to your email\u0026rdquo; ☐ 23 Check email for password reset code Receive 6-digit reset code from Amazon Cognito ☐ 24 Enter reset code + new strong password Success: \u0026ldquo;Password reset successfully\u0026rdquo; → redirected to Sign In ☐ 25 Sign in with new password Success → Dashboard loads correctly All 25 checks passed? → You\u0026rsquo;ve just built a 100% working, production-grade authentication system with complete password management!\nwhen you access dashboard like this picture . You\u0026rsquo;re officially done — go deploy it and show the world!\nNavigation:\nPrevious: 5.5 Authentication Functions Next Step: 5.7 Clean Up Resources → Remove AWS resources to avoid charges Next → 5.7 Clean Up Resources (optional but recommended)\n"},{"uri":"https://rsshive.github.io/fcj-workshop/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at AWS - First Cloud Journey from September 9, 2025 to December 9, 2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world cloud computing environment.\nI participated in comprehensive AWS cloud services training and hands-on workshops, focusing on core AWS services including EC2, S3, VPC, Lambda, and various networking and security solutions. Through this experience, I improved my skills in cloud architecture design, infrastructure automation, networking configuration, security best practices, technical documentation, and problem-solving in cloud environments.\nThroughout the program, I engaged in multiple practical labs and real-world scenarios, translating technical AWS blogs, participating in AWS community events, and developing a complete workshop demonstrating S3 access patterns through VPC Gateway Endpoints and Interface Endpoints. This hands-on approach significantly enhanced my understanding of cloud technologies and AWS best practices.\nIn terms of work ethic, I always strived to complete tasks well, maintained consistent progress through weekly worklogs, and actively engaged with the AWS community and fellow participants to improve my learning efficiency and contribute knowledge back to others.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ✅ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ✅ ☐ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"https://rsshive.github.io/fcj-workshop/5-workshop/5.7-cleanup/","title":"Clean Up Resources","tags":[],"description":"","content":"5.7 Clean Up Resources Delete everything to avoid any charges (even tiny ones)\nYou’ve successfully completed the workshop — congratulations!\nNow let’s make sure your AWS account stays $0.00.\nResources You Created (and Must Delete) Resource Location Cost if Left Running Must Delete? Cognito User Pool AWS Console → Amazon Cognito Free tier: 50,000 MAU → still $0 Recommended App Client (inside pool) Inside the User Pool No extra cost Auto-deleted Cognito Groups (admin/user) Inside the User Pool No cost Auto-deleted Test users (you signed up) Inside the User Pool → Users No cost Optional Important: Amazon Cognito User Pools are free for up to 50,000 monthly active users.\nYou will never be charged for this workshop — but it’s still best practice to clean up.\nStep-by-Step Cleanup (Takes 60 Seconds) Go to the AWS Console → Amazon Cognito\n→ ⁦https://console.aws.amazon.com/cognito/home⁩\nSelect your region (the one you used, e.g., us-east-1)\nYou’ll see your User Pool (probably named something like my-cognito-app-pool or the default name)\nClick on your User Pool → Delete user pool (top-right button)\nType delete in the confirmation box Click Delete user pool\nDone! Everything is permanently deleted.\nOptional: Also Delete Test Users (if you want a completely clean slate) • Inside the User Pool → Users tab → select any test accounts → Delete\nYou’re All Set!\nYour AWS account is now clean and back to $0.00.\nFinal Summary – What You’ve Accomplished You have built a 2025-standard, production-ready authentication system using:\n• Amazon Cognito (fully managed identity) • AWS Amplify Gen 2 (v6+) – the correct, modern, SSR-safe way • Next.js 14 App Router + React Server Components • Role-based access control via Cognito Groups • Protected routes \u0026amp; global auth context • Beautiful, accessible UI with shadcn/ui + Tailwind\nThis is exactly how professional teams at startups, enterprises, and AWS Partners build authentication today.\nYou now have a complete, deployable, real-world project you can: • Add to your portfolio • Use as a starter template • Deploy instantly to Vercel/Netlify • Show in job interviews\nYou absolutely crushed this workshop.\nNow go deploy it, share it, and be proud — you\u0026rsquo;ve earned it!\nNavigation:\nPrevious: 5.6 Full Testing \u0026amp; Verification Workshop Complete! 🎉 Return to Workshop Overview or Main Index End of Workshop\n"},{"uri":"https://rsshive.github.io/fcj-workshop/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment at First Cloud Journey is excellent overall. Everything is well-organized, and the working conditions are very comfortable, providing an ideal space for learning and productivity. The office facilities, including computers, internet connectivity, and workstations, are modern and reliable, which greatly supports the work efficiency.\nHowever, one notable limitation is the lunch break arrangement. The facility has limited options for lunch breaks, requiring interns to go outside for meals. While this isn\u0026rsquo;t a major issue, having an on-site cafeteria or designated lunch area would add more convenience, especially during busy days or when working on tight deadlines. Despite this, the overall atmosphere remains professional and conducive to focused work.\n2. Support from Mentor / Team Admin\nThe mentor and admin team have been incredibly supportive throughout my internship. The mentor provides enthusiastic, detailed, and friendly guidance on every task and challenge I encounter. They take time to explain complex AWS concepts thoroughly, share practical tips from real-world experience, and always encourage me to ask questions without hesitation.\nWhat I particularly appreciate is the mentor\u0026rsquo;s approach of guiding rather than simply giving answers—they help me develop problem-solving skills by encouraging me to research and think critically first, then step in with clarification when needed. The admin team is equally responsive, handling administrative matters efficiently and ensuring all necessary resources and documentation are readily available. Their combined support has made my learning journey smooth and enriching.\n3. Relevance of Work to Academic Major\nThe work assigned during this internship is highly relevant to my academic major in computer science and information technology. The program allowed me to apply theoretical knowledge from university courses—such as networking, system administration, and programming—into real-world cloud computing scenarios.\nMoreover, the internship introduced me to cutting-edge technologies and industry-standard practices that aren\u0026rsquo;t typically covered in depth at university, including AWS services like EC2, S3, VPC, Lambda, and Infrastructure as Code concepts. This combination of reinforcing foundational knowledge while gaining practical, industry-relevant skills has significantly enhanced my technical competency and prepared me better for a career in cloud computing and DevOps.\n4. Learning \u0026amp; Skill Development Opportunities\nThe learning opportunities provided during this internship have been exceptional. Beyond the structured training on AWS cloud services, I gained hands-on experience through practical workshops and real-world scenarios that challenged me to think critically and solve problems independently.\nOne of the highlights was participating in AWS community events where I had the chance to meet and learn from industry veterans and senior professionals. These events featured insightful talks, experience-sharing sessions, and networking opportunities that exposed me to different career paths and best practices in the cloud computing field. The speakers shared valuable lessons from their professional journeys, offered career advice, and discussed emerging trends in technology.\nAdditionally, translating technical AWS blogs helped improve both my technical understanding and communication skills, while developing a complete workshop enhanced my ability to structure and present technical content effectively. These diverse learning experiences have equipped me with both hard technical skills and soft skills like presentation, documentation, and professional communication.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture at First Cloud Journey is remarkably positive and supportive. There is a strong emphasis on collaboration, knowledge sharing, and continuous learning. Everyone treats each other with respect regardless of role or seniority level, creating an inclusive environment where even interns feel valued and heard.\nThe team spirit is evident in how members actively help each other, celebrate achievements together, and maintain open communication channels. The culture encourages asking questions, experimenting with new ideas, and learning from mistakes without fear of judgment. This growth-oriented mindset aligns perfectly with the AWS community\u0026rsquo;s culture of innovation and customer obsession.\nWhat impressed me most is how the organization balances professionalism with a friendly atmosphere—work is taken seriously, but there\u0026rsquo;s also room for casual conversations, knowledge sharing, and mutual encouragement. This positive culture made me feel like a genuine part of the team rather than just a temporary intern.\n6. Internship Policies / Benefits\nThe internship policies and benefits provided by First Cloud Journey are fair and well-structured. The program offers a clear roadmap with defined milestones and expectations, which helps interns stay focused and track their progress effectively through weekly worklog submissions.\nThe flexibility in learning pace and the availability of resources—including access to AWS documentation, training materials, and community support—demonstrate the organization\u0026rsquo;s commitment to intern development. The opportunity to participate in external AWS events and community activities adds significant value beyond the standard internship experience.\nWhile the program is primarily focused on learning and skill development, the structured approach, comprehensive support system, and access to industry events create an enriching experience that goes beyond typical internship offerings. The policies promote self-directed learning while providing sufficient guidance and support when needed.\nAdditional Questions What did you find most satisfying during your internship?\nThe most satisfying aspect of my internship was the hands-on approach to learning AWS cloud services. Rather than just theoretical study, I was able to build real projects, troubleshoot actual issues, and see the direct application of concepts I learned. The opportunity to participate in AWS community events and interact with industry professionals was particularly fulfilling, as it gave me broader perspective on career possibilities and industry practices.\nAdditionally, I found great satisfaction in successfully completing the workshop project that demonstrates S3 access patterns through VPC endpoints. Seeing a project through from concept to completion and creating documentation that others can learn from gave me a genuine sense of accomplishment and contribution.\nWhat do you think the company should improve for future interns?\nThe main area for improvement would be the scheduling and coordination of on-site working slots. Currently, the arrangement for when interns need to come to the office could be better organized and communicated earlier. Having a more structured schedule or advance notice for on-site requirements would help interns plan their time more effectively, especially those who may be balancing internship with academic commitments.\nAdditionally, as mentioned earlier, providing a lunch facility or partnering with nearby food services for meal arrangements would greatly improve the daily experience for interns who spend full days at the office.\nIf recommending to a friend, would you suggest they intern here? Why or why not?\nAbsolutely, I would highly recommend this internship to friends interested in cloud computing and AWS technologies. The program offers excellent learning opportunities, hands-on experience with industry-relevant technologies, and exposure to professional working environments that are hard to find elsewhere.\nThe supportive mentorship, access to AWS community events, and the emphasis on practical skill development make this an invaluable experience for anyone looking to build a career in cloud computing or DevOps. The minor inconveniences like lunch arrangements and scheduling coordination are far outweighed by the quality of learning and professional growth opportunities provided.\nFor students serious about developing cloud computing skills and gaining real-world experience with AWS services, this internship program is an exceptional choice that will significantly enhance their career prospects.\nSuggestions \u0026amp; Expectations Suggestions to improve the internship experience:\nEstablish a more structured schedule for on-site attendance with advance notification (e.g., monthly calendar shared at the beginning of each month) Consider arranging lunch solutions such as partnering with nearby restaurants for group discounts or creating a simple pantry area with basic facilities Perhaps organize periodic intern cohort meetings or virtual check-ins to facilitate peer learning and networking among interns Provide a clear guideline or checklist at the start of the internship outlining all deliverables, milestones, and expectations Would you like to continue this program in the future?\nYes, I would be very interested in continuing my association with First Cloud Journey in the future, whether through advanced training programs, community contributions, or potential employment opportunities. The experience has been highly valuable, and I believe there is still much more to learn and contribute to the AWS community.\nAny other comments (free sharing):\nI want to express my sincere gratitude to the First Cloud Journey team, especially my mentor and the admin staff, for their patience, guidance, and support throughout this internship. This experience has not only enhanced my technical skills but also shaped my professional attitude and career aspirations.\nThe exposure to AWS services, participation in community events, and the hands-on project work have given me confidence in my abilities and a clear direction for my career path in cloud computing. I look forward to staying connected with the FCJ community and contributing back in whatever capacity I can in the future. Thank you for this wonderful opportunity!\n"},{"uri":"https://rsshive.github.io/fcj-workshop/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Integrate TMDB data source into the chatbot Develop Lambda functions to support chatbot operations Enhance chatbot with movie/TV show information capabilities Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Research TMDB API structure and capabilities - Plan TMDB data integration strategy - Define required movie/TV data fields 11/11/2025 11/11/2025 https://developers.themoviedb.org/ 3 - Develop Lambda functions for TMDB API calls + Search movies/TV shows + Get details and metadata + Handle API rate limits 12/11/2025 12/11/2025 TMDB API documentation 4 - Integrate TMDB data with chatbot + Connect TMDB Lambda to Bedrock + Enhance chatbot prompts with real data + Format movie/TV information 13/11/2025 13/11/2025 Lambda integration patterns 5 - Develop supporting Lambda functions + Data caching layer + Error handling and retries + Response optimization 14/11/2025 14/11/2025 AWS Lambda best practices 6 - Practice: + Test chatbot with TMDB data + Validate data accuracy + Optimize performance 15/11/2025 15/11/2025 Testing and optimization guides Week 10 Achievements: Successfully integrated TMDB data source:\nConfigured TMDB API access Implemented secure API key management Set up rate limiting and caching Developed comprehensive Lambda functions:\nTMDB Search Lambda: Search movies and TV shows TMDB Details Lambda: Fetch detailed information Data Processing Lambda: Format and optimize data Cache Management Lambda: Manage frequently accessed data Enhanced chatbot capabilities:\nReal-time movie/TV show information Accurate cast and crew details Release dates and ratings Recommendations based on TMDB data View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://rsshive.github.io/fcj-workshop/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Complete all project documentation Refine APIs to provide sufficient information for Frontend Ensure API responses meet Frontend requirements Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review all project documentation - Identify gaps in technical documentation - Plan documentation improvements 18/11/2025 18/11/2025 Documentation best practices 3 - Write comprehensive API documentation + Endpoint descriptions + Request/response examples + Error codes and handling 19/11/2025 19/11/2025 OpenAPI/Swagger standards 4 - Gather Frontend requirements feedback - Analyze API response gaps - Plan API adjustments 20/11/2025 20/11/2025 Frontend-Backend collaboration guides 5 - Refine APIs for Frontend needs + Add missing data fields + Optimize response structures + Improve error messages 21/11/2025 21/11/2025 API design patterns 6 - Practice: + Final integration testing + Update documentation + Validate all endpoints 22/11/2025 22/11/2025 Testing and validation guides Week 11 Achievements: Completed comprehensive project documentation:\nTechnical architecture documentation API reference guide with examples Deployment and setup instructions User guides for chatbot features Refined APIs based on Frontend requirements:\nAdded missing user profile fields Enhanced movie/TV data responses Improved error message clarity Optimized response payload sizes API improvements implemented:\nAdded pagination support Included additional metadata fields Enhanced filtering capabilities Improved response time by 30% View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://rsshive.github.io/fcj-workshop/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Review and identify gaps from previous development phases Finalize all project documentation Complete OJT (On-the-Job Training) period successfully Prepare final presentation and deliverables Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Conduct comprehensive project review - Identify missing features and gaps - Create prioritized list of remaining tasks 25/11/2025 25/11/2025 Project retrospective guidelines 3 - Team discussion on gaps and improvements - Assign remaining tasks to team members - Set final deadlines for completion 26/11/2025 26/11/2025 Agile retrospective methods 4 - Final documentation touches + Polish technical documentation + Update README files + Complete user guides + Add troubleshooting sections 27/11/2025 27/11/2025 Documentation best practices 5 - Prepare OJT completion materials + Final project report + Presentation slides + Demo preparation + Lessons learned document 28/11/2025 28/11/2025 OJT completion guidelines 6 - Final review: + Complete all testing + Final deployment check + Submit OJT deliverables + Celebrate completion! 29/11/2025 29/11/2025 Project closeout procedures Week 12 Achievements: Completed comprehensive project review:\nIdentified and documented all gaps Prioritized critical fixes and improvements Resolved remaining technical debt Finalized all project documentation:\nTechnical architecture document Complete API documentation Deployment and operations guide User manuals and tutorials Troubleshooting guide Code documentation and comments Successfully completed OJT period:\nDelivered fully functional movie/TV recommendation system Integrated AWS Bedrock chatbot with TMDB data Implemented secure authentication with Cognito Achieved all project milestones Prepared and submitted final deliverables:\nFinal project report Technical presentation Live demo of the application Lessons learned documentation Key accomplishments over 12 weeks:\nBuilt production-ready serverless application Mastered AWS services (Lambda, API Gateway, Bedrock, Cognito, etc.) Developed AI-powered chatbot functionality Implemented best practices for cloud architecture Gained hands-on experience with modern development practices Successfully transitioned from learning to building real-world applications\n"},{"uri":"https://rsshive.github.io/fcj-workshop/2-proposal/tmp/","title":"","tags":[],"description":"","content":"IoT Weather Platform for Lab Research A Unified AWS Serverless Solution for Real-Time Weather Monitoring 1. Executive Summary The IoT Weather Platform is designed for the ITea Lab team in Ho Chi Minh City to enhance weather data collection and analysis. It supports up to 5 weather stations, with potential scalability to 10-15, utilizing Raspberry Pi edge devices with ESP32 sensors to transmit data via MQTT. The platform leverages AWS Serverless services to deliver real-time monitoring, predictive analytics, and cost efficiency, with access restricted to 5 lab members via Amazon Cognito.\n2. Problem Statement What’s the Problem? Current weather stations require manual data collection, becoming unmanageable with multiple units. There is no centralized system for real-time data or analytics, and third-party platforms are costly and overly complex.\nThe Solution The platform uses AWS IoT Core to ingest MQTT data, AWS Lambda and API Gateway for processing, Amazon S3 for storage (including a data lake), and AWS Glue Crawlers and ETL jobs to extract, transform, and load data from the S3 data lake to another S3 bucket for analysis. AWS Amplify with Next.js provides the web interface, and Amazon Cognito ensures secure access. Similar to Thingsboard and CoreIoT, users can register new devices and manage connections, though this platform operates on a smaller scale and is designed for private use. Key features include real-time dashboards, trend analysis, and low operational costs.\nBenefits and Return on Investment The solution establishes a foundational resource for lab members to develop a larger IoT platform, serving as a study resource, and provides a data foundation for AI enthusiasts for model training or analysis. It reduces manual reporting for each station via a centralized platform, simplifying management and maintenance, and improves data reliability. Monthly costs are $0.66 USD per the AWS Pricing Calculator, with a 12-month total of $7.92 USD. All IoT equipment costs are covered by the existing weather station setup, eliminating additional development expenses. The break-even period of 6-12 months is achieved through significant time savings from reduced manual work.\n3. Solution Architecture The platform employs a serverless AWS architecture to manage data from 5 Raspberry Pi-based stations, scalable to 15. Data is ingested via AWS IoT Core, stored in an S3 data lake, and processed by AWS Glue Crawlers and ETL jobs to transform and load it into another S3 bucket for analysis. Lambda and API Gateway handle additional processing, while Amplify with Next.js hosts the dashboard, secured by Cognito. The architecture is detailed below:\nAWS Services Used AWS IoT Core: Ingests MQTT data from 5 stations, scalable to 15. AWS Lambda: Processes data and triggers Glue jobs (two functions). Amazon API Gateway: Facilitates web app communication. Amazon S3: Stores raw data in a data lake and processed outputs (two buckets). AWS Glue: Crawlers catalog data, and ETL jobs transform and load it. AWS Amplify: Hosts the Next.js web interface. Amazon Cognito: Secures access for lab users. Component Design Edge Devices: Raspberry Pi collects and filters sensor data, sending it to IoT Core. Data Ingestion: AWS IoT Core receives MQTT messages from the edge devices. Data Storage: Raw data is stored in an S3 data lake; processed data is stored in another S3 bucket. Data Processing: AWS Glue Crawlers catalog the data, and ETL jobs transform it for analysis. Web Interface: AWS Amplify hosts a Next.js app for real-time dashboards and analytics. User Management: Amazon Cognito manages user access, allowing up to 5 active accounts. 4. Technical Implementation Implementation Phases This project has two parts—setting up weather edge stations and building the weather platform—each following 4 phases:\nBuild Theory and Draw Architecture: Research Raspberry Pi setup with ESP32 sensors and design the AWS serverless architecture (1 month pre-internship) Calculate Price and Check Practicality: Use AWS Pricing Calculator to estimate costs and adjust if needed (Month 1). Fix Architecture for Cost or Solution Fit: Tweak the design (e.g., optimize Lambda with Next.js) to stay cost-effective and usable (Month 2). Develop, Test, and Deploy: Code the Raspberry Pi setup, AWS services with CDK/SDK, and Next.js app, then test and release to production (Months 2-3). Technical Requirements\nWeather Edge Station: Sensors (temperature, humidity, rainfall, wind speed), a microcontroller (ESP32), and a Raspberry Pi as the edge device. Raspberry Pi runs Raspbian, handles Docker for filtering, and sends 1 MB/day per station via MQTT over Wi-Fi. Weather Platform: Practical knowledge of AWS Amplify (hosting Next.js), Lambda (minimal use due to Next.js), AWS Glue (ETL), S3 (two buckets), IoT Core (gateway and rules), and Cognito (5 users). Use AWS CDK/SDK to code interactions (e.g., IoT Core rules to S3). Next.js reduces Lambda workload for the fullstack web app. 5. Timeline \u0026amp; Milestones Project Timeline\nPre-Internship (Month 0): 1 month for planning and old station review. Internship (Months 1-3): 3 months. Month 1: Study AWS and upgrade hardware. Month 2: Design and adjust architecture. Month 3: Implement, test, and launch. Post-Launch: Up to 1 year for research. 6. Budget Estimation You can find the budget estimation on the AWS Pricing Calculator.\nOr you can download the Budget Estimation File.\nInfrastructure Costs AWS Services: AWS Lambda: $0.00/month (1,000 requests, 512 MB storage). S3 Standard: $0.15/month (6 GB, 2,100 requests, 1 GB scanned). Data Transfer: $0.02/month (1 GB inbound, 1 GB outbound). AWS Amplify: $0.35/month (256 MB, 500 ms requests). Amazon API Gateway: $0.01/month (2,000 requests). AWS Glue ETL Jobs: $0.02/month (2 DPUs). AWS Glue Crawlers: $0.07/month (1 crawler). MQTT (IoT Core): $0.08/month (5 devices, 45,000 messages). Total: $0.7/month, $8.40/12 months\nHardware: $265 one-time (Raspberry Pi 5 and sensors). 7. Risk Assessment Risk Matrix Network Outages: Medium impact, medium probability. Sensor Failures: High impact, low probability. Cost Overruns: Medium impact, low probability. Mitigation Strategies Network: Local storage on Raspberry Pi with Docker. Sensors: Regular checks and spares. Cost: AWS budget alerts and optimization. Contingency Plans Revert to manual methods if AWS fails. Use CloudFormation for cost-related rollbacks. 8. Expected Outcomes Technical Improvements: Real-time data and analytics replace manual processes.\nScalable to 10-15 stations.\nLong-term Value 1-year data foundation for AI research.\nReusable for future projects.\n"},{"uri":"https://rsshive.github.io/fcj-workshop/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://rsshive.github.io/fcj-workshop/tags/","title":"Tags","tags":[],"description":"","content":""}]